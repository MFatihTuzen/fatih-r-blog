[
  {
    "objectID": "posts/2025-06-04_correlation_vs_causation/index.html#introduction",
    "href": "posts/2025-06-04_correlation_vs_causation/index.html#introduction",
    "title": "Correlation vs Causation: Understanding the Difference",
    "section": "1 Introduction",
    "text": "1 Introduction\n“Correlation is not causation” – it’s a refrain we hear often, yet the distinction between these concepts is deceptively easy to overlook. Correlation refers to a statistical association: when one variable changes, another tends to change as well. Causation, on the other hand, means a change in one variable directly produces a change in another. In other words, there is a cause-and-effect relationship. A crucial insight (sometimes phrased as “causation implies correlation (but not vice versa)”) is that while causation always entails some correlation, observing a correlation by itself does not prove causation. This article will explore the theoretical basis of correlation and causation, illustrate the difference with real-world examples in economics and healthcare, and demonstrate with R code how misleading correlations can arise – and how we can attempt to control for confounding factors. Along the way, we’ll dispel common misconceptions and share expert insights to encourage critical thinking about causal claims in data.\n\nJudea Pearl, a pioneer of modern causal inference, put it succinctly: “Correlation is not causation; merely observing a relationship between variables does not imply a causal connection”. In practical terms, correlation is a symmetric relationship – X and Y vary together – whereas causation is directional: X produces Y. If two things are correlated, there are several possibilities: X causes Y, Y causes X, or some other factor influences both (or it could even be a chance coincidence). As statistician David Freedman warned, “Misinterpreting correlation as causation can lead to erroneous conclusions and misguided actions”. To use data responsibly, we must dig deeper than surface-level associations."
  },
  {
    "objectID": "posts/2025-06-04_correlation_vs_causation/index.html#theoretical-background-correlation-in-a-nutshell",
    "href": "posts/2025-06-04_correlation_vs_causation/index.html#theoretical-background-correlation-in-a-nutshell",
    "title": "Correlation vs Causation: Understanding the Difference",
    "section": "2 Theoretical Background: Correlation in a Nutshell",
    "text": "2 Theoretical Background: Correlation in a Nutshell\nIn statistics, correlation is often measured by the Pearson correlation coefficient (usually notated r). Mathematically, for variables X and Y, this coefficient is defined as:\n\\[\nr_{XY} = \\frac{\\\\Cov(X,Y)}{\\sigma_X \\sigma_Y}\n\\]\nwhere Cov(X,Y) is the covariance and σ denotes standard deviations. This value ranges from –1 (perfect negative correlation) to +1 (perfect positive correlation). An r near 0 indicates no linear relationship. Correlation captures how closely two variables move in sync. For example, if higher values of X tend to coincide with higher values of Y (and lower with lower), the correlation is positive. If one tends to go up when the other goes down, the correlation is negative. Crucially, correlation is a descriptive statistic – it quantifies an association, but it does not explain why the variables are related.\nCorrelation alone is silent on mechanism. It answers “Are X and Y related?” not “Does X change Y?”. To establish causation, we usually rely on theory, controlled experiments, or advanced observational study designs. In the language of causality, we think about interventions: if we do something to X, does Y change as a result? This is fundamentally a different question than observing X and Y moving together. Empirically, evidence of causation typically requires satisfying conditions such as temporal precedence (cause precedes effect), a credible mechanism linking X to Y, consistency with other evidence, and ruling out alternative explanations (confounders)."
  },
  {
    "objectID": "posts/2025-06-04_correlation_vs_causation/index.html#why-correlation-causation-confounders-coincidences-and-reverse-causality",
    "href": "posts/2025-06-04_correlation_vs_causation/index.html#why-correlation-causation-confounders-coincidences-and-reverse-causality",
    "title": "Correlation vs Causation: Understanding the Difference",
    "section": "3 Why Correlation ≠ Causation: Confounders, Coincidences, and Reverse Causality",
    "text": "3 Why Correlation ≠ Causation: Confounders, Coincidences, and Reverse Causality\nIf correlation doesn’t imply causation, what might be going on when two variables track together? There are a few common scenarios:\n\nConfounding (Third Variables): A hidden factor influences both variables. This lurking variable makes X and Y move together, creating an illusion that they’re directly linked. Classic example: children’s shoe sizes are strongly correlated with their reading ability. Obviously, bigger shoes don’t make kids read better. The confounder is age: older children have larger feet and also read more proficiently – age drives both. Once age is taken into account, the shoe size–reading correlation disappears. As Pearl humorously noted, “The third variable problem highlights the danger of assuming causation based solely on correlation”. We’ll demonstrate a confounding example with R shortly.\nPure Coincidence (Spurious Correlations): With enough data, you’re bound to find some weird correlations by chance alone. In fact, whole websites are devoted to absurd correlations. Tyler Vigen’s famous collection of spurious correlations highlights gems like a 0.95 correlation between U.S. per capita cheese consumption and the number of people who died by becoming tangled in their bedsheets. It’s a comical reminder that with countless variables in the world, some will line up in sync purely by accident. High correlation can occur in entirely unrelated data — a cautionary tale for data miners. We should always ask: Is there a plausible reason for this correlation, or could it be random?\nReverse Causation (Directionality): Sometimes X and Y truly are causally related, but not in the direction one assumes. For example, suppose data show a correlation between depression and low vitamin D levels. Does lack of vitamin D cause depression, or do depressed individuals tend to get less sunlight and thus have lower vitamin D? The data alone can’t tell us the direction. Another example: cities with more police officers tend to have higher crime rates. This doesn’t mean police cause crime; rather, high-crime areas hire more police. In economic contexts, we’ll see debates like “Do higher interest rates reduce inflation, or is it that rising inflation prompts central banks to raise rates?” – in such cases, cause and effect can be easily confused if we only look at correlations.\nSelection Bias and Other Pitfalls: In observational data, how samples are collected can also create misleading correlations. For instance, a medical study might find that patients on a certain medication have higher survival rates – but if those patients were also healthier or younger on average (selection bias), the medication’s effect is confounded. Correlation can even vanish or flip sign when data is disaggregated, a phenomenon known as Simpson’s Paradox. The aggregate data might show one trend, while each subgroup shows the opposite trend. This often indicates a confounding variable at play.\n\n\n3.1 Simulating a Spurious Correlation in R\nTo make these ideas concrete, let’s simulate an example in R. We’ll create a scenario with two groups (“Young” and “Old”) where within each group, there is no relationship between our variables, but when we combine the groups, we observe a strong correlation. This mimics a confounding situation (here, age group is the confounder).\n\n# Simulate data for two groups: 'Young' and 'Old'\nset.seed(42)\nAgeGroup &lt;- rep(c(\"Young\", \"Old\"), each = 50)\n# For Young group, generate foot_size and reading_score with no true correlation\nfoot_size &lt;- c(rnorm(50, mean = 20, sd = 2),    # Young have smaller feet on average\n               rnorm(50, mean = 25, sd = 2))    # Old have larger feet on average\nreading_score &lt;- c(rnorm(50, mean = 50, sd = 5),# Young have lower reading scores on avg\n                   rnorm(50, mean = 80, sd = 5))# Old have higher reading scores on avg\n\n# Check correlations\ncor(foot_size, reading_score)                        # overall correlation\n\n[1] 0.7597618\n\ncor(foot_size[AgeGroup==\"Young\"], reading_score[AgeGroup==\"Young\"])  # within Young group\n\n[1] 0.1043372\n\ncor(foot_size[AgeGroup==\"Old\"], reading_score[AgeGroup==\"Old\"])      # within Old group\n\n[1] -0.07429122\n\n\nRunning the code above, we might find an overall Pearson correlation around ~0.75 between foot_size and reading_score for all 100 individuals combined. Yet within each age group separately, the correlation is near 0 (essentially no relationship). In our simulation, foot size was not actually affecting reading ability at all – the apparent overall correlation arose because the Old group had higher values for both variables than the Young group. Age group was the lurking factor. This is a toy example of Simpson’s Paradox, where aggregation masks the true story.\nWe can visualize this:\n\n# Plot the data, coloring by group, and add regression lines\nlibrary(ggplot2)\ndf &lt;- data.frame(AgeGroup, foot_size, reading_score)\nggplot(df, aes(x = foot_size, y = reading_score, color = AgeGroup)) +\n  geom_point(size = 2, alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = FALSE) + \n  geom_smooth(aes(group = 1), method = \"lm\", se = FALSE, color = \"black\", linetype = \"dashed\") +\n  labs(x = \"Foot size (cm)\", y = \"Reading score\",\n       title = \"Spurious correlation: Foot size vs Reading score\",\n       subtitle = \"Colored by AgeGroup. Solid lines = separate group fits (no correlation); dashed line = overall fit.\")\n\n\n\n\n\n\n\n\nInterpretation: In the plot, blue points (younger kids) cluster toward the lower-left, and red points (older kids) cluster at the upper-right. The black dashed line through all data has a clear upward slope, indicating a positive overall correlation. However, the solid trend lines fitted to each group are nearly flat – within each group there’s no meaningful correlation between foot size and reading skill. It’s the group difference (older children are both larger and more literate) that created the misleading overall association. This example underscores why we must be cautious: if we naively observed all the data, we might have (laughably) concluded that “big feet cause better reading”! Only by accounting for the confounding variable (age) do we see the true picture.\nFigure: An example of spurious correlation. Each point is an individual child; foot size and reading score are uncorrelated within the Young (blue) and Old (red) groups, but when pooled together there is a strong positive correlation. The overall trend (black dashed line) is entirely driven by the age-group effect. Such patterns illustrate how a lurking variable can create a misleading correlation."
  },
  {
    "objectID": "posts/2025-06-04_correlation_vs_causation/index.html#real-world-example-1-interest-rates-and-inflation",
    "href": "posts/2025-06-04_correlation_vs_causation/index.html#real-world-example-1-interest-rates-and-inflation",
    "title": "Correlation vs Causation: Understanding the Difference",
    "section": "4 Real-World Example 1: Interest Rates and Inflation",
    "text": "4 Real-World Example 1: Interest Rates and Inflation\nOne arena where correlation vs causation debates rage is macroeconomics. Consider interest rates and inflation – two metrics that often move in tandem. Central banks (like the U.S. Federal Reserve or Bank of England) adjust interest rates as a policy tool, aiming to control inflation. Intuition says raising interest rates should cause inflation to decrease (by cooling off spending and investment). Indeed, periods of tight monetary policy often coincide with inflation coming down. But does that correlation mean the rate hikes caused the relief in inflation? Not necessarily. As one economics blogger noted during the 2022–2023 inflation surge: “One might be tempted to draw a direct line between higher interest rates and lower inflation rates. But correlation does not necessarily imply causation.” In that episode, global inflation started easing after its 2022 peak, at the same time central banks were aggressively raising rates. However, careful analysis suggested much of the inflation decline was due to resolving supply chain issues and falling commodity prices – factors largely independent of interest rate moves. In other words, inflation would have started abating on its own as pandemic-era supply shocks faded, even if interest rates had not been hiked so sharply. The overlap in timing was a correlation, not a definitive proof of causation.\nEconomists have to untangle these relationships with statistical tools and historical data. One approach is to look at lead-lag relationships: if interest rate changes truly cause lower inflation, we’d expect to see inflation consistently drop a few quarters after rate hikes. If instead we observe that inflation spikes often precede rate hikes (as central banks react to rising inflation), that indicates reverse causation – inflation causing interest rate changes. Studies of the UK economy, for instance, found that in the short run, raising interest rates sometimes correlated with higher inflation in subsequent quarters. This counter-intuitive positive correlation could mean that initial rate hikes were implemented when inflation was already rising (so inflation kept climbing shortly after), or that rate hikes had supply-side effects (e.g. raising business costs) that temporarily stoked inflation. Only after a longer time lag did the correlation turn negative as expected (inflation easing modestly) – and even then, the effect was statistically weak in some analyses. An outside observer summed up the mixed evidence wryly: “If correlation means causality then possibly not. [Rate hikes] may have an effect, but the effect might be weak on inflation and brutal on society”. In other words, simply correlating past interest increases with inflation outcomes can be misleading; it takes careful modeling to isolate the causal impact (and it might be smaller than popularly assumed).\nThis example highlights two key points: First, directionality matters – are we seeing X→Y or Y→X or both? (In economics, feedback loops are common: inflation could prompt rate changes, which in turn influence future inflation, a two-way causality.) Second, confounding variables abound – other factors like global supply conditions, fiscal policy, or consumer expectations can drive inflation, obscuring the effect of interest rates. Analysts tackle these challenges with techniques such as Vector Autoregression (VAR) models, instrumental variables, or by “clustering” data to compare similar periods or countries. A commenter on an economics forum pointed out that failing to control for such factors is akin to falling for Simpson’s Paradox: “Plotting inflation vs interest rates can be misleading unless you cluster to avoid confounding variables”. The lesson: even in highly data-driven fields like economics, correlation alone can support multiple stories, and solid conclusions require digging into the causal structure of the problem."
  },
  {
    "objectID": "posts/2025-06-04_correlation_vs_causation/index.html#real-world-example-2-vaccines-and-disease-prevalence",
    "href": "posts/2025-06-04_correlation_vs_causation/index.html#real-world-example-2-vaccines-and-disease-prevalence",
    "title": "Correlation vs Causation: Understanding the Difference",
    "section": "5 Real-World Example 2: Vaccines and Disease Prevalence",
    "text": "5 Real-World Example 2: Vaccines and Disease Prevalence\nFew areas demonstrate the difference between correlation and causation as starkly as healthcare and epidemiology. Let’s examine vaccines and disease rates. Vaccines are designed based on a known causal mechanism (they induce immunity, which prevents disease), and countless studies and trials have validated their efficacy. Thus, when a vaccine is introduced, we expect disease incidence to drop as a causal result. Conversely, if vaccination rates fall, diseases can surge. Both correlations have been observed in reality – one led to a life-saving public health success, the other to a dangerous resurgence of disease – and they underline why understanding causality is critical.\nCorrelation used as evidence of causation (correctly): In the 1950s, polio was a dreaded disease paralyzing tens of thousands each year. In 1955, the Salk polio vaccine was introduced. Within just a few years, polio cases plummeted. In the United States, annual polio cases dropped from ~58,000 to about 5,600 by 1957, and only 161 cases by 1961. The timing and magnitude of this drop, alongside laboratory and clinical evidence, provided convincing proof that the vaccine caused the decline in polio. Here the correlation (vaccine rollout followed by disease collapse) was no coincidence – it was a predicted outcome based on a causal understanding of immunity. As another example, when the HPV vaccine was introduced, health officials observed sharp declines in HPV infections and related cancers in subsequent years, consistent with the expected causal effect of vaccinating adolescents. In such cases, correlation was a strong hint that led scientists to conclude causation, bolstered by controlled trials and biological plausibility.\nCorrelation misinterpreted as causation (incorrectly): Not all observed links are what they seem. A notorious case was the now-debunked claim that the MMR (measles, mumps, rubella) vaccine caused autism in children. This idea stemmed from a 1998 study (later found fraudulent) and the anecdotal observation that autism diagnoses were often made around the same age children receive lots of vaccines. In truth, the apparent correlation was driven by coincidental timing and increased awareness/diagnosis of autism in the 1990s – not by vaccines. Extensive research over decades showed no causal link: “Research over the past 15 years has shown that childhood vaccines don’t cause autism”. Unfortunately, the fear incited by the false correlation led many parents to avoid the MMR vaccine. The result? Measles, once near-eliminated, came roaring back. Great Britain, for instance, experienced a measles epidemic in the 2000s as vaccination rates fell. Public health officials directly attributed this to the drop in vaccinations after the autism scare: “Great Britain is in the midst of a measles epidemic, one that public health officials say is the result of parents refusing to vaccinate their children after a safety scare that was later proved to be fraudulent”. In regions where MMR vaccination rates fell below about 80%, measles cases spiked dramatically. One commentator lamented, “This is the legacy of the Wakefield scare”. The correlation here – lower vaccination accompanied by higher disease incidence – reflected a causal relationship, but in the opposite direction of the original false claim. Vaccines prevent measles, so when vaccination dropped, measles returned. It’s a sobering example of how a misunderstood correlation (vaccines and autism) led to behaviors that revealed a very real causation (lack of vaccines causing disease outbreaks).\nIn summary, the vaccine story teaches us that we must have external evidence and domain knowledge to distinguish meaningful correlations from spurious ones. When strong theory and additional evidence support a correlation (as with vaccines preventing disease), we can infer causation with confidence. But when a correlation flies in the face of established knowledge or lacks a plausible mechanism (as with vaccines causing autism), it demands deep skepticism and further investigation. Correlation may open the door to a hypothesis, but only rigorous science can confirm causality."
  },
  {
    "objectID": "posts/2025-06-04_correlation_vs_causation/index.html#from-correlation-to-causation-how-can-we-tell",
    "href": "posts/2025-06-04_correlation_vs_causation/index.html#from-correlation-to-causation-how-can-we-tell",
    "title": "Correlation vs Causation: Understanding the Difference",
    "section": "6 From Correlation to Causation: How Can We Tell?",
    "text": "6 From Correlation to Causation: How Can We Tell?\nSo, if correlation alone isn’t enough, how do scientists and statisticians actually establish causation? This is the realm of causal inference, and entire textbooks (and careers) are devoted to it. Here we’ll outline a few key principles and methods:\n\nControlled Experiments: The gold standard for testing causality is the randomized controlled trial (RCT). By randomly assigning subjects to a treatment (X) or control, we ensure no systematic confounders differ between groups. Any difference in outcomes (Y) can then be attributed to X (within known statistical error). As statistician Paul Rosenbaum emphasizes, “Experimental design is crucial for establishing causal relationships and overcoming confounding factors”. In fields like medicine, RCTs are required to claim a drug causes an effect. In more complex domains (economics, social sciences) where RCTs may be infeasible or unethical, researchers look for natural experiments or instrumental variables to approximate that level of control.\nTemporal Checks: Ensure the cause precedes the effect. Sounds obvious, but it’s a simple way to weed out some mistaken causal interpretations. If Y happens before X, X cannot be the cause. Sometimes lagged correlations or time-series analyses (like Granger causality tests in economics) are used to see if changes in X consistently come before changes in Y. In our interest rate example, analysts examined whether inflation tended to drop after interest rate hikes (and found mixed results, indicating caution in the causal claim).\nControlling for Confounders: In observational studies, a common strategy is to measure possible confounding variables and include them in a regression or stratify the analysis. For instance, if we suspect age is a confounder in our earlier example, we can compare individuals of similar age (or include age in a multiple regression model) to see if foot size still correlates with reading ability within those strata. If the correlation vanishes after controlling for the third variable, it was likely spurious. Techniques like multiple regression, matching, propensity score adjustment, and difference-in-differences analysis are all about simulating a “ceteris paribus” condition – i.e. comparing like with like, so that the effect of interest can be isolated. In R, one might use lm() (linear modeling) to adjust for confounders. For example, lm(reading_score ~ foot_size + Age, data=df) would tell us if foot_size still has any predictive power for reading_score once Age is accounted for. (In our simulated data, it would show foot_size is not significant when Age is included, reinforcing that foot size itself wasn’t causing better reading.)\nMultiple Studies and Triangulation: We gain confidence in causation when multiple independent studies, using different methods, consistently point to the same conclusion. If correlational evidence is supported by lab experiments, longitudinal studies, and perhaps natural experiments, the case for causality strengthens. In the smoking and lung cancer example: early on, skeptics said “correlation is not causation” – maybe smokers had other habits causing cancer. But over time, mountains of evidence (animal experiments, biological mechanisms, epidemiological studies controlling for diet, etc.) converged to establish that smoking does cause cancer.\nPlausibility and Mechanism: A correlation accompanied by a plausible mechanism is more convincing. If we can explain how X could influence Y (through physics, biology, or logic), we are more likely to consider X a potential cause of Y. In contrast, if no one can conceive a realistic way that X would affect Y, we suspect a lurking variable or coincidence. (For instance, it’s hard to imagine how eating more cheese would directly cause strangulation by bedsheets – more likely, as one humorous analysis noted, it’s just an “accidental, misleading pattern” or related to a confounder like time or lifestyle).\nCausal Graphs and Models: Modern data science sometimes employs causal graphs or Bayesian networks (à la Judea Pearl’s do-calculus) to formally model assumptions about causation and test if the observed correlations fit a causal structure. While beyond the scope of this article, these tools provide a framework to encode “X causes Y” assumptions and see what observational patterns should emerge if that’s true. They also help identify what additional data or experiments are needed to distinguish between competing causal hypotheses.\n\nIn practice, determining causation is often like solving a puzzle. We marshal all available evidence, use critical thinking, and sometimes still end up with uncertainty. However, the effort is worthwhile because acting on false causal assumptions can be costly. Misattributing causation can lead to bad policy, ineffective or harmful interventions, or simply wasting resources chasing the wrong problem. As we’ve seen, data should be approached with a skeptical eye. Correlations can be tantalizing – they can indeed be hints to causal relationships – but we must verify those hints. By combining statistical rigor with domain expertise, we improve our chances of getting the causation right."
  },
  {
    "objectID": "posts/2025-06-04_correlation_vs_causation/index.html#conclusion",
    "href": "posts/2025-06-04_correlation_vs_causation/index.html#conclusion",
    "title": "Correlation vs Causation: Understanding the Difference",
    "section": "7 Conclusion",
    "text": "7 Conclusion\nUnderstanding the difference between correlation and causation is essential for anyone who consumes data-driven information (which these days is all of us). We’ve covered how correlation is a mathematical relationship that can flag interesting connections but can also mislead us through confounding, coincidence, or reversed cause-and-effect. We explored examples from economics and healthcare where these distinctions have real-world consequences – from guiding central bank policies to informing public health decisions. The key takeaway is to think critically: when you hear that X is linked to Y, ask why and how. Look for evidence that goes beyond the raw correlation. As the saying goes (often attributed to many scientists), “Correlation is not causation, but it sure is a hint.” Use the hint to investigate further, not to jump to conclusions.\nIn the words of statistician David Freedman, misinterpreting correlation as causation is not just an academic error but one that can lead to “misguided actions”. By staying curious and skeptical – and by leveraging tools like R to analyze data properly – we can uncover the true stories our data are telling us. Correlation can open the door to discovery, but only rigorous analysis and critical thinking will reveal what’s inside."
  },
  {
    "objectID": "posts/2025-06-04_correlation_vs_causation/index.html#references-further-reading",
    "href": "posts/2025-06-04_correlation_vs_causation/index.html#references-further-reading",
    "title": "Correlation vs Causation: Understanding the Difference",
    "section": "8 References & Further Reading",
    "text": "8 References & Further Reading\n\nBandholz, H. “Correlation does not imply causation: higher interest and lower inflation rates” – Macro to Go blog (2023)\nEddy-OJB. “Do Interest Rates Tackle Inflation?” – Medium (2023)\nStatsig Team. Examples of misleading correlations (2024)\nMatthews, R. “Storks Deliver Babies (p = 0.008)” – Teaching Statistics via Priceonomics\nScribbr – Correlation vs Causation (2021)\nNPR – Measles epidemic after vaccine scare (2013)\nWHO – History of polio vaccination (2021)"
  },
  {
    "objectID": "posts/2025-03-11_underrated_functions/index.html",
    "href": "posts/2025-03-11_underrated_functions/index.html",
    "title": "Underrated Gems in R: Must-Know Functions You’re Probably Missing Out On",
    "section": "",
    "text": "R is packed with powerhouse tools—think dplyr for data wrangling, ggplot2 for stunning visuals, or tidyr for tidying up messes. But beyond the headliners, there’s a lineup of lesser-known functions that deserve a spot in your toolkit. These hidden gems can streamline your code, solve tricky problems, and even make you wonder how you managed without them. In this post, we’ll uncover four underrated R functions: Reduce, vapply, do.call and janitor::clean_names. With practical examples ranging from beginner-friendly to advanced, plus outputs to show you what’s possible, this guide will have you itching to try them out in your next project. Let’s dive in and see what these under-the-radar stars can do!"
  },
  {
    "objectID": "posts/2025-03-11_underrated_functions/index.html#reduce-collapse-with-control",
    "href": "posts/2025-03-11_underrated_functions/index.html#reduce-collapse-with-control",
    "title": "Underrated Gems in R: Must-Know Functions You’re Probably Missing Out On",
    "section": "1. Reduce: Collapse with Control",
    "text": "1. Reduce: Collapse with Control\n\nWhat It Does and Its Arguments\nReduce is a base R function that iteratively applies a two-argument function to a list or vector, shrinking it down to a single result. It’s like a secret weapon for avoiding loops while keeping things elegant.\nKey Arguments:\n\nf: The function to apply (e.g., +, *, or a custom one).\nx: The list or vector to reduce.\ninit (optional): A starting value (defaults to the first element of x if omitted).\naccumulate (optional): If TRUE, returns all intermediate results (defaults to FALSE).\n\n\n\nUse Cases\n\nSumming or multiplying without explicit iteration.\nCombining data structures step-by-step.\nSimplifying recursive tasks.\n\n\n\nExamples\n\nSimple: Quick Sum\n\nnumbers &lt;- 1:5\ntotal &lt;- Reduce(`+`, numbers)\nprint(total)\n\n[1] 15\n\n\nExplanation: Reduce adds 1 + 2 = 3, then 3 + 3 = 6, 6 + 4 = 10, and 10 + 5 = 15. It’s a sleek alternative to sum().\n\n\nIntermediate: String Building\n\nwords &lt;- c(\"R\", \"is\", \"awesome\")\nsentence &lt;- Reduce(paste, words, init = \"\")\nprint(sentence)\n\n[1] \" R is awesome\"\n\n\nExplanation: Starting with an empty string (init = ““), Reduce glues the words together with spaces. Skip init, and it starts with”R”, which might not be what you want.\n\n\nAdvanced: Merging Data Frames\n\ndf1 &lt;- data.frame(a = 1:2, b = c(\"x\", \"y\"))\ndf2 &lt;- data.frame(a = 3:4, b = c(\"z\", \"w\"))\ndf3 &lt;- data.frame(a = 5:6, b = c(\"p\", \"q\"))\ncombined &lt;- Reduce(rbind, list(df1, df2, df3))\nprint(combined)\n\n  a b\n1 1 x\n2 2 y\n3 3 z\n4 4 w\n5 5 p\n6 6 q\n\n\nExplanation: Reduce stacks three data frames row-wise, pairing them up one by one. It’s a loop-free way to handle multiple merges.\n\n\n\n\n\n\nA Quick Note on purrr::reduce()\n\n\n\nIf you’re a fan of the tidyverse, check out purrr::reduce(). It’s a modern take on base R’s Reduce, offering a consistent syntax with other purrr functions (like .x and .y for arguments) and handy shortcuts like ~ .x + .y for inline functions. It also defaults to left-to-right reduction but can go right-to-left with reduce_right(). Worth a look if you want a more polished, tidyverse-friendly alternative!\nHere’s an intermediate-level example of using the reduce() function from the purrr package for joining multiple dataframes:\n\nlibrary(purrr)\nlibrary(dplyr)\n\n# Create three sample dataframes representing different aspects of customer data\ncustomers &lt;- data.frame(\n  customer_id = 1:5,\n  name = c(\"Alice\", \"Bob\", \"Charlie\", \"Diana\", \"Edward\"),\n  age = c(32, 45, 28, 36, 52)\n)\n\norders &lt;- data.frame(\n  order_id = 101:108,\n  customer_id = c(1, 2, 2, 3, 3, 3, 4, 5),\n  order_date = as.Date(c(\"2023-01-15\", \"2023-01-20\", \"2023-02-10\", \n                        \"2023-01-05\", \"2023-02-15\", \"2023-03-20\",\n                        \"2023-02-25\", \"2023-03-10\")),\n  amount = c(120.50, 85.75, 200.00, 45.99, 75.25, 150.00, 95.50, 210.25)\n)\n\nfeedback &lt;- data.frame(\n  feedback_id = 201:206,\n  customer_id = c(1, 2, 3, 3, 4, 5),\n  rating = c(4, 5, 3, 4, 5, 4),\n  feedback_date = as.Date(c(\"2023-01-20\", \"2023-01-25\", \"2023-01-10\",\n                          \"2023-02-20\", \"2023-03-01\", \"2023-03-15\"))\n)\n\n# List of dataframes to join with the joining column\ndataframes_to_join &lt;- list(\n  list(df = customers, by = \"customer_id\"),\n  list(df = orders, by = \"customer_id\"),\n  list(df = feedback, by = \"customer_id\")\n)\n\n# Using reduce to join all dataframes\n# Start with customers dataframe and progressively join the others\njoined_data &lt;- reduce(\n  dataframes_to_join[-1],  # Exclude first dataframe as it's our starting point\n  function(acc, x) {\n    left_join(acc, x$df, by = x$by)\n  },\n  .init = dataframes_to_join[[1]]$df  # Start with customers dataframe\n)\n\n# View the result\nprint(joined_data)\n\n   customer_id    name age order_id order_date amount feedback_id rating\n1            1   Alice  32      101 2023-01-15 120.50         201      4\n2            2     Bob  45      102 2023-01-20  85.75         202      5\n3            2     Bob  45      103 2023-02-10 200.00         202      5\n4            3 Charlie  28      104 2023-01-05  45.99         203      3\n5            3 Charlie  28      104 2023-01-05  45.99         204      4\n6            3 Charlie  28      105 2023-02-15  75.25         203      3\n7            3 Charlie  28      105 2023-02-15  75.25         204      4\n8            3 Charlie  28      106 2023-03-20 150.00         203      3\n9            3 Charlie  28      106 2023-03-20 150.00         204      4\n10           4   Diana  36      107 2023-02-25  95.50         205      5\n11           5  Edward  52      108 2023-03-10 210.25         206      4\n   feedback_date\n1     2023-01-20\n2     2023-01-25\n3     2023-01-25\n4     2023-01-10\n5     2023-02-20\n6     2023-01-10\n7     2023-02-20\n8     2023-01-10\n9     2023-02-20\n10    2023-03-01\n11    2023-03-15\n\n\nThis example demonstrates how to use reduce() to join multiple dataframes in a sequential, elegant way. This pattern is particularly useful when dealing with complex data integration tasks where you need to combine multiple data sources with a common identifier."
  },
  {
    "objectID": "posts/2025-03-11_underrated_functions/index.html#vapply-iteration-with-assurance",
    "href": "posts/2025-03-11_underrated_functions/index.html#vapply-iteration-with-assurance",
    "title": "Underrated Gems in R: Must-Know Functions You’re Probably Missing Out On",
    "section": "2. vapply: Iteration with Assurance",
    "text": "2. vapply: Iteration with Assurance\n\nWhat It Does and Its Arguments\nvapply is another base R gem, similar to lapply but with a twist: it forces you to specify the output type and length upfront. This makes it safer and more predictable, especially for critical tasks.\nKey Arguments:\n\nX: The list or vector to process.\nFUN: The function to apply to each element.\nFUN.VALUE: A template for the output (e.g., numeric(1) for a single number).\n\n\n\nUse Cases\n\nGuaranteeing consistent output types.\nExtracting specific stats from lists.\nWriting reliable code for packages or production.\n\n\n\nExamples\n\nSimple: Doubling Up\n\nvalues &lt;- 1:3\ndoubled &lt;- vapply(values, function(x) x * 2, numeric(1))\nprint(doubled)\n\n[1] 2 4 6\n\n\nExplanation: Each value doubles, and numeric(1) ensures a numeric vector—simple and rock-solid.\n\n\nIntermediate: Word Lengths\n\nterms &lt;- c(\"data\", \"science\", \"R\")\nlengths &lt;- vapply(terms, nchar, numeric(1))\nprint(lengths)\n\n   data science       R \n      4       7       1 \n\n\nExplanation: vapply counts characters per word, delivering a numeric vector every time—no surprises like sapply might throw.\n\n\nAdvanced: Stats Snapshot\n\nsamples &lt;- list(c(1, 2, 3), c(4, 5), c(6, 7, 8))\nstats &lt;- vapply(samples, function(x) c(mean = mean(x), sd = sd(x)), numeric(2))\nprint(stats)\n\n     [,1]      [,2] [,3]\nmean    2 4.5000000    7\nsd      1 0.7071068    1\n\n\nExplanation: For each sample, vapply computes mean and standard deviation, returning a matrix (2 rows, 3 columns). It’s a tidy, type-safe summary."
  },
  {
    "objectID": "posts/2025-03-11_underrated_functions/index.html#do.call-dynamic-function-magic",
    "href": "posts/2025-03-11_underrated_functions/index.html#do.call-dynamic-function-magic",
    "title": "Underrated Gems in R: Must-Know Functions You’re Probably Missing Out On",
    "section": "3. do.call: Dynamic Function Magic",
    "text": "3. do.call: Dynamic Function Magic\n\nWhat It Does and Its Arguments\ndo.call in base R lets you call a function with a list of arguments, making it a go-to for flexible, on-the-fly operations. It’s like having a universal remote for your functions.\nKey Arguments:\n\nwhat: The function to call (e.g., rbind, paste).\nargs: A list of arguments to pass.\nquote (optional): Rarely used, defaults to FALSE.\n\n\n\nUse Cases\n\nCombining variable inputs.\nRunning functions dynamically.\nSimplifying calls with list-based data.\n\n\n\nExamples\n\nSimple: Vector Mashup\n\nchunks &lt;- list(1:3, 4:6)\nall &lt;- do.call(c, chunks)\nprint(all)\n\n[1] 1 2 3 4 5 6\n\n\nExplanation: do.call feeds the list to c(), stitching the vectors together effortlessly.\n\n\nIntermediate: Custom Join\n\nbits &lt;- list(\"Code\", \"Runs\", \"Fast\")\njoined &lt;- do.call(paste, c(bits, list(sep = \"|\")))\nprint(joined)\n\n[1] \"Code|Runs|Fast\"\n\n\nExplanation: do.call combines the list with a sep argument, creating a piped string in one smooth move.\n\n\nAdvanced: Flexible Binding\n\ndf_list &lt;- list(data.frame(x = 1:2), data.frame(x = 3:4))\ndirection &lt;- \"vertical\"\nbound &lt;- do.call(if (direction == \"vertical\") rbind else cbind, df_list)\nprint(bound)\n\n  x\n1 1\n2 2\n3 3\n4 4\n\n\nExplanation: With direction = “vertical”, do.call uses rbind to stack rows. Change it to “horizontal”, and cbind takes over—dynamic and smart."
  },
  {
    "objectID": "posts/2025-03-11_underrated_functions/index.html#janitorclean_names-tame-your-column-chaos",
    "href": "posts/2025-03-11_underrated_functions/index.html#janitorclean_names-tame-your-column-chaos",
    "title": "Underrated Gems in R: Must-Know Functions You’re Probably Missing Out On",
    "section": "4. janitor::clean_names: Tame Your Column Chaos",
    "text": "4. janitor::clean_names: Tame Your Column Chaos\n\nWhat It Does and Its Arguments\nFrom the janitor package, clean_names() transforms messy column names into consistent, code-friendly formats (e.g., lowercase with underscores). It’s a time-saver you’ll wish you’d known sooner.\nKey Arguments:\n\ndat: The data frame to clean.\ncase: The style for names (e.g., “snake”, “small_camel”, defaults to “snake”).\nreplace: A named vector for custom replacements (optional).\n\n\n\nUse Cases\n\nStandardizing imported data with ugly headers.\nPrepping data frames for analysis or plotting.\nAvoiding frustration with inconsistent naming.\n\n\n\nExamples\n\nSimple: Basic Cleanup\n\nlibrary(janitor)\n\n# Create a dataframe with messy column names\ndf &lt;- data.frame(\n  `First Name` = c(\"John\", \"Mary\", \"David\"),\n  `Last.Name` = c(\"Smith\", \"Johnson\", \"Williams\"),\n  `Email-Address` = c(\"john@example.com\", \"mary@example.com\", \"david@example.com\"),\n  `Annual Income ($)` = c(65000, 78000, 52000),\n  check.names = FALSE\n)\n\n# View original column names\nnames(df)\n\n[1] \"First Name\"        \"Last.Name\"         \"Email-Address\"    \n[4] \"Annual Income ($)\"\n\n# Clean the names\nclean_df &lt;- clean_names(df)\n\n# View cleaned column names\nnames(clean_df)\n\n[1] \"first_name\"    \"last_name\"     \"email_address\" \"annual_income\"\n\n\nWhat clean_names() specifically does:\n\nConverts all names to lowercase\nReplaces spaces with underscores\nRemoves special characters like periods and hyphens\nCreates names that are valid R variable names and follow standard naming conventions\n\nThis standardization makes your data more consistent, easier to work with, and helps prevent errors when manipulating or joining datasets.\n\n\nIntermediate: Custom Style\n\nlibrary(dplyr)\nlibrary(purrr)\n\n# Create multiple dataframes with inconsistent naming\ndf1 &lt;- data.frame(\n  `Customer ID` = 1:3,\n  `First Name` = c(\"John\", \"Mary\", \"David\"),\n  `LAST NAME` = c(\"Smith\", \"Johnson\", \"Williams\"),\n  check.names = FALSE\n)\n\ndf2 &lt;- data.frame(\n  `customer.id` = 4:6,\n  `firstName` = c(\"Michael\", \"Linda\", \"James\"),\n  `lastName` = c(\"Brown\", \"Davis\", \"Miller\"),\n  check.names = FALSE\n)\n\ndf3 &lt;- data.frame(\n  `cust_id` = 7:9,\n  `first-name` = c(\"Robert\", \"Jennifer\", \"Thomas\"),\n  `last-name` = c(\"Wilson\", \"Martinez\", \"Anderson\"),\n  check.names = FALSE\n)\n\n# List of dataframes\ndfs &lt;- list(df1, df2, df3)\n\n# Clean names of all dataframes\nclean_dfs &lt;- map(dfs, clean_names)\n\n# Print column names for each cleaned dataframe\nmap(clean_dfs, names)\n\n[[1]]\n[1] \"customer_id\" \"first_name\"  \"last_name\"  \n\n[[2]]\n[1] \"customer_id\" \"first_name\"  \"last_name\"  \n\n[[3]]\n[1] \"cust_id\"    \"first_name\" \"last_name\" \n\n# Bind the dataframes (now possible because of standardized column names)\ncombined_df &lt;- bind_rows(clean_dfs)\nprint(combined_df)\n\n  customer_id first_name last_name cust_id\n1           1       John     Smith      NA\n2           2       Mary   Johnson      NA\n3           3      David  Williams      NA\n4           4    Michael     Brown      NA\n5           5      Linda     Davis      NA\n6           6      James    Miller      NA\n7          NA     Robert    Wilson       7\n8          NA   Jennifer  Martinez       8\n9          NA     Thomas  Anderson       9\n\n\nThis code demonstrates a more advanced use case of the clean_names() function when working with multiple data frames that have inconsistent naming conventions. Note that because of the different column names for customer ID, we have missing values in the combined dataframe. This example demonstrates why standardized naming is important.\n\n\nAdvanced: Targeted Fixes\n\ndf &lt;- data.frame(\"ID#\" = 1:2, \"Sales_%\" = c(10, 20), \"Q1 Revenue\" = c(100, 200))\ncleaned &lt;- clean_names(df, replace = c(\"#\" = \"_num\", \"%\" = \"_pct\"))\nprint(names(cleaned))\n\n[1] \"id\"         \"sales\"      \"q1_revenue\"\n\n\nExplanation: Custom replace swaps # for _num and % for _pct, while clean_names handles the rest—precision meets polish.\n\nlibrary(readxl)\n\n\n# Create a temporary Excel file with problematic column names\ntemp_file &lt;- tempfile(fileext = \".xlsx\")\ndf &lt;- data.frame(\n  `ID#` = 1:5,\n  `%_Completed` = c(85, 92, 78, 100, 65),\n  `Result (Pass/Fail)` = c(\"Pass\", \"Pass\", \"Fail\", \"Pass\", \"Fail\"),\n  `μg/mL` = c(0.5, 0.8, 0.3, 1.2, 0.4),\n  `p-value` = c(0.03, 0.01, 0.08, 0.002, 0.06),\n  check.names = FALSE\n)\n\n# Save as Excel (simulating real-world data source)\nif (require(writexl)) {\n  write_xlsx(df, temp_file)\n} else {\n  # Fall back to CSV if writexl not available\n  write.csv(df, sub(\"\\\\.xlsx$\", \".csv\", temp_file), row.names = FALSE)\n  temp_file &lt;- sub(\"\\\\.xlsx$\", \".csv\", temp_file)\n}\n\n# Read the file back\nif (temp_file == sub(\"\\\\.xlsx$\", \".csv\", temp_file)) {\n  imported_df &lt;- read.csv(temp_file, check.names = FALSE)\n} else {\n  imported_df &lt;- read_excel(temp_file)\n}\n\n# View original column names\nprint(names(imported_df))\n\n[1] \"ID#\"                \"%_Completed\"        \"Result (Pass/Fail)\"\n[4] \"μg/mL\"              \"p-value\"           \n\n# Create custom replacements\ncustom_replacements &lt;- c(\n  \"μg\" = \"ug\",  # Replace Greek letter\n  \"%\" = \"percent\",  # Replace percent symbol\n  \"#\" = \"num\"   # Replace hash\n)\n\n# Clean with custom replacements\nclean_df &lt;- imported_df %&gt;%\n  clean_names() %&gt;%\n  rename_with(~ stringr::str_replace_all(., \"p_value\", \"probability\"))\n\n# View cleaned column names\nprint(names(clean_df))\n\n[1] \"id_number\"         \"percent_completed\" \"result_pass_fail\" \n[4] \"mg_m_l\"            \"probability\"      \n\n# Print the cleaned dataframe\nprint(clean_df)\n\n# A tibble: 5 × 5\n  id_number percent_completed result_pass_fail mg_m_l probability\n      &lt;dbl&gt;             &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n1         1                85 Pass                0.5       0.03 \n2         2                92 Pass                0.8       0.01 \n3         3                78 Fail                0.3       0.08 \n4         4               100 Pass                1.2       0.002\n5         5                65 Fail                0.4       0.06 \n\n\nThe final output shows the transformation from problematic column names to standardized ones:\nFrom:\n\nID#\n%_Completed\nResult (Pass/Fail)\nμg/mL\np-value\n\nTo:\n\nid_num\npercent_completed\nresult_pass_fail\nug_m_l\nprobability\n\nThis example demonstrates how clean_names() can be part of a more sophisticated data preparation workflow, especially when working with real-world data sources that contain problematic characters and naming conventions."
  },
  {
    "objectID": "posts/2025-03-11_underrated_functions/index.html#conclusion-why-these-functions-deserve-your-attention",
    "href": "posts/2025-03-11_underrated_functions/index.html#conclusion-why-these-functions-deserve-your-attention",
    "title": "Underrated Gems in R: Must-Know Functions You’re Probably Missing Out On",
    "section": "Conclusion: Why These Functions Deserve Your Attention",
    "text": "Conclusion: Why These Functions Deserve Your Attention\nR’s ecosystem is vast, but it’s easy to stick to the familiar and miss out on tools like Reduce, vapply, do.call and clean_names. These functions might not top the popularity charts, yet they pack a punch—whether it’s collapsing data without loops, ensuring type safety, adapting on the fly, fixing messy names, or mining text for gold. The examples here show just a taste of what they can do, from quick fixes to complex tasks. Curious to see how they fit into your workflow? Fire up R, play with them, and discover how these underdogs can become your new go-tos. What other hidden R treasures have you found? Drop them in the comments—I’d love to hear!"
  },
  {
    "objectID": "posts/2025-03-11_underrated_functions/index.html#references",
    "href": "posts/2025-03-11_underrated_functions/index.html#references",
    "title": "Underrated Gems in R: Must-Know Functions You’re Probably Missing Out On",
    "section": "References",
    "text": "References\n\nR Core Team (2025). R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing, Vienna, Austria. Available at: https://www.R-project.org/\nFirke, Sam (2023). janitor: Simple Tools for Examining and Cleaning Dirty Data. CRAN. Available at: https://CRAN.R-project.org/package=janitor\nR Documentation for Reduce, vapply, do.call, clean_names."
  },
  {
    "objectID": "posts/2024-12-16_oecd/index.html",
    "href": "posts/2024-12-16_oecd/index.html",
    "title": "Extracting Data from OECD Databases in R: Using the oecd and rsdmx Packages",
    "section": "",
    "text": "The OECD (Organisation for Economic Co-operation and Development) provides extensive databases for economic, social, and environmental indicators. Accessing these programmatically through R is efficient and reproducible. In this article, we explore two popular R packages for accessing OECD data—oecd and rsdmx—and discuss critical updates to the OECD Developer API that have impacted package functionality.\nWe also provide practical examples, emphasize the importance of applying filters during data retrieval, and guide users on how to work with the latest tools to ensure seamless data access."
  },
  {
    "objectID": "posts/2024-12-16_oecd/index.html#introduction",
    "href": "posts/2024-12-16_oecd/index.html#introduction",
    "title": "Extracting Data from OECD Databases in R: Using the oecd and rsdmx Packages",
    "section": "",
    "text": "The OECD (Organisation for Economic Co-operation and Development) provides extensive databases for economic, social, and environmental indicators. Accessing these programmatically through R is efficient and reproducible. In this article, we explore two popular R packages for accessing OECD data—oecd and rsdmx—and discuss critical updates to the OECD Developer API that have impacted package functionality.\nWe also provide practical examples, emphasize the importance of applying filters during data retrieval, and guide users on how to work with the latest tools to ensure seamless data access."
  },
  {
    "objectID": "posts/2024-12-16_oecd/index.html#why-programmatic-access-matters",
    "href": "posts/2024-12-16_oecd/index.html#why-programmatic-access-matters",
    "title": "Extracting Data from OECD Databases in R: Using the oecd and rsdmx Packages",
    "section": "Why Programmatic Access Matters",
    "text": "Why Programmatic Access Matters\nAccessing data programmatically offers several benefits:\n\nCustomization: Tailor requests to retrieve only the data you need (e.g., specific countries, indicators, and years).\nEfficiency: Save time and bandwidth by filtering data before download.\nReproducibility: Ensure that analyses can be easily updated or shared.\nAutomation: Streamline workflows by automating data extraction."
  },
  {
    "objectID": "posts/2024-12-16_oecd/index.html#oecd-data-explorer-exploring-and-accessing-data",
    "href": "posts/2024-12-16_oecd/index.html#oecd-data-explorer-exploring-and-accessing-data",
    "title": "Extracting Data from OECD Databases in R: Using the oecd and rsdmx Packages",
    "section": "OECD Data Explorer: Exploring and Accessing Data",
    "text": "OECD Data Explorer: Exploring and Accessing Data\nThe OECD provides programmatic access to OECD data for OECD countries and selected non-member economies through a RESTful application programming interface (API) based on the SDMX standard. The APIs allow developers to easily query the OECD data in several ways to create innovative software applications which use dynamically updated OECD data.\nThe OECD Data Explorer is an interactive web-based platform that allows users to explore, visualize, and download data from the OECD databases. It is particularly useful for users who want to manually browse through datasets before deciding on specific data points for analysis. Here, we provide an overview of the OECD Data Explorer, including how to navigate the platform, customize filters, and access API links for programmatic use.\nThe OECD Data Explorer is available at: https://data-explorer.oecd.org/\n\n\n\n\n\nWhen you visit the site, you are greeted with a clean interface for navigating through datasets. The platform organizes data into themes such as;\n\nEconomy\nEducation\nEnvironment\nHealth\nInnovation and Technology\nEmployment\n\nEach theme contains various datasets that can be explored interactively.\n\nUsing the OECD Data Explorer\n\n1. Search for a Dataset\nThe search bar allows you to quickly locate datasets. For example, if you are interested in unemployment data, simply type “unemployment” in the search bar.\n\n\n2. Customize Filters\nOnce you’ve selected a dataset (e.g., Labour Market Statistics), you can apply various filters to narrow down the data you need. Some of them are given below:\n\nGeographical Region: Choose specific countries or regions (e.g., USA, France, OECD Total).\nTime Period: Select years of interest (e.g., 2015–2023).\nIndicator: Specify what you are analyzing (e.g., Unemployment Rate, Employment-to-Population Ratio).\nMeasurement Units: Choose relevant units (e.g., percentages, index values).\n\n\n\n3. Explore Data Visualizations\nThe platform provides instant visualizations, such as tables, line charts, and bar charts, based on your selected filters. These visualizations make it easy to understand trends and patterns in the data.\n\n\n4. Exporting Data\nOnce you’ve customized the dataset, you can download in available formats, such as Excel or CSV by manually. the other choice is accessing the API Link. For programmatic access, the OECD Data Explorer provides API links that can be used in R or other programming languages. After selecting your filters, click on the Developer API and copy the generated link.\nFor example, let’s want to pull data about the unemployment rates of some countries. After applying the filters I want, such a link will be created.\nhttps://sdmx.oecd.org/public/rest/data/OECD.SDD.TPS,DSD_LFS@DF_IALFS_UNE_M,1.0/BEL+AUS+AUT+CAN+DNK+FRA+DEU+GRC+HUN+IRL+ITA+JPN+NLD+NZL+NOR+PRT+SVN+ESP+SWE+CHE+USA+GBR+TUR..PT_LF_SUB._Z.Y._T.Y_GE15..M?startPeriod=2023-11&dimensionAtObservation=AllDimensions\nThis link can be directly used with R packages like rsdmx to fetch data programmatically.\n\n\n\n\n\nAlso you can get detailed information from https://www.oecd.org/en/data/insights/data-explainers/2024/09/api.html. This page provides detailed information on how to programmatically retrieve data from the OECD Data Explorer via the API."
  },
  {
    "objectID": "posts/2024-12-16_oecd/index.html#the-oecd-package-accessing-oecd-data-in-r",
    "href": "posts/2024-12-16_oecd/index.html#the-oecd-package-accessing-oecd-data-in-r",
    "title": "Extracting Data from OECD Databases in R: Using the oecd and rsdmx Packages",
    "section": "The OECD Package: Accessing OECD Data in R",
    "text": "The OECD Package: Accessing OECD Data in R\nThe oecd package is an R package designed to provide a convenient interface for accessing data from the OECD Developer API. It allows users to:\n\nExplore available datasets in the OECD databases.\nRetrieve filtered data programmatically for specific countries, indicators, and time periods.\nWork with data in a reproducible way directly within R.\n\nHowever, the version of the OECD package available on CRAN is currently outdated due to recent changes in the OECD API (2024). These changes have impacted the functionality of some key features in the CRAN release. You can find more information about changes in the OECD API from https://www.oecd.org/en/data/insights/data-explainers/2024/09/OECD-DE-FAQ.html.\nTo overcome these limitations, it is recommended to use the updated version of the OECDpackage available on GitHub, which is fully compatible with the latest OECD API.\nFor installation and usage details, refer to the updated package repository:\nhttps://github.com/expersso/OECD\nInstalling the Updated oecd Package:\n\n# Install devtools if not already installed\ninstall.packages(\"devtools\")\n\n# Install the updated oecd package from GitHub\ndevtools::install_github(\"expersso/OECD\")\n\nThe updated version of the OECDpackage simplifies interaction with the OECD API, focusing on just two core functions: get_data_structure() and get_dataset(). Here’s a brief overview of their functionality and arguments:\n\n1. get_data_structure()\nThis function retrieves metadata about a specific dataset from the OECD API. It provides information about variables, classifications, adjustments, unit measures etc. For example, we can access this information about the unemployment rates of some countries by taking the code of the relevant data set from the link given above. Then we can extract dataset information from the link we received from the developer API section, starting with slash (/) after the data expression and up to the next slash (Shown in blue in screenshot).\n\n\n\n\n\n\nlibrary(OECD)\ndataset_unemprate &lt;- \"OECD.SDD.TPS,DSD_LFS@DF_IALFS_UNE_M,1.0\"\ndata_str &lt;- get_data_structure(dataset_unemprate)\nstr(data_str, max.level = 1)\n\nList of 15\n $ VAR_DESC               :'data.frame':    17 obs. of  2 variables:\n $ CL_ACTIVITY_ISIC4      :'data.frame':    958 obs. of  2 variables:\n $ CL_ADJUSTMENT          :'data.frame':    17 obs. of  2 variables:\n $ CL_AGE                 :'data.frame':    308 obs. of  2 variables:\n $ CL_AREA                :'data.frame':    469 obs. of  2 variables:\n $ CL_SECTOR              :'data.frame':    216 obs. of  2 variables:\n $ CL_SEX                 :'data.frame':    7 obs. of  2 variables:\n $ CL_TRANSFORMATION      :'data.frame':    59 obs. of  2 variables:\n $ CL_UNIT_MEASURE        :'data.frame':    670 obs. of  2 variables:\n $ CL_WORKER_STATUS_ICSE93:'data.frame':    13 obs. of  2 variables:\n $ CL_MEASURE_LFS_TPS     :'data.frame':    30 obs. of  2 variables:\n $ CL_DECIMALS            :'data.frame':    16 obs. of  2 variables:\n $ CL_FREQ                :'data.frame':    34 obs. of  2 variables:\n $ CL_OBS_STATUS          :'data.frame':    20 obs. of  4 variables:\n $ CL_UNIT_MULT           :'data.frame':    31 obs. of  4 variables:\n\n\n\n\n2. get_dataset()\nThis function retrieves the actual data from a specified dataset, with optional filters for dimensions like country, time, and indicators.\n\nget_dataset(\n  dataset,\n  filter = NULL,\n  start_time = NULL,\n  end_time = NULL,\n  last_n_observations = NULL,\n  ...\n)\n\nFor filters, you need to start with “/” after the part for dataset and take it until question mark “?”. But be careful, don’t include question mark. For the time filtering, start_time or end_time arguments can be used.\n\n\n\n\n\n\ndata_filters_unemprate &lt;- \"BEL+AUS+AUT+CAN+DNK+FRA+DEU+GRC+HUN+IRL+ITA+JPN+NLD+NZL+NOR+PRT+SVN+ESP+SWE+CHE+USA+GBR+TUR..PT_LF_SUB._Z.Y._T.Y_GE15..M\"\n\ndf &lt;- get_dataset(dataset = dataset_unemprate,\n                  filter = data_filters_unemprate,\n                  start_time = 2014)\n\nhead(df)\n\n  ACTIVITY ADJUSTMENT    AGE DECIMALS FREQ  MEASURE OBS_STATUS ObsValue\n1       _Z          Y Y_GE15        1    M UNE_LF_M          A      3.6\n2       _Z          Y Y_GE15        1    M UNE_LF_M          A      3.7\n3       _Z          Y Y_GE15        1    M UNE_LF_M          A      5.5\n4       _Z          Y Y_GE15        1    M UNE_LF_M          A      5.6\n5       _Z          Y Y_GE15        1    M UNE_LF_M          A      5.7\n6       _Z          Y Y_GE15        1    M UNE_LF_M          A      4.3\n  REF_AREA SEX TIME_PERIOD TRANSFORMATION UNIT_MEASURE UNIT_MULT\n1      USA  _T     2019-06             _Z    PT_LF_SUB         0\n2      USA  _T     2019-07             _Z    PT_LF_SUB         0\n3      BEL  _T     2024-06             _Z    PT_LF_SUB         0\n4      BEL  _T     2024-07             _Z    PT_LF_SUB         0\n5      BEL  _T     2024-08             _Z    PT_LF_SUB         0\n6      DEU  _T     2015-08             _Z    PT_LF_SUB         0"
  },
  {
    "objectID": "posts/2024-12-16_oecd/index.html#using-the-rsdmx-package",
    "href": "posts/2024-12-16_oecd/index.html#using-the-rsdmx-package",
    "title": "Extracting Data from OECD Databases in R: Using the oecd and rsdmx Packages",
    "section": "Using the rsdmx Package",
    "text": "Using the rsdmx Package\nThe rsdmx package allows interaction with the OECD Developer API through SDMX format. It is particularly useful if you prefer working directly with API URLs.\n\nInstalling the rsdmx Package\n\ninstall.packages(\"rsdmx\")\n\n\nKey Functions in rsdmx\n\nreadSDMX(): Fetches data from an SDMX-compatible API endpoint.\nas.data.frame(): Converts the retrieved SDMX object into a data frame.\n\n\n\nExample Workflow with rsdmx\nHere’s how you can retrieve unemployment data:\n\n# Load the rsdmx package\nlibrary(rsdmx)\n\nWarning: package 'rsdmx' was built under R version 4.3.3\n\n# Define the API URL for unemployment rates\noecd_url &lt;- \"https://sdmx.oecd.org/public/rest/data/OECD.SDD.TPS,DSD_LFS@DF_IALFS_UNE_M,1.0/BEL+AUS+AUT+CAN+DNK+FRA+DEU+GRC+HUN+IRL+ITA+JPN+NLD+NZL+NOR+PRT+SVN+ESP+SWE+CHE+USA+GBR+TUR..PT_LF_SUB._Z.Y._T.Y_GE15..M?startPeriod=2023-11&dimensionAtObservation=AllDimensions\"\n\n# Step 1: Fetch the data\nunemployment_data &lt;- readSDMX(oecd_url)\n\n# Step 2: Convert to a data frame\nunemployment_df &lt;- as.data.frame(unemployment_data)\n\n# View the data\nhead(unemployment_df)\n\n  TIME_PERIOD REF_AREA  MEASURE UNIT_MEASURE TRANSFORMATION ADJUSTMENT SEX\n1     2024-03      ITA UNE_LF_M    PT_LF_SUB             _Z          Y  _T\n2     2024-04      ITA UNE_LF_M    PT_LF_SUB             _Z          Y  _T\n3     2024-08      BEL UNE_LF_M    PT_LF_SUB             _Z          Y  _T\n4     2024-09      BEL UNE_LF_M    PT_LF_SUB             _Z          Y  _T\n5     2024-10      BEL UNE_LF_M    PT_LF_SUB             _Z          Y  _T\n6     2023-12      SVN UNE_LF_M    PT_LF_SUB             _Z          Y  _T\n     AGE ACTIVITY FREQ obsValue UNIT_MULT DECIMALS OBS_STATUS\n1 Y_GE15       _Z    M      6.9         0        1          A\n2 Y_GE15       _Z    M      6.7         0        1          A\n3 Y_GE15       _Z    M      5.7         0        1          A\n4 Y_GE15       _Z    M      5.8         0        1          A\n5 Y_GE15       _Z    M      5.8         0        1          A\n6 Y_GE15       _Z    M      3.4         0        1          A"
  },
  {
    "objectID": "posts/2024-12-16_oecd/index.html#conclusion",
    "href": "posts/2024-12-16_oecd/index.html#conclusion",
    "title": "Extracting Data from OECD Databases in R: Using the oecd and rsdmx Packages",
    "section": "Conclusion",
    "text": "Conclusion\nBoth oecd and rsdmx allow you to specify filters directly in your API request, which is critical for:\n\nTime Efficiency: Smaller, focused datasets download faster.\nStorage Optimization: Filtering minimizes the size of the retrieved dataset.\nSimpler Analysis: Pre-filtered data reduces the need for extensive preprocessing.\n\nWhen working with OECD databases in R, the updated version of the oecd package (available on GitHub) is a reliable choice, provided you install it from its GitHub repository. If you prefer working directly with API URLs, the rsdmx package is another strong option.\nRegardless of the package, applying filters in your data requests is essential to ensure efficiency and reproducibility. By integrating these tools into your workflow, you can access OECD data programmatically and focus on the analysis itself."
  },
  {
    "objectID": "posts/2024-12-16_oecd/index.html#references",
    "href": "posts/2024-12-16_oecd/index.html#references",
    "title": "Extracting Data from OECD Databases in R: Using the oecd and rsdmx Packages",
    "section": "References",
    "text": "References\n\nOECD Data Explorer\nOECD Data via API\nUpdated oecd Package on GitHub\nrsdmx Package Documentation\nOECD Data API documentation\nUpgrading your queries from the legacy OECD.Stat APIs to the new OECD Data API"
  },
  {
    "objectID": "posts/2024-09-30_lubridate/index.html",
    "href": "posts/2024-09-30_lubridate/index.html",
    "title": "Mastering Date and Time Data in R with lubridate",
    "section": "",
    "text": "Artwork by: Allison Horst"
  },
  {
    "objectID": "posts/2024-09-30_lubridate/index.html#what-is-lubridate",
    "href": "posts/2024-09-30_lubridate/index.html#what-is-lubridate",
    "title": "Mastering Date and Time Data in R with lubridate",
    "section": "What is lubridate?",
    "text": "What is lubridate?\nlubridate is a powerful and widely-used package in the tidyverse ecosystem, specifically designed for making date-time manipulation in R both easier and more intuitive. It was created to address the common difficulties users face when working with dates and times, which are often stored in a variety of inconsistent formats or require complex arithmetic operations.\nDeveloped and maintained by the RStudio team as part of the tidyverse collection of packages, lubridate introduces a simpler syntax for parsing, extracting, and manipulating date-time data, allowing for faster and more accurate operations.\nKey benefits of using lubridate include:\n\nSimplified parsing of dates and times from a wide variety of formats.\nEasy extraction of components such as year, month, day, or hour from date-time objects.\nSeamless handling of time zones, allowing conversion between different zones with ease.\nEfficient arithmetic operations on dates, such as adding or subtracting days, months, or years.\nSupport for durations and intervals, crucial for working with time spans in real-world applications.\n\nFor further documentation, tutorials, and resources, you can explore the lubridate official website: https://lubridate.tidyverse.org."
  },
  {
    "objectID": "posts/2024-09-30_lubridate/index.html#introduction-to-date-and-time-formats",
    "href": "posts/2024-09-30_lubridate/index.html#introduction-to-date-and-time-formats",
    "title": "Mastering Date and Time Data in R with lubridate",
    "section": "Introduction to Date and Time Formats",
    "text": "Introduction to Date and Time Formats\nDate and time data are essential in many fields, from finance and biology to web analytics and logistics. However, handling such data can be difficult due to the variety of formats and time zones involved. In R, base functions like as.Date() or strptime() can handle date-time data, but their syntax can be cumbersome when dealing with multiple formats or time zones.\nThe lubridate package simplifies these tasks by offering intuitive functions that handle date-time data efficiently, helping us avoid many of the common pitfalls associated with date and time manipulation."
  },
  {
    "objectID": "posts/2024-09-30_lubridate/index.html#why-do-we-need-lubridate",
    "href": "posts/2024-09-30_lubridate/index.html#why-do-we-need-lubridate",
    "title": "Mastering Date and Time Data in R with lubridate",
    "section": "Why Do We Need lubridate?",
    "text": "Why Do We Need lubridate?\nWhile R provides several built-in functions for date-time manipulation, they can quickly become limited or difficult to use in more complex scenarios. The lubridate package provides solutions by:\n\nOffering intuitive functions to parse and format dates.\nSupporting a variety of date-time formats in a single command.\nSimplifying the extraction and modification of date-time components (like year, month, or hour).\nFacilitating the handling of time zones, durations, and intervals."
  },
  {
    "objectID": "posts/2024-09-30_lubridate/index.html#date-and-time-formats-in-r",
    "href": "posts/2024-09-30_lubridate/index.html#date-and-time-formats-in-r",
    "title": "Mastering Date and Time Data in R with lubridate",
    "section": "Date and Time Formats in R",
    "text": "Date and Time Formats in R\nIn R, dates are typically stored in Date format (which does not include time information), while date-time data is stored in POSIXct or POSIXlt formats. These formats support timestamps and can handle time zones. For example:\n\ndate_example &lt;- as.Date(\"2024-09-30\")\ndate_example\n\n[1] \"2024-09-30\"\n\ndatetime_example &lt;- as.POSIXct(\"2024-09-30 14:45:00\", tz = \"UTC\")\ndatetime_example\n\n[1] \"2024-09-30 14:45:00 UTC\"\n\n\nThese formats work well for simple tasks but quickly become difficult to manage in more complex scenarios. That’s where lubridate steps in."
  },
  {
    "objectID": "posts/2024-09-30_lubridate/index.html#common-lubridate-functions-and-their-arguments",
    "href": "posts/2024-09-30_lubridate/index.html#common-lubridate-functions-and-their-arguments",
    "title": "Mastering Date and Time Data in R with lubridate",
    "section": "Common lubridate Functions and Their Arguments",
    "text": "Common lubridate Functions and Their Arguments\n\nParsing Dates and Times\nOne of the core strengths of lubridate is its ability to simplify the parsing of date and time data from various formats. Functions like ymd(), mdy(), dmy(), and their date-time counterparts (ymd_hms(), mdy_hms(), etc.) make it easy to convert strings into R’s Date or POSIXct objects.\n\nWhat do the letters y, m, d stand for?\nThe functions are named according to the order in which the date components appear in the input string:\n\ny stands for year\nm stands for month\nd stands for day\nh, m, s (used in date-time functions) stand for hours, minutes, and seconds\n\nFor example:\n\nymd() parses a string where the date components are in the order year-month-day.\nmdy() parses a string formatted as month-day-year.\ndmy() parses a string in day-month-year order.\n\n\n\nFunctions: ymd(), mdy(), dmy(), ymd_hms(), mdy_hms(), dmy_hms()\n\n\nlibrary(lubridate)\n\n# Convert date strings to Date objects\ndate1 &lt;- ymd(\"2024-09-30\")\ndate1\n\n[1] \"2024-09-30\"\n\ndate2 &lt;- dmy(\"30-09-2024\")\ndate2\n\n[1] \"2024-09-30\"\n\ndate3 &lt;- mdy(\"09/30/2024\")\ndate3\n\n[1] \"2024-09-30\"\n\n# Convert to date-time\ndatetime1 &lt;- ymd_hms(\"2024-09-21 14:45:00\", tz = \"UTC\")\ndatetime1\n\n[1] \"2024-09-21 14:45:00 UTC\"\n\ndatetime2 &lt;- mdy_hms(\"09/21/2024 02:45:00 PM\", tz = \"America/New_York\")\ndatetime2\n\n[1] \"2024-09-21 14:45:00 EDT\"\n\n\nBy using specific functions for different formats (ymd(), mdy(), dmy()), you don’t need to worry about the order of date components. This ensures flexibility and reduces errors when working with various data sources.\nThese functions simplify the process by allowing you to focus only on the structure of the input data and not on specifying complex format strings, as would be necessary with base R functions like as.Date() or strptime().\n\n\n\nExtracting Date-Time Components\nOnce you have parsed a date-time object using lubridate, you often need to extract or modify specific components, such as the year, month, day, or time. This is essential when analyzing data based on time periods, summarizing by year, or creating time-based features for models.\nFunctions to Extract Date-Time Components\nHere are the most commonly used lubridate functions to extract specific parts of a date-time object:\n\nyear(): Extracts or sets the year.\nmonth(): Extracts or sets the month. This function can also return the month’s name if label = TRUE is used.\nday(): Extracts or sets the day of the month.\nhour(): Extracts or sets the hour (for time-based objects).\nminute(): Extracts or sets the minute.\nsecond(): Extracts or sets the second.\nwday(): Extracts the day of the week (can return the weekday’s name if label = TRUE).\nyday(): Extracts the day of the year (1–365 or 366 for leap years).\nmday(): Extracts the day of the month.\n\nLet’s work with a parsed date-time object and extract its components:\n\nlibrary(lubridate)\n\n# Parsing a date-time object\ndatetime &lt;- ymd_hms(\"2024-09-30 14:45:30\")\n\n# Extracting components\nyear(datetime)\n\n[1] 2024\n\nmonth(datetime) \n\n[1] 9\n\nday(datetime) \n\n[1] 30\n\nhour(datetime) \n\n[1] 14\n\nminute(datetime)\n\n[1] 45\n\nsecond(datetime)\n\n[1] 30\n\n# Extracting weekday\nwday(datetime)\n\n[1] 2\n\nwday(datetime, label = TRUE)\n\n[1] Mon\nLevels: Sun &lt; Mon &lt; Tue &lt; Wed &lt; Thu &lt; Fri &lt; Sat\n\n\nIn this example, we extracted different components of the date-time object. The wday() function can return the day of the week either as a number (1 for Sunday, 7 for Saturday) or as a label (the weekday name) when using label = TRUE.\nIn addition to extraction, lubridate allows you to modify specific components of a date or time without manually manipulating the entire string. This is particularly useful when you need to adjust dates or times in your data for analysis or alignment.\n\n# Modifying components\ndatetime\n\n[1] \"2024-09-30 14:45:30 UTC\"\n\nyear(datetime) &lt;- 2025\nmonth(datetime) &lt;- 12\nhour(datetime) &lt;- 8\n\ndatetime\n\n[1] \"2025-12-30 08:45:30 UTC\"\n\n\nIn this example, the original date-time 2024-09-30 14:45:30 was modified to change the year, month, and hour, resulting in a new date-time value of 2025-12-21 08:45:30.\nlubridate allows you to extract and modify months or weekdays by name as well, which is particularly useful when working with human-readable data or when creating reports:\n\n# Extracting month by name\nmonth(datetime, label = TRUE, abbr = FALSE)\n\n[1] December\n12 Levels: January &lt; February &lt; March &lt; April &lt; May &lt; June &lt; ... &lt; December\n\n# Changing the month by name\nmonth(datetime) &lt;- 7\ndatetime\n\n[1] \"2025-07-30 08:45:30 UTC\"\n\n\nIn this example, label = TRUE and abbr = FALSE give the full name of the month (July) instead of the numeric value or abbreviation. You can also modify the month by name for more human-readable processing.\nFor higher-level time units such as weeks and quarters, lubridate offers convenient functions:\n\nweek(): Extracts the week of the year (1–52/53).\nquarter(): Extracts the quarter of the year (1–4).\n\n\n# Extracting the week number\nweek(datetime)\n\n[1] 31\n\n# Extracting the quarter\nquarter(datetime)\n\n[1] 3\n\n\n\n\nDealing with Time Zones\nAnother significant advantage of lubridate is that it handles time zones effectively when extracting date-time components. If you work with global datasets, being able to accurately account for time zones is crucial:\n\n# Set a different time zone\ndatetime\n\n[1] \"2025-07-30 08:45:30 UTC\"\n\ndatetime_tz &lt;- with_tz(datetime, \"America/New_York\")\ndatetime_tz\n\n[1] \"2025-07-30 04:45:30 EDT\"\n\n# Extract hour in the new time zone\nhour(datetime_tz)\n\n[1] 4\n\n\nHere, we changed the time zone to Eastern Daylight Time (EDT) and extracted the hour component, which adjusted to the new time zone.\n\n\nCreating Durations, Periods, and Intervals\nIn data analysis, we often need to measure time spans, whether to calculate the difference between two dates, schedule recurring events, or model time-based phenomena. lubridate offers three powerful time-related concepts to handle these scenarios: durations, periods, and intervals. While they may seem similar, they each serve distinct purposes and behave differently depending on the use case.\n\nDurations\nA duration is an exact measurement of time, expressed in seconds. Durations are useful when you need precise, unambiguous time differences regardless of calendar variations (such as leap years, varying month lengths, or daylight saving changes).\n\nDuration syntax: You can create durations using the dseconds(), dminutes(), dhours(), ddays(), dweeks(), dyears() functions.\n\n\n# Creating a duration of 1 day\none_day &lt;- ddays(1)\none_day\n\n[1] \"86400s (~1 days)\"\n\n# Duration of 2 hours and 30 minutes\nduration_time &lt;- dhours(2) + dminutes(30)\nduration_time\n\n[1] \"9000s (~2.5 hours)\"\n\n# Adding a duration to a date\nstart_date &lt;- ymd(\"2024-09-30\")\nend_date &lt;- start_date + ddays(7)\nend_date\n\n[1] \"2024-10-07\"\n\n\nIn this example, durations are defined as fixed time lengths. Adding a duration to a date will move the date forward by the exact number of seconds, regardless of any irregularities in the calendar.\n\n\nPeriods\nUnlike durations, periods are time spans measured in human calendar terms: years, months, days, hours, etc. Periods account for calendar variations, such as leap years and daylight saving time. This makes periods more intuitive for real-world use cases, but less precise in terms of exact seconds.\n\nPeriod syntax: Use years(), months(), weeks(), days(), hours(), minutes(), seconds() functions to create periods.\n\n\n# Creating a period of 2 years, 3 months, and 10 days\nmy_period &lt;- years(2) + months(3) + days(10)\nmy_period \n\n[1] \"2y 3m 10d 0H 0M 0S\"\n\n# Adding the period to a date\nnew_date &lt;- start_date + my_period\nnew_date\n\n[1] \"2027-01-09\"\n\n\nIn this example, the period accounts for differences in calendar length (such as varying days in months). The start_date was 2024-09-30, and after adding 2 years, 3 months, and 10 days, the result is 2027-01-09.\n\n\nIntervals\nAn interval represents the time span between two specific dates or times. It is useful when you want to measure or compare spans between known start and end points. Intervals take into account the exact length of time between two dates, allowing you to calculate durations or periods over that span.\n\nInterval syntax: Use the interval() function to create an interval between two dates or date-times.\n\n\n# Creating an interval between two dates\nstart_date &lt;- ymd(\"2024-01-01\")\nend_date &lt;- ymd(\"2024-12-31\")\ntime_interval &lt;- interval(start_date, end_date)\ntime_interval\n\n[1] 2024-01-01 UTC--2024-12-31 UTC\n\n# Checking how many days/weeks are in the interval\nas.duration(time_interval)\n\n[1] \"31536000s (~52.14 weeks)\"\n\n\nIn this example, an interval is created between 2024-01-01 and 2024-12-31. The interval accounts for the exact time between the two dates, and using as.duration() allows us to calculate the number of seconds (or days/weeks) in that interval.\nSometimes you need to combine these time spans to perform calculations or model time-based processes. For example, you might want to measure the duration of an interval and adjust it using a period.\n\n# Create an interval between two dates\nstart_date &lt;- ymd(\"2024-09-01\")\nend_date &lt;- ymd(\"2024-12-01\")\ninterval_span &lt;- interval(start_date, end_date)\ninterval_span\n\n[1] 2024-09-01 UTC--2024-12-01 UTC\n\n# Extend the end date by 1 month\nnew_end_date &lt;- end_date + months(1)\n\n# Create a new interval with the updated end date\nextended_interval &lt;- interval(start_date, new_end_date)\n\n# Display the extended interval\nextended_interval\n\n[1] 2024-09-01 UTC--2025-01-01 UTC\n\n\n\nOriginal interval: We first create the interval interval_span between 2024-09-01 and 2024-12-01.\nAdding 1 month: Instead of adding the period to the interval directly, we add months(1) to the end date (end_date + months(1)).\nNew interval: We then create a new interval using the original start date and the updated end date (new_end_date).\n\n\n\n\nDate Arithmetic\nDate arithmetic is a fundamental aspect of working with date-time data, especially in data analysis and time series forecasting. The lubridate package makes it easy to perform arithmetic operations on date-time objects, enabling users to manipulate dates effectively. This section discusses common date arithmetic operations, including adding and subtracting time intervals, calculating durations, and handling periods.\nYou can perform basic arithmetic operations directly on date-time objects. These operations include addition and subtraction of various time intervals.\nAdding Days to a Date:\n\n# Define a starting date\nstart_date &lt;- ymd(\"2024-01-01\")\n\n# Add 30 days to the starting date\nnew_date &lt;- start_date + days(30)\n\n# Display the new date\nnew_date\n\n[1] \"2024-01-31\"\n\n\nIn this example:\n\nWe define a starting date using ymd().\nWe add 30 days to this date using the days() function.\nThe result is a new date that is 30 days later.\n\nSubtracting Days from a Date:\n\n# Subtract 15 days from the starting date\nprevious_date &lt;- start_date - days(15)\n\n# Display the previous date\nprevious_date\n\n[1] \"2023-12-17\"\n\n\nHere, we demonstrate how to subtract days from a date. This operation can also be performed with other time intervals, such as months, years, hours, etc.\nDate arithmetic is commonly used in various practical applications, such as:\n\nTime Series Analysis: Analyzing trends over specific periods (e.g., monthly sales growth).\nEvent Planning: Calculating the duration between events (e.g., project deadlines).\nScheduling: Determining time slots for meetings or tasks based on calendar events.\n\n\n# Define task durations\ntask_duration &lt;- hours(3)  # Each task takes 3 hours\nstart_time &lt;- ymd_hms(\"2024-01-01 09:00:00\")\n\n# Schedule three tasks\nschedule &lt;- start_time + task_duration * 0:2\n\n# Display the schedule for tasks\nschedule\n\n[1] \"2024-01-01 09:00:00 UTC\" \"2024-01-01 12:00:00 UTC\"\n[3] \"2024-01-01 15:00:00 UTC\"\n\n\nIn this example, we define a 3-hour task duration and schedule three tasks based on the start time, displaying their scheduled times."
  },
  {
    "objectID": "posts/2024-09-30_lubridate/index.html#using-lubridate-with-time-series-data-in-r",
    "href": "posts/2024-09-30_lubridate/index.html#using-lubridate-with-time-series-data-in-r",
    "title": "Mastering Date and Time Data in R with lubridate",
    "section": "Using lubridate with Time Series Data in R",
    "text": "Using lubridate with Time Series Data in R\nIn time series analysis, properly handling date and time variables is crucial for ensuring accurate results. lubridate simplifies working with dates and times, but it’s also important to know how to integrate it with base R’s time series objects like ts and more flexible formats like date-time data frames.\n\nCreating Time Series with ts() in R\nBase R’s ts function is typically used to create regular time series objects. Time series data must have a defined frequency (e.g., daily, monthly, quarterly) and a starting point.\n\n# Sample data: monthly sales from 2020 to 2022\nsales_data &lt;- c(100, 120, 150, 170, 160, 130, 140, 180, 200, 190, 210, 220,\n                230, 250, 270, 300, 280, 260, 290, 310, 330, 340, 350, 360)\n\n# Creating a time series object (monthly data starting from Jan 2020)\nts_sales &lt;- ts(sales_data, start = c(2020, 1), frequency = 12)\nts_sales\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n2020 100 120 150 170 160 130 140 180 200 190 210 220\n2021 230 250 270 300 280 260 290 310 330 340 350 360\n\n\nThis code creates a time series object representing monthly sales from January 2020 to December 2021.\n\nstart = c(2020, 1) indicates the time series starts in January 2020.\nfrequency = 12 specifies that the data is monthly (12 periods per year).\n\n\n\nConverting a ts Object to a Data Frame with a Date Variable\nWhen working with time series data, we often need to convert a ts object into a data frame to analyze it along with specific dates. lubridate can be used to handle date conversions easily.\n\n# Convert time series to a data frame with date information\nsales_df &lt;- data.frame(\n  date = seq(ymd(\"2020-01-01\"), by = \"month\", length.out = length(ts_sales)),\n  sales = as.numeric(ts_sales)\n)\n\n# Display the resulting data frame\nsales_df\n\n         date sales\n1  2020-01-01   100\n2  2020-02-01   120\n3  2020-03-01   150\n4  2020-04-01   170\n5  2020-05-01   160\n6  2020-06-01   130\n7  2020-07-01   140\n8  2020-08-01   180\n9  2020-09-01   200\n10 2020-10-01   190\n11 2020-11-01   210\n12 2020-12-01   220\n13 2021-01-01   230\n14 2021-02-01   250\n15 2021-03-01   270\n16 2021-04-01   300\n17 2021-05-01   280\n18 2021-06-01   260\n19 2021-07-01   290\n20 2021-08-01   310\n21 2021-09-01   330\n22 2021-10-01   340\n23 2021-11-01   350\n24 2021-12-01   360\n\n\nIn this example, we:\n\nConvert the ts object to a numeric vector (as.numeric(ts_sales)).\nUse seq() and lubridate’s ymd() function to create a sequence of dates starting from \"2020-01-01\", incrementing monthly (by = \"month\").\nThe result is a data frame with a date column containing actual dates and a sales column with the sales data.\n\n\n\nCreating Time Series from Date-Time Data\nTime series data can also be created directly from date-time information, such as daily, hourly, or minute-based data. lubridate can be used to efficiently generate or manipulate such time series.\n\n# Generate a sequence of daily dates\ndaily_dates &lt;- seq(ymd(\"2023-01-01\"), by = \"day\", length.out = 30)\n\n# Create a sample dataset with random values for each day\ndaily_data &lt;- data.frame(\n  date = daily_dates,\n  value = runif(30, min = 100, max = 200)\n)\n\n# View the first few rows of the dataset\nhead(daily_data)\n\n        date    value\n1 2023-01-01 193.6868\n2 2023-01-02 185.3293\n3 2023-01-03 176.3545\n4 2023-01-04 110.1109\n5 2023-01-05 181.5913\n6 2023-01-06 114.3141\n\n\nIn this example, we create a time series dataset for daily data:\n\nymd() is used to generate a sequence of daily dates starting from \"2023-01-01\".\nrunif() generates random values to simulate daily observations.\n\nYou can use this type of time series in various analysis techniques, including plotting trends over time or aggregating data by week, month, or year.\n\n\nWorking with Time Series Intervals\nSometimes, you need to manipulate time series data by grouping or splitting it into different intervals. lubridate makes this task easier by providing intuitive functions to work with intervals, durations, and periods.\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.3.3\n\n# Sample dataset: daily values over one month\nset.seed(123)\ntime_series_data &lt;- data.frame(\n  date = seq(ymd(\"2023-01-01\"), by = \"day\", length.out = 30),\n  value = runif(30, min = 50, max = 150)\n)\n\n# Aggregating the data by week\nweekly_data &lt;- time_series_data |&gt; \n  mutate(week = floor_date(date, \"week\")) |&gt; \n  group_by(week) |&gt; \n  summarize(weekly_avg = mean(value))\n\n# View the aggregated data\nweekly_data\n\n# A tibble: 5 × 2\n  week       weekly_avg\n  &lt;date&gt;          &lt;dbl&gt;\n1 2023-01-01      105. \n2 2023-01-08      115. \n3 2023-01-15       99.5\n4 2023-01-22      119. \n5 2023-01-29       71.8\n\n\nHere, we use lubridate’s floor_date() function to round each date down to the start of its respective week. The data is then grouped by week and summarized to compute the weekly average. This approach can easily be adapted for other time periods like months or quarters using floor_date(date, \"month\").\n\n\nHandling Irregular Time Series\nNot all time series data comes in regular intervals (e.g., daily, weekly). For irregular time series, lubridate can be used to efficiently handle missing or irregular dates.\n\n# Example of irregular dates (missing some days)\nirregular_dates &lt;- c(ymd(\"2023-01-01\"), ymd(\"2023-01-02\"), ymd(\"2023-01-05\"),\n                     ymd(\"2023-01-07\"), ymd(\"2023-01-10\"))\n\n# Create a dataset with missing dates\nirregular_data &lt;- data.frame(\n  date = irregular_dates,\n  value = runif(5, min = 100, max = 200)\n)\n\n# Complete the time series by filling missing dates\ncomplete_dates &lt;- data.frame(\n  date = seq(min(irregular_data$date), max(irregular_data$date), by = \"day\")\n)\n\n# Join the original data with the complete sequence of dates\ncomplete_data &lt;- merge(complete_dates, irregular_data, by = \"date\", all.x = TRUE)\n\n# View the completed data with missing values\ncomplete_data\n\n         date    value\n1  2023-01-01 196.3024\n2  2023-01-02 190.2299\n3  2023-01-03       NA\n4  2023-01-04       NA\n5  2023-01-05 169.0705\n6  2023-01-06       NA\n7  2023-01-07 179.5467\n8  2023-01-08       NA\n9  2023-01-09       NA\n10 2023-01-10 102.4614\n\n\nIn this example:\n\nlubridate’s ymd() is used to handle irregular dates.\nWe fill missing dates by generating a complete sequence of dates (seq()) and merging it with the original data using merge().\nMissing values are introduced in the value column for dates that were absent in the original data.\n\n\n\nUsing Time Series Formats with lubridate Functions\nYou can combine lubridate functions with base R’s ts objects for more flexible time series analysis. For example, extracting specific components from a ts series, such as year, month, or week, can be achieved using lubridate.\n\n# Converting a ts object to a data frame with dates\nts_data &lt;- ts(sales_data, start = c(2020, 1), frequency = 12)\n\n# Create a data frame from the ts object\ndf_ts &lt;- data.frame(\n  date = seq(ymd(\"2020-01-01\"), by = \"month\", length.out = length(ts_data)),\n  sales = as.numeric(ts_data)\n)\n\n# Extract year and month using lubridate\ndf_ts &lt;- df_ts %&gt;%\n  mutate(year = year(date), month = month(date))\n\n# View the data with extracted components\ndf_ts\n\n         date sales year month\n1  2020-01-01   100 2020     1\n2  2020-02-01   120 2020     2\n3  2020-03-01   150 2020     3\n4  2020-04-01   170 2020     4\n5  2020-05-01   160 2020     5\n6  2020-06-01   130 2020     6\n7  2020-07-01   140 2020     7\n8  2020-08-01   180 2020     8\n9  2020-09-01   200 2020     9\n10 2020-10-01   190 2020    10\n11 2020-11-01   210 2020    11\n12 2020-12-01   220 2020    12\n13 2021-01-01   230 2021     1\n14 2021-02-01   250 2021     2\n15 2021-03-01   270 2021     3\n16 2021-04-01   300 2021     4\n17 2021-05-01   280 2021     5\n18 2021-06-01   260 2021     6\n19 2021-07-01   290 2021     7\n20 2021-08-01   310 2021     8\n21 2021-09-01   330 2021     9\n22 2021-10-01   340 2021    10\n23 2021-11-01   350 2021    11\n24 2021-12-01   360 2021    12\n\n\nHere, we convert the ts object into a data frame and use lubridate’s year() and month() functions to extract date components, which can be used for further analysis (e.g., grouping by month or year)."
  },
  {
    "objectID": "posts/2024-09-30_lubridate/index.html#solving-real-world-date-time-issues",
    "href": "posts/2024-09-30_lubridate/index.html#solving-real-world-date-time-issues",
    "title": "Mastering Date and Time Data in R with lubridate",
    "section": "Solving Real-World Date-Time Issues",
    "text": "Solving Real-World Date-Time Issues\nHandling date-time data in real-world applications often involves dealing with a variety of formats and potential inconsistencies. The lubridate package provides powerful functions to parse, manipulate, and format date-time data efficiently. This section focuses on how to use these functions, especially parse_date_time(), to address common date-time challenges.\nWhen working with datasets, date-time values may not always be in a standard format. For instance, you might encounter dates represented as strings in various formats like \"YYYY-MM-DD\", \"MM/DD/YYYY\", or even \"Month DD, YYYY\". To perform analysis accurately, it’s crucial to convert these strings into proper date-time objects.\nThe parse_date_time() function is one of the most versatile functions in the lubridate package. It allows you to specify multiple possible formats for parsing a date-time string. This flexibility is especially useful when dealing with datasets from different sources or with inconsistent date formats.\n\nparse_date_time(x, orders, tz = \"UTC\", quiet = FALSE)\n\n\nx: A character vector of date-time strings to be parsed.\norders: A vector of possible formats for the date-time strings (e.g., \"ymd\", \"mdy\", etc.).\ntz: The time zone to use (default is \"UTC\").\nquiet: If TRUE, suppress warnings.\n\n\n# Example date-time strings in various formats\ndates &lt;- c(\"2024-01-15\", \"01/16/2024\", \"March 17, 2024\", \"18-04-2024\")\n\n# Parse the dates using parse_date_time\nparsed_dates &lt;- parse_date_time(dates, orders = c(\"ymd\", \"mdy\", \"dmy\", \"B d, Y\"))\n\n# Display the parsed dates\nparsed_dates\n\n[1] \"2024-01-15 UTC\" \"2024-01-16 UTC\" \"2024-03-17 UTC\" \"2024-04-18 UTC\"\n\n\nIn this example:\n\nThe dates vector contains strings in various formats.\nThe parse_date_time() function attempts to parse each date according to the specified orders.\nThe output is a vector of parsed date-time objects, all converted to the same format."
  },
  {
    "objectID": "posts/2024-09-30_lubridate/index.html#alternative-packages-and-comparison-with-lubridate",
    "href": "posts/2024-09-30_lubridate/index.html#alternative-packages-and-comparison-with-lubridate",
    "title": "Mastering Date and Time Data in R with lubridate",
    "section": "Alternative Packages and Comparison with lubridate",
    "text": "Alternative Packages and Comparison with lubridate\nSeveral R packages can handle date-time data, each with its strengths and weaknesses. Below, we discuss these packages, comparing their functionalities with those of the lubridate package.\n\nBase R Functions\nSimilarities:\n\nBoth lubridate and base R offer essential functions for converting character strings to date or date-time objects (e.g., as.Date(), as.POSIXct()).\n\nDifferences:\n\nBase R functions require more manual handling of date-time formats, whereas lubridate offers a more user-friendly and intuitive syntax for parsing and manipulating dates.\n\nAdvantages of Base R:\n\nNo additional package installation is required, making it lightweight.\nSuitable for basic date-time manipulations.\n\nDisadvantages of Base R:\n\nLimited functionality for complex date-time operations.\nSyntax can be less intuitive, especially for beginners.\n\n\n\nchron Package\nSimilarities:\n\nBoth chron and lubridate provide functionalities for working with dates and times, making it easy to manage these data types.\n\nDifferences:\n\nchron is focused more on simpler date-time representations and does not handle time zones as effectively as lubridate.\n\nAdvantages of chron:\n\nStraightforward for handling date-time data without complexity.\nLightweight and easy to use for simple applications.\n\nDisadvantages of chron:\n\nLacks advanced features for manipulating dates and times.\nLimited support for time zones and complex date-time arithmetic.\n\n\n\ndata.table Package\nSimilarities:\n\nBoth packages allow for efficient date-time operations, and data.table provides functions to convert to date objects (e.g., as.IDate()).\n\nDifferences:\n\ndata.table is primarily a data manipulation package optimized for speed and performance, whereas lubridate focuses specifically on date-time operations.\n\nAdvantages of data.table:\n\nExcellent performance with large datasets.\nIntegrates well with data manipulation tasks, including date-time operations.\n\nDisadvantages of data.table:\n\nMore complex syntax, especially for users unfamiliar with data.table conventions.\nPrimarily focused on data manipulation rather than dedicated date-time handling.\n\n\n\nzoo and xts Packages\nSimilarities:\n\nBoth zoo and xts provide tools for handling time series data and can manage date-time objects effectively.\n\nDifferences:\n\nlubridate excels in date-time parsing and manipulation, while zoo and xts focus more on creating and manipulating time series objects.\n\nAdvantages of zoo and xts:\n\nSpecialized for handling irregularly spaced time series.\nProvides robust tools for time series analysis, including indexing and subsetting.\n\nDisadvantages of zoo and xts:\n\nNot as intuitive for general date-time manipulation tasks.\nRequires additional knowledge of time series concepts.\n\n\n\nAdvantages of lubridate\n\nUser-Friendly Syntax: lubridate offers intuitive functions for parsing, manipulating, and formatting date-time objects, making it accessible to users of all skill levels.\nFlexible Parsing: It can automatically recognize and parse multiple date-time formats, reducing the need for manual formatting.\nComprehensive Functionality: Provides a wide range of functions for date-time arithmetic, extracting components, and working with durations, periods, and intervals.\nTime Zone Handling: Strong support for working with time zones, making it easy to convert between different zones.\n\n\n\nDisadvantages of lubridate\n\nPerformance: For very large datasets, lubridate may not be as performant as packages like data.table or xts due to its more extensive functionality and overhead.\nLearning Curve: Although user-friendly, beginners may still face a learning curve when transitioning from basic date-time manipulation in base R to more advanced functionalities in lubridate.\nDependency: Requires installation of an additional package, which may not be ideal for all projects or environments.\n\n\n\nConclusion\nThe lubridate package is a powerful tool for handling date and time data in R, offering user-friendly functions for parsing, manipulating, and formatting date-time objects. Key features include:\n\nFlexible Parsing: Functions like ymd(), mdy(), and parse_date_time() make it easy to convert various formats into date-time objects.\nComponent Extraction: Extracting components such as year, month, and day with functions like year() and month() simplifies detailed analysis.\nTime Measurements: Creating durations, periods, and intervals allows for nuanced time calculations, enhancing temporal analysis.\n\nWhile lubridate excels in usability and flexibility, it’s important to consider its performance limitations with large datasets and the potential learning curve for new users. Comparing it with alternatives like base R, chron, data.table, zoo, and xts reveals that each package has its strengths, but lubridate stands out for its comprehensive approach to date-time manipulation.\nIncorporating lubridate into your R workflow will streamline your date-time processing, enabling more efficient data analysis and deeper insights.\nFor more information, refer to the official lubridate documentation."
  },
  {
    "objectID": "posts/2024-07-09_text_analyze/index.html",
    "href": "posts/2024-07-09_text_analyze/index.html",
    "title": "Text Data Analysis in R: Understanding grep, grepl, sub and gsub",
    "section": "",
    "text": "https://carlalexander.ca/beginners-guide-regular-expressions/"
  },
  {
    "objectID": "posts/2024-07-09_text_analyze/index.html#introduction",
    "href": "posts/2024-07-09_text_analyze/index.html#introduction",
    "title": "Text Data Analysis in R: Understanding grep, grepl, sub and gsub",
    "section": "Introduction",
    "text": "Introduction\nIn text data analysis, being able to search for patterns, validate their existence, and perform substitutions is crucial. R provides powerful base functions like grep, grepl, sub, and gsub to handle these tasks efficiently. This blog post will delve into how these functions work, using examples ranging from simple to complex, to show how they can be leveraged for text manipulation, classification, and grouping tasks."
  },
  {
    "objectID": "posts/2024-07-09_text_analyze/index.html#understanding-grep-and-grepl",
    "href": "posts/2024-07-09_text_analyze/index.html#understanding-grep-and-grepl",
    "title": "Text Data Analysis in R: Understanding grep, grepl, sub and gsub",
    "section": "1. Understanding grep and grepl",
    "text": "1. Understanding grep and grepl\n\nWhat is grep?\n\nFunctionality: Searches for matches to a specified pattern in a vector of character strings.\nUsage: grep(pattern, x, ...)\nExample: Searching for specific words or patterns in text.\n\n\n\nWhat is grepl?\n\nFunctionality: Returns a logical vector indicating whether a pattern is found in each element of a character vector.\nUsage: grepl(pattern, x, ...)\nExample: Checking if specific patterns exist in text data.\n\n\n\nDifferences, Advantages, and Disadvantages\n\nDifferences: grep returns indices or values matching the pattern, while grepl returns a logical vector.\nAdvantages: Fast pattern matching over large datasets.\nDisadvantages: Exact matching without inherent flexibility for complex patterns."
  },
  {
    "objectID": "posts/2024-07-09_text_analyze/index.html#using-sub-and-gsub-for-text-substitution",
    "href": "posts/2024-07-09_text_analyze/index.html#using-sub-and-gsub-for-text-substitution",
    "title": "Text Data Analysis in R: Understanding grep, grepl, sub and gsub",
    "section": "2. Using sub and gsub for Text Substitution",
    "text": "2. Using sub and gsub for Text Substitution\n\nWhat is sub?\n\nFunctionality: Replaces the first occurrence of a pattern in a string.\nUsage: sub(pattern, replacement, x, ...)\nExample: Substituting specific patterns with another string.\n\n\n\nWhat is gsub?\n\nFunctionality: Replaces all occurrences of a pattern in a string.\nUsage: gsub(pattern, replacement, x, ...)\nExample: Global substitution of patterns throughout text data.\n\n\n\nDifferences, Advantages, and Disadvantages\n\nDifferences: sub replaces only the first occurrence, while gsub replaces all occurrences.\nAdvantages: Efficient for bulk text replacements.\nDisadvantages: Lack of advanced pattern matching features compared to other libraries."
  },
  {
    "objectID": "posts/2024-07-09_text_analyze/index.html#practical-examples-with-a-synthetic-dataset",
    "href": "posts/2024-07-09_text_analyze/index.html#practical-examples-with-a-synthetic-dataset",
    "title": "Text Data Analysis in R: Understanding grep, grepl, sub and gsub",
    "section": "3. Practical Examples with a Synthetic Dataset",
    "text": "3. Practical Examples with a Synthetic Dataset\n\nExample Dataset\nFor the purposes of this blog post, we’ll create a synthetic dataset. This dataset is a data frame that contains two columns: id and text. Each row represents a unique text entry with a corresponding identifier.\n\n# Creating a synthetic data frame\ntext_data &lt;- data.frame(\n  id = 1:15,\n  text = c(\"Cats are great pets.\",\n           \"Dogs are loyal animals.\",\n           \"Birds can fly high.\",\n           \"Fish swim in water.\",\n           \"Horses run fast.\",\n           \"Rabbits hop quickly.\",\n           \"Cows give milk.\",\n           \"Sheep have wool.\",\n           \"Goats are curious creatures.\",\n           \"Lions are the kings of the jungle.\",\n           \"Tigers have stripes.\",\n           \"Elephants are large animals.\",\n           \"Monkeys are very playful.\",\n           \"Giraffes have long necks.\",\n           \"Zebras have black and white stripes.\")\n)\n\n\n\nExplanation of the Dataset\n\nid Column: This is a simple identifier for each row, ranging from 1 to 15.\ntext Column: This contains various sentences about different animals. Each text string is unique and describes a characteristic or trait of the animal mentioned.\n\n\n\nApplying grep, grepl, sub, and gsub\n\nExample 1: Using grep to find specific words\n\n# Find rows containing the word 'are'\nindices &lt;- grep(\"are\", text_data$text, ignore.case = TRUE)\nresult_grep &lt;- text_data[indices, ]\nresult_grep\n\n   id                               text\n1   1               Cats are great pets.\n2   2            Dogs are loyal animals.\n9   9       Goats are curious creatures.\n10 10 Lions are the kings of the jungle.\n12 12       Elephants are large animals.\n13 13          Monkeys are very playful.\n\n\nExplanation: grep(\"are\", text_data$text, ignore.case = TRUE) searches for the word “are” in the text column of text_data, ignoring case, and returns the indices of the matching rows. The resulting rows will be displayed.\n\n\nExample 2: Applying grepl for conditional checks\n\n# Add a new column indicating if the word 'fly' is present\n\ntext_data$contains_fly &lt;- grepl(\"fly\", text_data$text)\ntext_data\n\n   id                                 text contains_fly\n1   1                 Cats are great pets.        FALSE\n2   2              Dogs are loyal animals.        FALSE\n3   3                  Birds can fly high.         TRUE\n4   4                  Fish swim in water.        FALSE\n5   5                     Horses run fast.        FALSE\n6   6                 Rabbits hop quickly.        FALSE\n7   7                      Cows give milk.        FALSE\n8   8                     Sheep have wool.        FALSE\n9   9         Goats are curious creatures.        FALSE\n10 10   Lions are the kings of the jungle.        FALSE\n11 11                 Tigers have stripes.        FALSE\n12 12         Elephants are large animals.        FALSE\n13 13            Monkeys are very playful.        FALSE\n14 14            Giraffes have long necks.        FALSE\n15 15 Zebras have black and white stripes.        FALSE\n\n\nExplanation: grepl(\"fly\", text_data$text) checks each element of the text column for the presence of the word “fly” and returns a logical vector. This vector is then added as a new column contains_fly.\n\n\nExample 3: Using sub to replace a pattern in text\n\n# Replace the first occurrence of 'a' with 'A' in the text column\n\ntext_data$text_sub &lt;- sub(\" a \", \" A \", text_data$text)\ntext_data[,c(\"text\",\"text_sub\")]\n\n                                   text                             text_sub\n1                  Cats are great pets.                 Cats are great pets.\n2               Dogs are loyal animals.              Dogs are loyal animals.\n3                   Birds can fly high.                  Birds can fly high.\n4                   Fish swim in water.                  Fish swim in water.\n5                      Horses run fast.                     Horses run fast.\n6                  Rabbits hop quickly.                 Rabbits hop quickly.\n7                       Cows give milk.                      Cows give milk.\n8                      Sheep have wool.                     Sheep have wool.\n9          Goats are curious creatures.         Goats are curious creatures.\n10   Lions are the kings of the jungle.   Lions are the kings of the jungle.\n11                 Tigers have stripes.                 Tigers have stripes.\n12         Elephants are large animals.         Elephants are large animals.\n13            Monkeys are very playful.            Monkeys are very playful.\n14            Giraffes have long necks.            Giraffes have long necks.\n15 Zebras have black and white stripes. Zebras have black and white stripes.\n\n\nExplanation: sub(\" a \", \" A \", text_data$text) replaces the first occurrence of ’ a ’ with ’ A ’ in each element of the text column. The resulting text is stored in a new column text_sub.\n\n\nExample 4: Applying gsub for global pattern replacement\n\n# Replace all occurrences of 'a' with 'A' in the text column\n\ntext_data$text_gsub &lt;- gsub(\" a \", \" A \", text_data$text)\ntext_data[,c(\"text\",\"text_gsub\")]\n\n                                   text                            text_gsub\n1                  Cats are great pets.                 Cats are great pets.\n2               Dogs are loyal animals.              Dogs are loyal animals.\n3                   Birds can fly high.                  Birds can fly high.\n4                   Fish swim in water.                  Fish swim in water.\n5                      Horses run fast.                     Horses run fast.\n6                  Rabbits hop quickly.                 Rabbits hop quickly.\n7                       Cows give milk.                      Cows give milk.\n8                      Sheep have wool.                     Sheep have wool.\n9          Goats are curious creatures.         Goats are curious creatures.\n10   Lions are the kings of the jungle.   Lions are the kings of the jungle.\n11                 Tigers have stripes.                 Tigers have stripes.\n12         Elephants are large animals.         Elephants are large animals.\n13            Monkeys are very playful.            Monkeys are very playful.\n14            Giraffes have long necks.            Giraffes have long necks.\n15 Zebras have black and white stripes. Zebras have black and white stripes.\n\n\nExplanation: gsub(\" a \", \" A \", text_data$text) replaces all occurrences of ’ a ’ with ’ A ’ in each element of the text column. The resulting text is stored in a new column text_gsub.\n\n\n\nExample 5: Text-based Grouping and Assignment\nLet’s group the texts based on the presence of the word “bird” and assign a category.\n\n# Add a new column 'category' based on the presence of the word 'fly'\n\ntext_data$category &lt;- ifelse(grepl(\"fly\", text_data$text, ignore.case = TRUE), \"Can Fly\", \"Cannot Fly\")\ntext_data[,c(\"text\",\"category\")]\n\n                                   text   category\n1                  Cats are great pets. Cannot Fly\n2               Dogs are loyal animals. Cannot Fly\n3                   Birds can fly high.    Can Fly\n4                   Fish swim in water. Cannot Fly\n5                      Horses run fast. Cannot Fly\n6                  Rabbits hop quickly. Cannot Fly\n7                       Cows give milk. Cannot Fly\n8                      Sheep have wool. Cannot Fly\n9          Goats are curious creatures. Cannot Fly\n10   Lions are the kings of the jungle. Cannot Fly\n11                 Tigers have stripes. Cannot Fly\n12         Elephants are large animals. Cannot Fly\n13            Monkeys are very playful. Cannot Fly\n14            Giraffes have long necks. Cannot Fly\n15 Zebras have black and white stripes. Cannot Fly\n\n\nExplanation: grepl(\"fly\", text_data$text, ignore.case = TRUE) checks for the presence of the word “fly” in each element of the text column, ignoring case. The ifelse function is then used to create a new column category, assigning “Can Fly” if the word is present and “Cannot Fly” otherwise.\n\n\nAdditional Examples\n\nExample 6: Using grep to find multiple patterns\n\n# Find rows containing the words 'great' or 'loyal'\nindices &lt;- grep(\"great|loyal\", text_data$text, ignore.case = TRUE)\ntext_data[indices,c(\"text\") ]\n\n[1] \"Cats are great pets.\"    \"Dogs are loyal animals.\"\n\n\nExplanation: grep(\"great|loyal\", text_data$text, ignore.case = TRUE) searches for the words “great” or “loyal” in the text column, ignoring case, and returns the indices of the matching rows. The resulting rows will be displayed.\n\n\nExample 7: Using gsub for complex substitutions\n\n# Replace all occurrences of 'animals' with 'creatures' and 'pets' with 'companions'\n\ntext_data$text_gsub_complex &lt;- gsub(\"animals\", \"creatures\", gsub(\"pets\", \"companions\", text_data$text))\ntext_data[,c(\"text\",\"text_gsub_complex\")]\n\n                                   text                    text_gsub_complex\n1                  Cats are great pets.           Cats are great companions.\n2               Dogs are loyal animals.            Dogs are loyal creatures.\n3                   Birds can fly high.                  Birds can fly high.\n4                   Fish swim in water.                  Fish swim in water.\n5                      Horses run fast.                     Horses run fast.\n6                  Rabbits hop quickly.                 Rabbits hop quickly.\n7                       Cows give milk.                      Cows give milk.\n8                      Sheep have wool.                     Sheep have wool.\n9          Goats are curious creatures.         Goats are curious creatures.\n10   Lions are the kings of the jungle.   Lions are the kings of the jungle.\n11                 Tigers have stripes.                 Tigers have stripes.\n12         Elephants are large animals.       Elephants are large creatures.\n13            Monkeys are very playful.            Monkeys are very playful.\n14            Giraffes have long necks.            Giraffes have long necks.\n15 Zebras have black and white stripes. Zebras have black and white stripes.\n\n\nExplanation: The inner gsub replaces all occurrences of ‘pets’ with ‘companions’, and the outer gsub replaces all occurrences of ‘animals’ with ‘creatures’ in each element of the text column. The resulting text is stored in a new column text_gsub_complex.\n\n\nExample 8: Using grepl with multiple conditions\n\n# Add a new column indicating if the text contains either 'large' or 'playful'\n\ntext_data$contains_large_or_playful &lt;- grepl(\"large|playful\", text_data$text)\ntext_data[,c(\"text\",\"contains_large_or_playful\")]\n\n                                   text contains_large_or_playful\n1                  Cats are great pets.                     FALSE\n2               Dogs are loyal animals.                     FALSE\n3                   Birds can fly high.                     FALSE\n4                   Fish swim in water.                     FALSE\n5                      Horses run fast.                     FALSE\n6                  Rabbits hop quickly.                     FALSE\n7                       Cows give milk.                     FALSE\n8                      Sheep have wool.                     FALSE\n9          Goats are curious creatures.                     FALSE\n10   Lions are the kings of the jungle.                     FALSE\n11                 Tigers have stripes.                     FALSE\n12         Elephants are large animals.                      TRUE\n13            Monkeys are very playful.                      TRUE\n14            Giraffes have long necks.                     FALSE\n15 Zebras have black and white stripes.                     FALSE\n\n\nExplanation: grepl(\"large|playful\", text_data$text) checks each element of the text column for the presence of the words “large” or “playful” and returns a logical vector. This vector is then added as a new column contains_large_or_playful."
  },
  {
    "objectID": "posts/2024-07-09_text_analyze/index.html#understanding-regular-expressions",
    "href": "posts/2024-07-09_text_analyze/index.html#understanding-regular-expressions",
    "title": "Text Data Analysis in R: Understanding grep, grepl, sub and gsub",
    "section": "4. Understanding Regular Expressions",
    "text": "4. Understanding Regular Expressions\nRegular expressions (regex) are powerful tools used for pattern matching and text manipulation. They allow you to define complex search patterns using a combination of literal characters and special symbols. R’s grep, grepl, sub, and gsub functions all support the use of regular expressions.\n\nKey Components of Regular Expressions\n\nLiteral Characters: These are the basic building blocks of regex. For example, cat matches the string “cat”.\nMetacharacters: Special characters with unique meanings, such as ^, $, ., *, +, ?, |, [], (), {}\n\n^ matches the start of a string.\n$ matches the end of a string.\n. matches any single character except a newline.\n* matches zero or more occurrences of the preceding element.\n+ matches one or more occurrences of the preceding element.\n? matches zero or one occurrence of the preceding element.\n| denotes alternation (or).\n[] matches any one of the characters inside the brackets.\n() groups elements together.\n{} specifies a specific number of occurrences.\n\n\n\n\nExamples with Regular Expressions\nUsing the same synthetic dataset, let’s explore how to apply regular expressions with grep, grepl, sub, and gsub.\n\nExample 1: Matching Text that Starts with a Specific Word\n\n# Find rows where text starts with the word 'Cats'\nindices &lt;- grep(\"^Cats\", text_data$text)\ntext_data[indices,c(\"text\")]\n\n[1] \"Cats are great pets.\"\n\n\nExplanation: grep(\"^Cats\", text_data$text) uses the ^ metacharacter to find rows where the text starts with “Cats”.\n\n\nExample 2: Matching Text that Ends with a Specific Word\n\n# Find rows where text ends with the word 'water.'\nindices &lt;- grep(\"water\\\\.$\", text_data$text)\ntext_data[indices,c(\"text\")]\n\n[1] \"Fish swim in water.\"\n\n\nExplanation: grep(\"water\\\\.$\", text_data$text) uses the $ metacharacter to find rows where the text ends with “water.” The \\\\. is used to escape the dot character, which is a metacharacter in regex.\n\n\nExample 3: Matching Text that Contains a Specific Pattern\n\n# Find rows where text contains 'great' followed by any character and 'pets'\nindices &lt;- grep(\"great.pets\", text_data$text)\ntext_data[indices,c(\"text\")]\n\n[1] \"Cats are great pets.\"\n\n\nExplanation: grep(\"great.pets\", text_data$text) uses the . metacharacter to match any character between “great” and “pets”.\n\n\n\nExample 4: Using gsub with Regular Expressions\n\n# Replace all occurrences of words starting with 'C' with 'Animal'\ntext_data$text_gsub_regex &lt;- gsub(\"\\\\bC\\\\w+\", \"Animal\", text_data$text)\ntext_data[,c(\"text\",\"text_gsub_regex\")]\n\n                                   text                      text_gsub_regex\n1                  Cats are great pets.               Animal are great pets.\n2               Dogs are loyal animals.              Dogs are loyal animals.\n3                   Birds can fly high.                  Birds can fly high.\n4                   Fish swim in water.                  Fish swim in water.\n5                      Horses run fast.                     Horses run fast.\n6                  Rabbits hop quickly.                 Rabbits hop quickly.\n7                       Cows give milk.                    Animal give milk.\n8                      Sheep have wool.                     Sheep have wool.\n9          Goats are curious creatures.         Goats are curious creatures.\n10   Lions are the kings of the jungle.   Lions are the kings of the jungle.\n11                 Tigers have stripes.                 Tigers have stripes.\n12         Elephants are large animals.         Elephants are large animals.\n13            Monkeys are very playful.            Monkeys are very playful.\n14            Giraffes have long necks.            Giraffes have long necks.\n15 Zebras have black and white stripes. Zebras have black and white stripes.\n\n\nExplanation: gsub(\"\\\\bC\\\\w+\", \"Animal\", text_data$text) replaces all words starting with ‘C’ (\\\\b indicates a word boundary, C matches the character ‘C’, and \\\\w+ matches one or more word characters) with “Animal”.\n\nExample 5: Using grepl to Check for Complex Patterns\n\n# Add a new column indicating if the text contains a word ending with 's'\ntext_data$contains_s_end &lt;- grepl(\"\\\\b\\\\w+s\\\\b\", text_data$text)\ntext_data[,c(\"text\",\"contains_s_end\")]\n\n                                   text contains_s_end\n1                  Cats are great pets.           TRUE\n2               Dogs are loyal animals.           TRUE\n3                   Birds can fly high.           TRUE\n4                   Fish swim in water.          FALSE\n5                      Horses run fast.           TRUE\n6                  Rabbits hop quickly.           TRUE\n7                       Cows give milk.           TRUE\n8                      Sheep have wool.          FALSE\n9          Goats are curious creatures.           TRUE\n10   Lions are the kings of the jungle.           TRUE\n11                 Tigers have stripes.           TRUE\n12         Elephants are large animals.           TRUE\n13            Monkeys are very playful.           TRUE\n14            Giraffes have long necks.           TRUE\n15 Zebras have black and white stripes.           TRUE\n\n\nExplanation: grepl(\"\\\\b\\\\w+s\\\\b\", text_data$text) checks each element of the text column for the presence of a word ending with ‘s’. Here, \\\\b indicates a word boundary, \\\\w+ matches one or more word characters, and s matches the character ‘s’."
  },
  {
    "objectID": "posts/2024-07-09_text_analyze/index.html#conclusion",
    "href": "posts/2024-07-09_text_analyze/index.html#conclusion",
    "title": "Text Data Analysis in R: Understanding grep, grepl, sub and gsub",
    "section": "Conclusion",
    "text": "Conclusion\nThe grep, grepl, sub, and gsub functions in R are powerful tools for text data analysis. They allow for efficient searching, pattern matching, and text manipulation, making them essential for any data analyst or data scientist working with textual data. By understanding how to use these functions and leveraging regular expressions, you can perform a wide range of text processing tasks, from simple searches to complex pattern replacements and text-based classifications."
  },
  {
    "objectID": "posts/2024-01-22_functions/index.html",
    "href": "posts/2024-01-22_functions/index.html",
    "title": "R Function Writing 101:A Journey Through Syntax, Best Practices, and More",
    "section": "",
    "text": "R is a powerful and versatile programming language widely used in data analysis, statistics, and visualization. One of the key features that make R so flexible is its ability to create functions. Functions in R allow you to encapsulate a set of instructions into a reusable and modular block of code, promoting code organization and efficiency. Much like a well-engineered machine, where gears work together seamlessly, functions provide the backbone for modular, efficient, and structured code. As we delve into the syntax, best practices, and hands-on examples, envision the gears turning in unison, each function contributing to the overall functionality of your programs. In this blog post, we will delve into the world of writing functions in R, exploring the syntax, best practices, and showcasing interesting examples."
  },
  {
    "objectID": "posts/2024-01-22_functions/index.html#introduction",
    "href": "posts/2024-01-22_functions/index.html#introduction",
    "title": "R Function Writing 101:A Journey Through Syntax, Best Practices, and More",
    "section": "",
    "text": "R is a powerful and versatile programming language widely used in data analysis, statistics, and visualization. One of the key features that make R so flexible is its ability to create functions. Functions in R allow you to encapsulate a set of instructions into a reusable and modular block of code, promoting code organization and efficiency. Much like a well-engineered machine, where gears work together seamlessly, functions provide the backbone for modular, efficient, and structured code. As we delve into the syntax, best practices, and hands-on examples, envision the gears turning in unison, each function contributing to the overall functionality of your programs. In this blog post, we will delve into the world of writing functions in R, exploring the syntax, best practices, and showcasing interesting examples."
  },
  {
    "objectID": "posts/2024-01-22_functions/index.html#basics-of-writing-functions-in-r",
    "href": "posts/2024-01-22_functions/index.html#basics-of-writing-functions-in-r",
    "title": "R Function Writing 101:A Journey Through Syntax, Best Practices, and More",
    "section": "Basics of Writing Functions in R",
    "text": "Basics of Writing Functions in R\nSyntax:\nIn R, a basic function has the following syntax:\n\nmy_function &lt;- function(arg1, arg2, ...) {\n  # Function body\n  # Perform operations using arg1, arg2, ...\n  return(result)\n}\n\n\nmy_function: The name you assign to your function.\narg1, arg2, ...: Arguments passed to the function.\nreturn(result): The result that the function will produce.\n\nExample:\nLet’s create a simple function that adds two numbers:\n\n# Define a function named 'square'\nsquare &lt;- function(x) {\n  result &lt;- x^2\n  return(result)\n}\n\n# Usage of the function\nsquared_value &lt;- square(4)\nprint(squared_value)\n\n[1] 16\n\n\nNow, let’s break down the components of this example:\n\nFunction Definition:\n\nsquare is the name assigned to the function.\n\nParameter:\n\nx is the single parameter or argument that the function expects. It represents the number you want to square.\n\nFunction Body:\n\nThe body of the function is enclosed in curly braces {}. Inside, result &lt;- x^2 calculates the square of x.\n\nReturn Statement:\n\nreturn(result) specifies that the calculated square is the output of the function.\n\nUsage:\n\nsquare(4) is an example of calling the function with the value 4. The result is stored in the variable squared_value.\n\nPrint Output:\n\nprint(squared_value) prints the result to the console, and the output is 16.\n\n\nThis function takes a single argument, squares it, and returns the result. You can customize and use this type of function to perform specific operations on individual values, making your code more modular and readable."
  },
  {
    "objectID": "posts/2024-01-22_functions/index.html#advanced-function-features",
    "href": "posts/2024-01-22_functions/index.html#advanced-function-features",
    "title": "R Function Writing 101:A Journey Through Syntax, Best Practices, and More",
    "section": "Advanced Function Features",
    "text": "Advanced Function Features\n\nDefault Arguments\n“Default Arguments” refers to a feature in R functions that allows you to specify default values for function parameters. Default arguments provide a predefined value for a parameter in case the user does not explicitly provide a value when calling the function.\n\npower_function &lt;- function(x, exponent = 2) {\n  result &lt;- x ^ exponent\n  return(result)\n}\n\nIn this example, we define a function called power_function that takes two parameters: x and exponent. Here’s a step-by-step explanation:\n\nFunction Definition:\n\npower_function is the name of the function.\n\nParameters:\n\nx and exponent are the parameters (or arguments) that the function accepts.\n\nDefault Value:\n\nexponent = 2 indicates that if the user does not provide a value for exponent when calling the function, it will default to 2.\n\nFunction Body:\n\nThe function body is enclosed in curly braces {} and contains the code that the function will execute.\n\nCalculation:\n\nInside the function body, result &lt;- x ^ exponent calculates the result by raising x to the power of exponent.\n\nReturn Statement:\n\nreturn(result) specifies that the calculated result will be the output of the function.\n\n\nNow, let’s see how this function can be used:\n\n# Usage\npower_of_3 &lt;- power_function(3)\nprint(power_of_3) \n\n[1] 9\n\npower_of_3_cubed &lt;- power_function(3, 3)\nprint(power_of_3_cubed) \n\n[1] 27\n\n\nHere, we demonstrate two usages of the power_function:\n\nWithout Providing exponent:\n\npower_function(3) uses the default value of exponent = 2, resulting in 3 ^ 2, which is 9.\n\nProviding a Custom exponent:\n\npower_function(3, 3) explicitly provides a value for exponent, resulting in 3 ^ 3, which is 27.\n\n\nIn summary, the default argument (exponent = 2) makes the function more flexible by providing a sensible default value for the exponent parameter, but users can override it by supplying their own value when needed.\n\n\nVariable Arguments\nIn R, the ... (ellipsis) allows you to work with a variable number of arguments in a function, offering flexibility and convenience. This magical feature empowers you to create functions that can handle different inputs without explicitly defining each one.\nProperties of ...:\n\nVariable Number of Arguments:\n\n... allows you to accept an arbitrary number of arguments in your function.\n\nPassing Arguments to Other Functions:\n\nYou can pass the ellipsis (...) to other functions within your function, making it extremely versatile.\n\n\nLet’s break down the code example:\n\nsum_all &lt;- function(...) {\n  numbers &lt;- c(...)\n  result &lt;- sum(numbers)\n  return(result)\n}\n\nHere’s a step-by-step explanation of the code:\n\nFunction Definition:\n\nsum_all is the name of the function.\n\nVariable Arguments:\n\n... is used as a placeholder for a variable number of arguments. It allows the function to accept any number of arguments.\n\nCombining Arguments into a Vector:\n\nnumbers &lt;- c(...) combines all the arguments passed to the function into a vector named numbers.\n\nSummation:\n\nresult &lt;- sum(numbers) calculates the sum of all the numbers in the vector.\n\nReturn Statement:\n\nreturn(result) specifies that the calculated sum will be the output of the function.\n\n\nNow, let’s see how this function can be used:\n\n# Usage\ntotal_sum1 &lt;- sum_all(1, 2, 3, 4, 5)\nprint(total_sum1)  \n\n[1] 15\n\ntotal_sum2 &lt;- sum_all(10, 20, 30)\nprint(total_sum2) \n\n[1] 60\n\n\nIn the usage examples:\n\nsum_all(1, 2, 3, 4, 5) passes five arguments to the function, and the sum is calculated as 1 + 2 + 3 + 4 + 5, resulting in 15.\nsum_all(10, 20, 30) passes three arguments, and the sum is calculated as 10 + 20 + 30, resulting in 60.\n\nThis function allows flexibility by accepting any number of arguments, making it suitable for scenarios where the user may need to sum a dynamic set of values. The ellipsis (...) serves as a convenient mechanism for handling variable arguments in R functions.\n\n\nMultiple Arguments in R Functions\nUsing multiple arguments when writing a function in the R programming language means accepting and working with more than one input parameter.. In R, functions can be defined to take multiple arguments, allowing for greater flexibility and customization when calling the function with different sets of data.\nHere’s a general structure of a function with multiple arguments in R:\n\nmy_function &lt;- function(arg1, arg2, ...) {\n  # Function body\n  # Perform operations using arg1, arg2, ...\n  return(result)\n}\n\nLet’s break down the components:\n\nmy_function: The name you assign to your function.\narg1, arg2, ...: Parameters or arguments passed to the function.\n...: The ellipsis (...) represents variable arguments, allowing the function to accept a variable number of parameters.\n\nHere’s a more concrete example:\n\ncalculate_sum &lt;- function(x, y) {\n  result &lt;- x + y\n  return(result)\n}\n\n# Usage\nsum_result &lt;- calculate_sum(3, 5)\nprint(sum_result) \n\n[1] 8\n\n\nIn this example, the calculate_sum function takes two arguments (x and y) and returns their sum. You can call the function with different values for x and y to obtain different results.\n\n# Usage\nresult1 &lt;- calculate_sum(10, 15)\nprint(result1)\n\n[1] 25\n\nresult2 &lt;- calculate_sum(-5, 8)\nprint(result2)\n\n[1] 3\n\n\nThis flexibility in handling multiple arguments makes R functions versatile and adaptable to various tasks. You can design functions to perform complex operations or calculations by allowing users to input different sets of data through multiple parameters.\n\n\nReturning Multiple Outputs from a Function in R\nIn R, functions traditionally return a single object. However, in many real-world data analysis workflows, we often need a function to return multiple outputs simultaneously — such as several statistics, model results, or diagnostic values.\nTo achieve this, the most common approach in R is to return a named list. This provides flexibility, structure, and easy access to individual components.\nBelow are some practical examples demonstrating this concept.\n\nExample 1: Returning Multiple Summary Statistics\nLet’s say we want to compute the mean, median, and standard deviation of a numeric vector:\n\nsummary_stats &lt;- function(x) {\n  mean_x &lt;- mean(x, na.rm = TRUE)\n  median_x &lt;- median(x, na.rm = TRUE)\n  sd_x &lt;- sd(x, na.rm = TRUE)\n  \n  return(list(\n    mean = mean_x,\n    median = median_x,\n    sd = sd_x\n  ))\n}\n\ndata &lt;- c(10, 20, 30, 40, 50)\nresult &lt;- summary_stats(data)\n\nresult$mean    # 30\n\n[1] 30\n\nresult$median  # 30\n\n[1] 30\n\nresult$sd      # 15.81\n\n[1] 15.81139\n\n\nWhat’s happening?\n\nThe function summary_stats() returns a named list with three numeric values.\nYou can access each result using $, e.g., result$sd.\n\n\n\nExample 2: Returning a Data Frame and Plot Together\nSometimes we want a function to return both a table and a visualization.\n\nlibrary(ggplot2)\n\nanalyze_distribution &lt;- function(x) {\n  df &lt;- data.frame(\n    value = x,\n    z = scale(x)\n  )\n  \n  plot &lt;- ggplot(df, aes(x = value)) +\n    geom_histogram(bins = 10, fill = \"steelblue\", color = \"white\") +\n    theme_minimal()\n  \n  return(list(\n    table = df,\n    histogram = plot\n  ))\n}\n\ndata &lt;- rnorm(100)\noutput &lt;- analyze_distribution(data)\n\nhead(output$table)     # Shows the first few rows of the table\n\n       value          z\n1  1.2018165  1.2433736\n2  0.1627978  0.1343999\n3 -1.1448543 -1.2612939\n4  0.1921125  0.1656883\n5 -0.6020712 -0.6819663\n6  0.4271903  0.4165934\n\noutput$histogram       # Displays the ggplot2 histogram\n\n\n\n\n\n\n\n\nTakeaways:\n\nThis function returns both a data.frame and a ggplot object.\nThis is especially useful for reporting functions in packages or Shiny applications.\n\n\n\nBonus Tip: Named Lists vs. Tibbles\nWhile lists are flexible, in some modeling contexts (e.g., when nesting or mapping), it can be useful to wrap outputs in a tibble:\n\nlibrary(tibble)\n\nmulti_return &lt;- function(x) {\n  tibble(\n    input = list(x),\n    summary = list(summary(x)),\n    sd = sd(x)\n  )\n}\n\nIn summary; R does not support multiple return values like Python’s tuple unpacking, but lists and tibbles allow us to simulate this pattern elegantly. Whether you are building utility functions or modularizing a complex pipeline, returning multiple outputs as a single structured object is both powerful and idiomatic in R."
  },
  {
    "objectID": "posts/2024-01-22_functions/index.html#more-examples",
    "href": "posts/2024-01-22_functions/index.html#more-examples",
    "title": "R Function Writing 101:A Journey Through Syntax, Best Practices, and More",
    "section": "More Examples",
    "text": "More Examples\n\nMean of a Numeric Vector\nLet’s create a simple function that calculates the mean of a numeric vector in R. The function will take a numeric vector as its argument and return the mean value.\n\n# Define a function named 'calculate_mean'\ncalculate_mean &lt;- function(numbers) {\n  # Check if 'numbers' is numeric\n  if (!is.numeric(numbers)) {\n    stop(\"Input must be a numeric vector.\")\n  }\n\n  # Calculate the mean\n  result &lt;- mean(numbers)\n  \n  # Return the mean\n  return(result)\n}\n\n# Usage of the function\nnumeric_vector &lt;- c(2, 4, 6, 8, 10)\nmean_result &lt;- calculate_mean(numeric_vector)\nprint(mean_result)\n\n[1] 6\n\n\nIn this function we also check the input validation. if (!is.numeric(numbers)) checks if the input vector is numeric. If not, an error message is displayed using stop().\n\n\nCalculate Exponential Growth\nLet’s create a function to calculate the exponential growth of a quantity over time. Exponential growth is a mathematical concept where a quantity increases by a fixed percentage rate over a given period.\nHere’s an example of how you might write a function in R to calculate exponential growth:\n\n# Define a function to calculate exponential growth\ncalculate_exponential_growth &lt;- function(initial_value, growth_rate, time_period) {\n  final_value &lt;- initial_value * (1 + growth_rate)^time_period\n  return(final_value)\n}\n\n# Usage of the function\ninitial_value &lt;- 1000  # Initial quantity\ngrowth_rate &lt;- 0.05    # 5% growth rate\ntime_period &lt;- 3       # 3 years\n\nfinal_result &lt;- calculate_exponential_growth(initial_value, growth_rate, time_period)\nprint(final_result)  \n\n[1] 1157.625\n\n\nExplanation:\n\nThe function calculate_exponential_growth takes three parameters: initial_value (the starting quantity), growth_rate (the percentage growth rate per period), and time_period (the number of periods).\nInside the function, it calculates the final value after the given time period using the formula for exponential growth:\n\n\\[\nFinal Value = Initial Value\\times (1+Growth Rate)^{TimePeriod}    \n\\]\n\nThe calculated final value is stored in the variable final_value.\nThe function returns the final value.\n\nIn the usage example:\n\nThe initial quantity is set to 1000.\nThe growth rate is set to 5% (0.05).\nThe time period is set to 3 years.\nThe function is called with these values, and the result is printed to the console.\n\nThis is just one example of how you might use a function to calculate exponential growth. Depending on your specific requirements, you can modify the function and parameters to suit different scenarios.\n\n\nCalculate Compound Interest\nSuppose that we want to create a function to calculate compound interest over time. Compound interest is a financial concept where interest is calculated not only on the initial principal amount but also on the accumulated interest from previous periods. The formula for compound interest is often expressed as:\n\\[\nA= P\\times(1+\\frac{r}{n})^{nt}\n\\]\nwhere:\n\n\\(A\\) is the amount of money accumulated after \\(n\\) years, including interest.\n\\(P\\) is the principal amount (initial investment).\n\\(r\\) is the annual interest rate (as a decimal).\n\\(n\\) is the number of times that interest is compounded per unit \\(t\\) (usually per year).\n\\(t\\) is the time the money is invested or borrowed for, in years.\n\nHere’s an example of how you might write a function in R to calculate compound interest:\n\n# Define a function to calculate compound interest\ncalculate_compound_interest &lt;- function(principal, rate, time, compounding_frequency) {\n  amount &lt;- principal * (1 + rate/compounding_frequency)^(compounding_frequency*time)\n  interest &lt;- amount - principal\n  return(interest)\n}\n\n# Usage of the function\ninitial_principal &lt;- 1000  # Initial investment\nannual_interest_rate &lt;- 0.05  # 5% annual interest rate\ninvestment_time &lt;- 3  # 3 years\ncompounding_frequency &lt;- 12  # Monthly compounding\n\ncompound_interest_result &lt;- calculate_compound_interest(initial_principal, annual_interest_rate, investment_time, compounding_frequency)\nprint(compound_interest_result)\n\n[1] 161.4722\n\n\nExplanation:\n\nThe function calculate_compound_interest takes four parameters: principal (the initial investment), rate (the annual interest rate), time (the time the money is invested for, in years), and compounding_frequency (the number of times interest is compounded per year).\nInside the function, it calculates the amount using the compound interest formula.\nIt then calculates the interest earned by subtracting the initial principal from the final amount.\nThe function returns the calculated compound interest.\n\nIn the usage example:\n\nThe initial investment is set to $1000.\nThe annual interest rate is set to 5% (0.05).\nThe investment time is set to 3 years.\nInterest is compounded monthly (12 times per year).\nThe function is called with these values, and the result (compound interest) is printed to the console.\n\nThis example illustrates how you can use a function to calculate compound interest for a given investment scenario. Adjust the parameters based on your specific financial context.\n\n\nCustom Plotting Function\nLet’s enhance the custom plotting function using the ellipsis (...) to allow for additional customization parameters. The ellipsis allows you to pass a variable number of arguments to the function, providing more flexibility.\n\n# Define a custom plotting function with ellipsis\ncustom_plot &lt;- function(x_values, y_values, ..., plot_type = \"line\", title = \"Custom Plot\") {\n  plot_title &lt;- paste(\"Custom Plot: \", title)\n  \n  if (plot_type == \"line\") {\n    plot(x_values, y_values, type = \"l\", col = \"blue\", main = plot_title, xlab = \"X-axis\", ylab = \"Y-axis\", ...)\n  } else if (plot_type == \"scatter\") {\n    plot(x_values, y_values, col = \"red\", main = plot_title, xlab = \"X-axis\", ylab = \"Y-axis\", ...)\n  } else {\n    warning(\"Invalid plot type. Defaulting to line plot.\")\n    plot(x_values, y_values, type = \"l\", col = \"blue\", main = plot_title, xlab = \"X-axis\", ylab = \"Y-axis\", ...)\n  }\n}\n\n# Usage of the custom plotting function with ellipsis\nx_data &lt;- c(1, 2, 3, 4, 5)\ny_data &lt;- c(2, 4, 6, 8, 10)\n\n# Create a line plot with additional customization (e.g., xlim, ylim)\ncustom_plot(x_data, y_data, plot_type = \"line\", xlim = c(0, 6), ylim = c(0, 12), title = \"Line Plot with Customization\")\n\n\n\n\n\n\n\n# Create a scatter plot with additional customization (e.g., pch, cex)\ncustom_plot(x_data, y_data, plot_type = \"scatter\", pch = 16, cex = 1.5, title = \"Scatter Plot with Customization\")\n\n\n\n\n\n\n\n\nExplanation:\n\nThe ... in the function definition allows for additional parameters to be passed to the plot function.\nInside the function, the plot function is called with the ... argument, allowing any additional customization options to be applied to the plot.\nIn the usage examples, additional parameters such as xlim, ylim, pch, and cex are passed to customize the appearance of the plots.\n\nWtih using ellipsis (...) the custom plotting function is more versatile, allowing users to pass any valid plotting parameters to further customize the appearance of the plots. Users can now customize the plots according to their specific needs without modifying the function itself."
  },
  {
    "objectID": "posts/2024-01-22_functions/index.html#best-practices-for-writing-functions",
    "href": "posts/2024-01-22_functions/index.html#best-practices-for-writing-functions",
    "title": "R Function Writing 101:A Journey Through Syntax, Best Practices, and More",
    "section": "Best Practices for Writing Functions",
    "text": "Best Practices for Writing Functions\nWriting functions in R is a fundamental aspect of creating efficient, readable, and maintainable code. As R enthusiasts, developers, and data scientists, adopting best practices for writing functions is crucial to ensure the quality and usability of our codebase. Whether you’re working on a small script or a large-scale project, following established guidelines can greatly enhance the clarity, modularity, and reliability of your functions.\nThis section will explore a set of best practices designed to streamline the process of function development in R. From choosing descriptive function names to documenting your code and validating inputs, each practice is geared towards fostering code that is not only functional but also comprehensible to both yourself and others. These practices are aimed at promoting consistency, minimizing errors, and facilitating collaboration by adhering to widely accepted conventions in the R programming community.\nWhether you are a novice R user or an experienced developer, integrating these best practices into your workflow will undoubtedly lead to more efficient and effective code. Let’s embark on a journey to explore the key principles that will elevate your R programming skills and empower you to create functions that are both powerful and user-friendly.\nHere are some key best practices for writing functions in R:\n\nUse Descriptive Function Names: Choose clear and descriptive names for your functions that convey their purpose. This makes the code more understandable.\n\n\n# Good example\ncalculate_mean &lt;- function(data) {\n  # Function body\n}\n\n# Avoid\nfn &lt;- function(d) {\n  # Function body\n}\n\n\nDocument Your Functions: Include comments or documentation (using #') within your function to explain its purpose, input parameters, and expected output. This helps other users (or yourself) understand how to use the function.\n\n\n# Good example\n#' Calculate the mean of a numeric vector.\n#'\n#' @param data Numeric vector for which mean is calculated.\n#' @return Mean value.\ncalculate_mean &lt;- function(data) {\n  # Function body\n}\n\n\nValidate Inputs: Check the validity of input parameters within your function. Ensure that the inputs meet the expected format and constraints.\n\n\n# Good example\ncalculate_mean &lt;- function(data) {\n  if (!is.numeric(data)) {\n    stop(\"Input must be a numeric vector.\")\n  }\n  # Function body\n}\n\n\nAvoid Global Variables: Minimize the use of global variables within your functions. Instead, pass required parameters as arguments to make functions more modular and reusable.\n\n\n# Good example\ncalculate_mean &lt;- function(data) {\n  # Function body using 'data'\n}\n\n\nSeparate Concerns: Divide your code into modular and focused functions, each addressing a specific concern. This promotes reusability and makes your code more maintainable.\n\n\n# Good example\ncalculate_mean &lt;- function(data) {\n  # Function body\n}\n\nplot_histogram &lt;- function(data) {\n  # Function body\n}\n\n\nAvoid Global Side Effects: Minimize changes to global variables within your functions. Functions should ideally return results rather than modifying global states.\n\n\n# Good example\ncalculate_mean &lt;- function(data) {\n  result &lt;- mean(data)\n  return(result)\n}\n\n\nUse Default Argument Values: Set default values for function arguments when it makes sense. This improves the usability of your functions by allowing users to omit optional arguments.\n\n\n# Good example\ncalculate_mean &lt;- function(data, na.rm = FALSE) {\n  result &lt;- mean(data, na.rm = na.rm)\n  return(result)\n}\n\n\nTest Your Functions: Develop test cases to ensure that your functions behave as expected. Testing helps catch bugs early and provides confidence in the reliability of your code.\n\n\n# Good example (using testthat package)\ntest_that(\"calculate_mean returns the correct result\", {\n  data &lt;- c(1, 2, 3, 4, 5)\n  result &lt;- calculate_mean(data)\n  expect_equal(result, 3)\n})\n\nBy following these best practices, you can create functions that are more robust, understandable, and adaptable, contributing to the overall quality of your R code."
  },
  {
    "objectID": "posts/2024-01-22_functions/index.html#conclusion",
    "href": "posts/2024-01-22_functions/index.html#conclusion",
    "title": "R Function Writing 101:A Journey Through Syntax, Best Practices, and More",
    "section": "Conclusion",
    "text": "Conclusion\nMastering the art of writing functions in R is essential for efficient and organized programming. Whether you’re performing simple calculations or tackling complex problems, functions empower you to write cleaner, more maintainable code. By following best practices and exploring diverse examples, you can elevate your R programming skills and unleash the full potential of this versatile language.\nAs we reach the conclusion of our exploration, take a moment to appreciate the symphony of gears turning—a reflection of the interconnected brilliance of functions in R. From simple calculations to complex algorithms, each function plays a vital role in the harmony of your code.\nArmed with a deeper understanding of syntax, best practices, and real-world examples, you now possess the tools to craft efficient and organized functions. Like a well-tuned machine, let your code operate smoothly, with each function contributing to the overall success of your programming endeavors.\nHappy coding, and may your gears always turn with precision! 🚀⚙️"
  },
  {
    "objectID": "posts/2023-12-29_dataframes/index.html",
    "href": "posts/2023-12-29_dataframes/index.html",
    "title": "Unraveling DataFrames in R: A Comprehensive Guide",
    "section": "",
    "text": "https://openscapes.org/blog/2020-10-12-tidy-data/\n\n\nIn R, a data frame is a fundamental data structure used for storing data in a tabular format, similar to a spreadsheet or a database table. It’s a collection of vectors of equal length arranged as columns. Each column can contain different types of data (numeric, character, factor, etc.), but within a column, all elements must be of the same data type.\nData frames are incredibly versatile and commonly used for data manipulation, analysis, and statistical operations in R. They allow you to work with structured data, perform operations on columns and rows, filter and subset data, and apply various statistical functions."
  },
  {
    "objectID": "posts/2023-12-29_dataframes/index.html#introduction",
    "href": "posts/2023-12-29_dataframes/index.html#introduction",
    "title": "Unraveling DataFrames in R: A Comprehensive Guide",
    "section": "",
    "text": "https://openscapes.org/blog/2020-10-12-tidy-data/\n\n\nIn R, a data frame is a fundamental data structure used for storing data in a tabular format, similar to a spreadsheet or a database table. It’s a collection of vectors of equal length arranged as columns. Each column can contain different types of data (numeric, character, factor, etc.), but within a column, all elements must be of the same data type.\nData frames are incredibly versatile and commonly used for data manipulation, analysis, and statistical operations in R. They allow you to work with structured data, perform operations on columns and rows, filter and subset data, and apply various statistical functions."
  },
  {
    "objectID": "posts/2023-12-29_dataframes/index.html#main-properties-of-dataframes",
    "href": "posts/2023-12-29_dataframes/index.html#main-properties-of-dataframes",
    "title": "Unraveling DataFrames in R: A Comprehensive Guide",
    "section": "Main Properties of Dataframes",
    "text": "Main Properties of Dataframes\nData frames in R possess several key properties that make them widely used for data manipulation and analysis:\n\nTabular Structure: Data frames organize data in a tabular format, resembling a table or spreadsheet, with rows and columns.\nColumns of Varying Types: Each column in a data frame can contain different types of data (numeric, character, factor, etc.). However, all elements within a column must be of the same data type.\nEqual Length Vectors: Columns are essentially vectors, and all columns within a data frame must have the same length. This ensures that each row corresponds to a complete set of observations across all variables.\nColumn Names: Data frames have column names that facilitate accessing and referencing specific columns using these names. Column names must be unique within a data frame.\nRow Names or Indices: Similar to columns, data frames have row names or indices, which help identify and reference specific rows. By default, rows are numbered starting from 1 unless row names are explicitly provided.\nData Manipulation: Data frames offer various functions and methods for data manipulation, including subsetting, filtering, merging, reshaping, and transforming data.\nCompatibility with Libraries: Data frames are the primary data structure used in many R packages and libraries for statistical analysis, data visualization, and machine learning. Most functions and tools in R are designed to work seamlessly with data frames.\nIntegration with R Syntax: R provides a rich set of functions and operators that can be directly applied to data frames, allowing for efficient data manipulation, analysis, and visualization.\n\nUnderstanding these properties helps users effectively manage and analyze data using data frames in R."
  },
  {
    "objectID": "posts/2023-12-29_dataframes/index.html#creating-dataframes",
    "href": "posts/2023-12-29_dataframes/index.html#creating-dataframes",
    "title": "Unraveling DataFrames in R: A Comprehensive Guide",
    "section": "Creating Dataframes",
    "text": "Creating Dataframes\nCreating a data frame in R can be done in several ways, such as manually inputting data, importing from external sources like CSV files, or generating it using functions. Here are a few common methods to create a data frame:\n\nMethod 1: Manual Creation\n\n# Creating a data frame manually\nnames &lt;- c(\"Alice\", \"Bob\", \"Charlie\", \"David\")\nages &lt;- c(25, 30, 28, 35)\nscores &lt;- c(88, 92, 75, 80)\n\n# Creating a data frame using the data\ndf &lt;- data.frame(Name = names, Age = ages, Score = scores)\nprint(df)\n\n     Name Age Score\n1   Alice  25    88\n2     Bob  30    92\n3 Charlie  28    75\n4   David  35    80\n\n\n\n\nMethod 2: Importing Data\nIn R, you can import data from various file formats to create DataFrames. Commonly used functions for importing data include read.csv(), read.table(), read.delim(), or read_excel from readxl package and more, each catering to specific file formats or data structures.\nFrom CSV:\n\n# Reading data from a CSV file into a data frame\ndf &lt;- read.csv(\"file.csv\")  # Replace \"file.csv\" with your file path\n\nFrom Excel (using readxl package):\n\n# Installing the readxl package if not installed\n# install.packages(\"readxl\")\n\nlibrary(readxl)\n\n# Importing an Excel file into a DataFrame\ndata &lt;- read_excel(\"file.xlsx\")\n\nSpecify the sheet name or number with sheet parameter if your Excel file contains multiple sheets.\n\n\nMethod 3: Generating Data\nUsing built-in functions:\n\n# Creating a data frame with sequences and vectors\nnames &lt;- c(\"Alice\", \"Bob\", \"Charlie\", \"David\")\nages &lt;- seq(from = 20, to = 35, by = 5)\nscores &lt;- sample(70:100, 4, replace = TRUE)\n\n# Creating a data frame using the data generated\ndf &lt;- data.frame(Name = names, Age = ages, Score = scores)\nprint(df)\n\n     Name Age Score\n1   Alice  20    76\n2     Bob  25    87\n3 Charlie  30    82\n4   David  35    96\n\n\n\n\nMethod 4: Combining Existing Data Frames\n\n# Creating two data frames\ndf1 &lt;- data.frame(ID = 1:3, Name = c(\"Alice\", \"Bob\", \"Charlie\"))\ndf2 &lt;- data.frame(ID = 2:4, Score = c(88, 92, 75))\n\n# Merging the two data frames by a common column (ID)\nmerged_df &lt;- merge(df1, df2, by = \"ID\")\nprint(merged_df)\n\n  ID    Name Score\n1  2     Bob    88\n2  3 Charlie    92\n\n\nThese methods provide flexibility in creating data frames from existing data, generating synthetic data, or importing data from external sources, making it easier to work with data in R."
  },
  {
    "objectID": "posts/2023-12-29_dataframes/index.html#accessing-elements-of-data-frames",
    "href": "posts/2023-12-29_dataframes/index.html#accessing-elements-of-data-frames",
    "title": "Unraveling DataFrames in R: A Comprehensive Guide",
    "section": "Accessing Elements of Data Frames",
    "text": "Accessing Elements of Data Frames\nUnderstanding how to access and manipulate elements within these data frames is fundamental for data analysis, transformation, and exploration. Here, we’ll explore the various methods to access specific elements within a data frame in R.\nLet’s begin by creating a sample dataset that simulates student information.\n\n# Sample data frame creation\nstudent_id &lt;- 1:5\nstudent_names &lt;- c(\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eva\")\nages &lt;- c(20, 22, 21, 23, 20)\nscores &lt;- c(85, 90, 78, 92, 88)\n\nstudents &lt;- data.frame(ID = student_id, Name = student_names, Age = ages, Score = scores)\n\n\nAccessing Entire Columns\nThe simplest way to access a column in a data frame is by using the $ , [ or [[ operator followed by the column name.\n\n# Accessing the 'Name' column using $\nstudents$Name\n\n[1] \"Alice\"   \"Bob\"     \"Charlie\" \"David\"   \"Eva\"    \n\n# Accessing the 'Age' column using double brackets [ ]\nstudents[\"Score\"]\n\n  Score\n1    85\n2    90\n3    78\n4    92\n5    88\n\n# Accessing the 'Age' column using double brackets [[ ]]\nstudents[[\"Age\"]]\n\n[1] 20 22 21 23 20\n\n\n\n\nAccessing Specific Rows and Columns\nTo access specific rows and columns, square brackets [rows, columns] are used. In R, the comma inside square brackets [ ] is used to index elements in two-dimensional data structures like matrices and data frames. It separates the row indices from the column indices, enabling access to specific rows and columns or both simultaneously.\n\n# Accessing rows 2 to 4 and columns 1 to 3\nstudents[2:4, 1:3]\n\n  ID    Name Age\n2  2     Bob  22\n3  3 Charlie  21\n4  4   David  23\n\n# Accessing specific rows and columns by name\nstudents[c(\"1\", \"3\"), c(\"Name\", \"Score\")]\n\n     Name Score\n1   Alice    85\n3 Charlie    78\n\n\n\n\nAccessing Individual Elements\nAccessing individual elements involves specifying row and column indices.\n\n# Accessing a single element in row 3, column 2\nstudents[3, 2]\n\n[1] \"Charlie\"\n\n# Accessing a single element by row and column names\nstudents[\"3\", \"Name\"]\n\n[1] \"Charlie\"\n\n\n\n\nLogical Indexing\nLogical conditions can be used to subset data. Logical indexing in R involves using logical conditions to extract specific elements or subsets of data that satisfy certain criteria. It’s a powerful technique applicable to data frames, matrices, and vectors, allowing for flexible data selection based on conditions.\n\n# Accessing rows where Age is greater than 20\nstudents[students$Age &gt; 20, ]\n\n  ID    Name Age Score\n2  2     Bob  22    90\n3  3 Charlie  21    78\n4  4   David  23    92\n\n# Selecting rows where Age is greater than 25 and Score is above 80\nstudents[students$Age &gt; 20 & students$Score &gt; 80, ]\n\n  ID  Name Age Score\n2  2   Bob  22    90\n4  4 David  23    92\n\n\nMastering these techniques for accessing elements within data frames empowers efficient data exploration and extraction, vital for comprehensive data analysis in R. Of course there are other options. For example, The dplyr package offers enhanced functionalities for data manipulation.\n\n\n\n\n\n\nNote\n\n\n\nThe dplyr package is a fundamental R package designed for efficient data manipulation and transformation. Developed by Hadley Wickham, dplyr provides a set of functions that streamline data processing tasks, making it easier to work with data frames. I plan to write about data manipulation processes related to this package in the future."
  },
  {
    "objectID": "posts/2023-12-29_dataframes/index.html#modern-dataframe-tibble",
    "href": "posts/2023-12-29_dataframes/index.html#modern-dataframe-tibble",
    "title": "Unraveling DataFrames in R: A Comprehensive Guide",
    "section": "Modern Dataframe: Tibble",
    "text": "Modern Dataframe: Tibble\nA tibble is a modern and enhanced version of the traditional data frame in R, introduced as part of the tibble package. Tibbles share many similarities with data frames but offer some improvements and differences in their behavior and structure.\n\nKey Differences Between Tibbles and Data Frames\n\nPrinting Method: Data frames print only a few rows and columns, while tibbles print the first 10 rows and all columns. This improves readability for larger datasets.\nSubsetting Behavior: Tibbles do not use row names in the same way as data frames. In data frames, row names are included as a separate column when subsetting. Tibbles do not have this behavior, offering a more consistent experience.\nColumn Types: Tibbles handle column types differently. They never automatically convert character vectors to factors, which is a default behavior in data frames. This helps prevent unexpected type conversions.\nConsole Output: When printing to the console, tibbles present data in a more organized and user-friendly manner compared to data frames. This makes it easier to inspect the data.\n\n\n\nBenefits of Tibbles\n\nImproved Printing: Tibbles offer better printing capabilities, displaying a concise summary of data, making it easier to view and understand larger datasets.\nConsistency: Tibbles have a more consistent behavior across different operations, reducing unexpected behavior compared to data frames.\nModern Data Handling: Designed to address some of the limitations and quirks of data frames, tibbles provide a more modern approach to working with tabular data in R.\n\n\n\nCreating Tibbles\n\n# Creating a tibble from a data frame\nlibrary(tibble)\n\n# Creating a tibble\nmy_tibble &lt;- tibble(\n  column1 = c(1, 2, 3),\n  column2 = c(\"A\", \"B\", \"C\")\n)\n\nmy_tibble\n\n# A tibble: 3 × 2\n  column1 column2\n    &lt;dbl&gt; &lt;chr&gt;  \n1       1 A      \n2       2 B      \n3       3 C      \n\n\n\n\nWhen to Use Tibbles\n\nFor data analysis and exploration tasks where improved printing and consistency in behavior are preferred.\nWhen working with larger datasets or in situations where the traditional data frame’s default behaviors might cause confusion.\n\nTibbles and data frames share many similarities, but tibbles offer a more modern and streamlined experience for handling tabular data in R, addressing some of the idiosyncrasies of data frames. They are designed to improve data manipulation and readability, especially for larger datasets."
  },
  {
    "objectID": "posts/2023-12-29_dataframes/index.html#conclusion",
    "href": "posts/2023-12-29_dataframes/index.html#conclusion",
    "title": "Unraveling DataFrames in R: A Comprehensive Guide",
    "section": "Conclusion",
    "text": "Conclusion\nBoth data frames and tibbles are valuable structures for working with tabular data in R. The choice between them often depends on the specific needs of the analysis and personal preferences. Data frames remain a solid choice, especially for users accustomed to their behavior and functionality. On the other hand, tibbles offer a more streamlined and user-friendly experience, particularly when working with larger datasets and when consistency in behavior is paramount. Ultimately, the decision to use data frames or tibbles depends on factors like data size, printing preferences, and desired consistency in data handling. Both structures play vital roles in R’s ecosystem, providing essential tools for data manipulation, analysis, and exploration."
  },
  {
    "objectID": "posts/2023-11-20_matrices/index.html",
    "href": "posts/2023-11-20_matrices/index.html",
    "title": "Understanding Matrices in R Programming",
    "section": "",
    "text": "https://www.vectorstock.com/royalty-free-vector/green-matrix-numbers-cyberspace-with-vector-24906241\n\n\nMatrices are an essential data structure in R programming that allows for the manipulation and analysis of data in a two-dimensional format. Understanding their creation, manipulation, and linear algebra operations is crucial for handling complex data effectively. They provide a convenient way to store and work with data that can be represented as rows and columns. In this post, we will delve into the basics of creating, manipulating, and operating on matrices in R. Especially, we discuss how to perform basic algebraic operations such as matrix multiplication, transpose, finding eigenvalues. We also cover data wrangling operations such as matrix subsetting and column- and rowwise aggregation."
  },
  {
    "objectID": "posts/2023-11-20_matrices/index.html#introduction",
    "href": "posts/2023-11-20_matrices/index.html#introduction",
    "title": "Understanding Matrices in R Programming",
    "section": "",
    "text": "https://www.vectorstock.com/royalty-free-vector/green-matrix-numbers-cyberspace-with-vector-24906241\n\n\nMatrices are an essential data structure in R programming that allows for the manipulation and analysis of data in a two-dimensional format. Understanding their creation, manipulation, and linear algebra operations is crucial for handling complex data effectively. They provide a convenient way to store and work with data that can be represented as rows and columns. In this post, we will delve into the basics of creating, manipulating, and operating on matrices in R. Especially, we discuss how to perform basic algebraic operations such as matrix multiplication, transpose, finding eigenvalues. We also cover data wrangling operations such as matrix subsetting and column- and rowwise aggregation."
  },
  {
    "objectID": "posts/2023-11-20_matrices/index.html#creating-matrices-in-r",
    "href": "posts/2023-11-20_matrices/index.html#creating-matrices-in-r",
    "title": "Understanding Matrices in R Programming",
    "section": "Creating Matrices in R",
    "text": "Creating Matrices in R\nMatrices can be created and analyzed in a few different ways in R. One way is to create the matrix yourself. There are a few different ways you can do this.\nThe matrix(a, nrow = b, ncol = c) command in R creates a matrix that repeats the element a in a matrix with b rows and c columns. A matrix can be manually created by using the c() command as well.\n\n# Creating a matrix including only 1's that are 2 by 3\nmatrix(1, nrow = 2, ncol = 3)\n\n     [,1] [,2] [,3]\n[1,]    1    1    1\n[2,]    1    1    1\n\n\nIf you want to create the following matrix:\n\\[\nA=\\begin{bmatrix}\n1&2&3\\\\\n3&6&8\\\\\n7&8&4\n\\end{bmatrix}\n\\]\nyou would do it like this:\n\nA &lt;- matrix(c(1, 2, 3, 3, 6, 8, 7, 8, 4), nrow = 3, byrow = TRUE)\nA\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    3    6    8\n[3,]    7    8    4\n\n\nIt converted an atomic vector of length nine to a matrix with three rows. The number of columns was determined automatically (ncol=3 could have been passed to get the same result). The option byrow = TRUE means that the rows of the matrix will be filled first. By default, the elements of the input vector are read column by column.\n\nmatrix(c(1, 2, 3, 3, 6, 8, 7, 8, 4), nrow = 3)\n\n     [,1] [,2] [,3]\n[1,]    1    3    7\n[2,]    2    6    8\n[3,]    3    8    4\n\n\nMatrices can also be created by concatenating multiple vectors. rbind performs row-based bottom-to-bottom concatenation, while cbind performs column-based side-by-side concatenation.\n\n\n\n\n\n\nCaution\n\n\n\nHere it is important to make sure that the vectors have the same dimensions.\n\n\n\nv1 &lt;- c(3,4,6,8,5)\nv2 &lt;- c(4,8,4,7,1)\nv3 &lt;- c(2,2,5,4,6)\nv4 &lt;- c(4,7,5,2,5)\nm1 &lt;- cbind(v1, v2, v3, v4)\nprint(m1)\n\n     v1 v2 v3 v4\n[1,]  3  4  2  4\n[2,]  4  8  2  7\n[3,]  6  4  5  5\n[4,]  8  7  4  2\n[5,]  5  1  6  5\n\ndim(m1)\n\n[1] 5 4\n\n\nIn this example, 4 vectors with 5 observations are merged side by side with cbind. This results in a 5x4 matrix, which we call m1.\n\nm2 &lt;- rbind(v1, v2, v3, v4)\nprint(m2)\n\n   [,1] [,2] [,3] [,4] [,5]\nv1    3    4    6    8    5\nv2    4    8    4    7    1\nv3    2    2    5    4    6\nv4    4    7    5    2    5\n\ndim(m2)\n\n[1] 4 5\n\n\nWith this example, 4 vectors are merged one below the other with rbind. As a result, a matrix of size 4x5, which we call m2, is obtained. We used dim function to learn dimension of matrices."
  },
  {
    "objectID": "posts/2023-11-20_matrices/index.html#accessing-and-modifying-elements",
    "href": "posts/2023-11-20_matrices/index.html#accessing-and-modifying-elements",
    "title": "Understanding Matrices in R Programming",
    "section": "Accessing and Modifying Elements",
    "text": "Accessing and Modifying Elements\nAccessing and modifying elements in a matrix is straightforward. Use the row and column indices to access specific elements and assign new values to modify elements.\n\n# Accessing the element in the second row and third column\nm1[2, 3] \n\nv3 \n 2 \n\n# Modifying the element at the specified position\nm1[2, 3] &lt;- 10  \nprint(m1)\n\n     v1 v2 v3 v4\n[1,]  3  4  2  4\n[2,]  4  8 10  7\n[3,]  6  4  5  5\n[4,]  8  7  4  2\n[5,]  5  1  6  5\n\n\nAlso, rows and columns of matrices can be named by using colnames and rownames functions.\n\n# Naming columns with the first 4 letters\ncolnames(m1) &lt;- LETTERS[1:4] \nm1 \n\n     A B  C D\n[1,] 3 4  2 4\n[2,] 4 8 10 7\n[3,] 6 4  5 5\n[4,] 8 7  4 2\n[5,] 5 1  6 5\n\n# Naming rows with the last 5 letters\nrownames(m1) &lt;- tail(LETTERS,5) \nm1\n\n  A B  C D\nV 3 4  2 4\nW 4 8 10 7\nX 6 4  5 5\nY 8 7  4 2\nZ 5 1  6 5"
  },
  {
    "objectID": "posts/2023-11-20_matrices/index.html#mathematical-operations",
    "href": "posts/2023-11-20_matrices/index.html#mathematical-operations",
    "title": "Understanding Matrices in R Programming",
    "section": "Mathematical Operations",
    "text": "Mathematical Operations\nVectorised functions such as round, sqrt, abs, log,exp etc., operate on each matrix element.\n\nA &lt;- matrix(c(1:6) * 0.15,nrow = 2)\nA\n\n     [,1] [,2] [,3]\n[1,] 0.15 0.45 0.75\n[2,] 0.30 0.60 0.90\n\nsqrt(A) # gets square root of every element in A\n\n          [,1]      [,2]      [,3]\n[1,] 0.3872983 0.6708204 0.8660254\n[2,] 0.5477226 0.7745967 0.9486833\n\nround(A, 1) # rounds every element in A\n\n     [,1] [,2] [,3]\n[1,]  0.1  0.4  0.8\n[2,]  0.3  0.6  0.9\n\n\nMathematical operations such as addition and subtraction can be performed on two or more matrices with the same dimensions. The operation performed here is elementwise.\n\nA &lt;- matrix(1:4,nrow=2)\nB &lt;- matrix(5:8,nrow=2)\nprint(A)\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nprint(B)\n\n     [,1] [,2]\n[1,]    5    7\n[2,]    6    8\n\nA + B  # elementwise addition\n\n     [,1] [,2]\n[1,]    6   10\n[2,]    8   12\n\nA * B  # elementwise multiplication\n\n     [,1] [,2]\n[1,]    5   21\n[2,]   12   32\n\n\nThey are simply the addition and multiplication of the corresponding elements of two given matrices. Also we can we can apply matrix-scalar operations. For example in the next example we squared every element in A.\n\nA^2 # the 2nd power of the A\n\n     [,1] [,2]\n[1,]    1    9\n[2,]    4   16"
  },
  {
    "objectID": "posts/2023-11-20_matrices/index.html#aggregating-rows-and-columns",
    "href": "posts/2023-11-20_matrices/index.html#aggregating-rows-and-columns",
    "title": "Understanding Matrices in R Programming",
    "section": "Aggregating rows and columns",
    "text": "Aggregating rows and columns\nWhen we call an aggregation function on a matrix, it will reduce all elements to a single number.\n\nmean(A) # get arithmetic mean of A\n\n[1] 2.5\n\nmin(A) # minimum of A\n\n[1] 1\n\n\nWe can also calculate sum or mean of each row/columns by using rowMeans, rowSums, colMeans and colSums.\n\nrowSums(A) # sum of rows\n\n[1] 4 6\n\nrowMeans(A) # mean of rows\n\n[1] 2 3\n\ncolSums(A) # sum of columns\n\n[1] 3 7\n\ncolMeans(A) # mean of columns\n\n[1] 1.5 3.5\n\n\n\n\n\n\n\n\nTip\n\n\n\nR provides the apply() function to apply functions to each row or column of a matrix. The arguments of the apply() function include the matrix, the margin (1 for rows, 2 for columns), and the function to be applied. The apply function can be used to summarise individual rows or columns in a matrix. So we call any aggregation function with apply.\n\napply(A, 1, f) applies a given function f on each row of a matrix A (over the first axis),\napply(A, 2, f) applies f on each column of A (over the second axis).\n\n\n# Applying functions to matrices\nrow_sums &lt;- apply(A, 1, sum)  # Applying sum function to each row (margin = 1)\nprint(row_sums)\n\n[1] 4 6"
  },
  {
    "objectID": "posts/2023-11-20_matrices/index.html#matrix-operations",
    "href": "posts/2023-11-20_matrices/index.html#matrix-operations",
    "title": "Understanding Matrices in R Programming",
    "section": "Matrix Operations",
    "text": "Matrix Operations\n\nTranspose of Matrix\nThe transpose of matrix, mathematically denoted with \\[A^T\\] is available by using t() function.\n\nA\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nt(A) # transpose of A\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\n\n\n\nMatrix Calculation\nWhen multiplying two matrices A and B, the number of columns in matrix A must be equal to the number of rows in matrix B. If A is of size m x n and B is of size n x p, then their product AB will be of size m x p. The individual elements of the resulting matrix are calculated by taking dot products of rows from matrix A and columns from matrix B.\nIn R * performs elementwise multiplication. For what we call the (algebraic) matrix multiplication, we use the %*% operator. It can only be performed on two matrices of compatible sizes: the number of columns in the left matrix must match the number of rows in the right operand.\n\nA &lt;- matrix(c(1, 3, 5 ,3, 4, 9), nrow = 2) # create 2 by 3 matrix\nB &lt;- matrix(c(6, 2, 4 ,7, 8, 4), nrow = 3) # create 3 by 2 matrix\nprint(A)\n\n     [,1] [,2] [,3]\n[1,]    1    5    4\n[2,]    3    3    9\n\nprint(B)\n\n     [,1] [,2]\n[1,]    6    7\n[2,]    2    8\n[3,]    4    4\n\nA %*% B # we get 2 by 2 matrix\n\n     [,1] [,2]\n[1,]   32   63\n[2,]   60   81\n\n\n\n\nDeterminant of Matrix\nThe determinant of a square matrix is a scalar value that represents some important properties of the matrix. In R programming, the det() function is used to calculate the determinant of a square matrix.\nUnderstanding Determinant:\n\nSquare Matrices: The determinant is a property specific to square matrices, meaning the number of rows must equal the number of columns.\nGeometric Interpretation: For a 2x2 matrix \\(\\mathbf{}\\left[\\begin{array}{rrr}a & b \\\\c & d \\end{array}\\right]\\) the determinant \\(ad-bc\\) represents the scaling factor of the area spanned by vectors formed by the columns of the matrix. For higher-dimensional matrices, the determinant has a similar geometric interpretation related to volume and scaling in higher dimensions.\nInvertibility: A matrix is invertible (has an inverse) if and only if its determinant is non-zero. If the determinant is zero, the matrix is singular and does not have an inverse.\n\nIn R, the det() function computes the determinant of a square matrix.\n\n# Create a square matrix\nA &lt;- matrix(c(1, 2, 3, 4), nrow = 2, ncol = 2)\n\n# Compute the determinant of the matrix\ndet(A)\n\n[1] -2\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIt’s essential to note a few considerations:\n\nNumerical Stability: Computing determinants of large matrices or matrices close to singularity (having a determinant close to zero) can lead to numerical instability due to rounding errors.\nComplexity: The computational complexity of determinant calculation increases rapidly with matrix size, especially for algorithms like cofactor expansion or LU decomposition used internally.\nUse in Linear Algebra: Determinants play a vital role in linear algebra, being used in solving systems of linear equations, calculating inverses of matrices, and understanding transformations and eigenvalues.\nSingular Matrices: If the determinant of a square matrix is zero, it signifies that the matrix is singular and not invertible.\n\n\n\nHere’s an example that checks the determinant and its relation to matrix invertibility:\n\n# Check the determinant and invertibility of a matrix\ndet_A &lt;- det(A)\n\nif (det_A != 0) {\n  print(\"Matrix is invertible.\")\n} else {\n  print(\"Matrix is singular, not invertible.\")\n}\n\n[1] \"Matrix is invertible.\"\n\n\nUnderstanding determinants is crucial in various mathematical applications, especially in linear algebra and systems of equations, as they provide valuable information about the properties of matrices and their behavior in transformations and computations.\n\n\nInverse of Matrix\nsolve() function is used to compute the inverse of a square matrix. The inverse of a matrix \\(A\\) is denoted as \\(A^{-1}\\) and has the property that when multiplied by the original matrix A, it yields the identity matrix I.\n\nprint(A)\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n# Compute the inverse of the matrix\nsolve(A)\n\n     [,1] [,2]\n[1,]   -2  1.5\n[2,]    1 -0.5\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIt’s important to note a few things about matrix inversion:\n\nSquare Matrices: The matrix must be square (i.e., the number of rows equals the number of columns) to have an inverse. Inverting a non-square matrix is not possible.\nDeterminant Non-Zero: The matrix must have a non-zero determinant for its inverse to exist. If the determinant is zero, the matrix is singular, and its inverse cannot be computed.\nErrors and Numerical Stability: Inverting matrices can be sensitive to numerical precision and errors, especially for matrices that are close to singular or ill-conditioned. Rounding errors can affect the accuracy of the computed inverse.\n\nIn practice, it’s essential to check the properties of the matrix, such as its determinant, before attempting to compute its inverse, especially when dealing with real-world data, as numerical issues can lead to unreliable results.\n\n\nHere’s an example that checks the determinant before computing the inverse:\n\n# Check the determinant before inverting the matrix\ndet_A &lt;- det(A)\n\nif (det_A != 0) {\n  inverse_matrix_A &lt;- solve(A)\n  print(inverse_matrix_A)\n} else {\n  print(\"Matrix is singular, inverse does not exist.\")\n}\n\n     [,1] [,2]\n[1,]   -2  1.5\n[2,]    1 -0.5\n\n\nUnderstanding matrix inversion is crucial in various fields like machine learning, optimization, and solving systems of linear equations, as it allows for the transformation of equations or operations involving matrices to simplify computations. However, always ensure that the matrix you’re working with satisfies the conditions for invertibility to avoid computational errors.\n\n\nEigenvalues and Eigenvectors\nIn R programming, eigenvalues and eigenvectors are fundamental concepts often computed using the eigen() function. These are important in various fields, including linear algebra, data analysis, signal processing, and machine learning.\nEigenvalues: They are scalar values that represent how a linear transformation (represented by a square matrix) behaves along its eigenvectors. For a square matrix A, an eigenvalue (\\(\\lambda\\)) and its corresponding eigenvector (\\(v\\)) satisfy the equation \\(Av=\\lambda v\\). It essentially means that when the matrix A operates on the eigenvector \\(v\\), the resulting vector is a scaled version of the original eigenvector \\(v\\), scaled by the eigenvalue \\(\\lambda\\).\nIn R, you can compute eigenvalues using the eigen() function.\n\n# Create a sample matrix\nA &lt;- matrix(c(4, 2, 1, -1), nrow = 2, byrow = TRUE)\n\n# Compute eigenvalues and eigenvectors\neig &lt;- eigen(A)\n\n# Access eigenvalues\neigenvalues &lt;- eig$values\nprint(eigenvalues)\n\n[1]  4.372281 -1.372281\n\n\nEigenvectors: They are non-zero vectors that are transformed only by a scalar factor when a linear transformation (represented by a matrix) is applied. Each eigenvalue has an associated eigenvector. Eigenvectors are important because they describe the directions along which the transformation represented by the matrix has a simple behavior, often stretching or compressing without changing direction.\nIn R, after computing the eigenvalues using eigen(), you can access the corresponding eigenvectors using:\n\n# Access eigenvectors\neigenvectors &lt;- eig$vectors\nprint(eigenvectors)\n\n          [,1]       [,2]\n[1,] 0.9831134 -0.3488887\n[2,] 0.1829974  0.9371642\n\n\nThese eigenvalues and eigenvectors play a significant role in various applications, including principal component analysis (PCA), diagonalization of matrices, solving systems of differential equations, and more. They provide crucial insights into the behavior and characteristics of linear transformations represented by matrices."
  },
  {
    "objectID": "posts/2023-11-20_matrices/index.html#conclusion",
    "href": "posts/2023-11-20_matrices/index.html#conclusion",
    "title": "Understanding Matrices in R Programming",
    "section": "Conclusion",
    "text": "Conclusion\nMatrices are indeed useful and statisticians are used to working with them. However, in my daily work I try to use matrices as needed and prefer an approach based on data frames, because working with data frames makes it easier to use R’s advanced functional programming language capabilities. I plan to publish a post on data frames in the future, and in the conclusion of this post I would like to discuss the advantages and disadvantages of both matrices and data frames.\nIn R programming, matrices and data frames serve different purposes, each with its own set of advantages and limitations.\nMatrices:\nPros:\n\nEfficient for Numeric Operations: Matrices are optimized for numerical computations. If you’re working primarily with numeric data and need to perform matrix algebra, calculations tend to be faster with matrices compared to data frames.\nHomogeneous Data: Matrices are homogeneous, meaning they store elements of the same data type (numeric, character, logical, etc.) throughout. This consistency simplifies some computations and analyses.\nMathematical Operations: Matrices are designed for linear algebra operations. Functions like matrix multiplication, transposition, and eigenvalue/eigenvector calculations are native to matrices in R.\n\nCons:\n\nLack of Flexibility: Matrices are restrictive when it comes to handling heterogeneous data or combining different data types within the same structure. They can only hold a single data type.\nRow and Column Names: Matrices do not inherently support row or column names, which might be necessary for better data representation and interpretation.\n\nData Frames:\nPros:\n\nHeterogeneous Data: Data frames can store different types of data (numeric, character, factor, etc.) within the same structure. This flexibility allows for handling diverse datasets efficiently.\nRow and Column Names: Data frames support row and column names, making it easier to reference specific rows or columns and improving data readability.\nData Manipulation and Analysis: R’s data manipulation libraries (e.g., dplyr, tidyr) are optimized for data frames. They offer a wide range of functions and operations tailored for efficient data manipulation, summarization, and analysis.\n\nCons:\n\nPerformance: Compared to matrices, data frames might have slower performance for numerical computations involving large datasets due to their heterogeneous nature and additional data structure overhead.\nOverhead for Numeric Operations: While data frames are versatile for handling different types of data, when it comes to pure numeric computations or linear algebra operations, they might be less efficient than matrices.\n\nIn summary, the choice between matrices and data frames in R depends on the nature of the data and the intended operations. If you’re working mainly with numeric data and require linear algebra operations, matrices might be more efficient. By understanding their creation, manipulation, operations, and application in advanced techniques like PCA, you can effectively handle complex data structures and perform sophisticated computations with ease. On the other hand, if you’re dealing with heterogeneous data and need more flexibility in data manipulation and analysis, data frames are a better choice. Often, data frames are preferred for general-purpose data handling and analysis due to their versatility, despite potential performance trade-offs for specific numerical operations."
  },
  {
    "objectID": "posts/2023-09-25_data_types/index.html",
    "href": "posts/2023-09-25_data_types/index.html",
    "title": "Understanding Data Types in R",
    "section": "",
    "text": "Learning R programming is akin to constructing a sturdy building. You need a powerful foundation to support the structure. Just as a building’s foundation dictates its strength and stability, a strong understanding of data types and data structures is essential when working with R. Data types and data structures are fundamental concepts in any programming language, and R is no exception. R offers a rich set of data types and versatile data structures that enable you to work with data efficiently and effectively. In this post, we will explore the critical concepts of data types and data structures in R programming and emphasizing their foundational importance. We’ll delve into the primary data structures used to organize and manipulate data, all illustrated with practical examples."
  },
  {
    "objectID": "posts/2023-09-25_data_types/index.html#introduction",
    "href": "posts/2023-09-25_data_types/index.html#introduction",
    "title": "Understanding Data Types in R",
    "section": "",
    "text": "Learning R programming is akin to constructing a sturdy building. You need a powerful foundation to support the structure. Just as a building’s foundation dictates its strength and stability, a strong understanding of data types and data structures is essential when working with R. Data types and data structures are fundamental concepts in any programming language, and R is no exception. R offers a rich set of data types and versatile data structures that enable you to work with data efficiently and effectively. In this post, we will explore the critical concepts of data types and data structures in R programming and emphasizing their foundational importance. We’ll delve into the primary data structures used to organize and manipulate data, all illustrated with practical examples."
  },
  {
    "objectID": "posts/2023-09-25_data_types/index.html#data-types-in-r",
    "href": "posts/2023-09-25_data_types/index.html#data-types-in-r",
    "title": "Understanding Data Types in R",
    "section": "Data Types in R",
    "text": "Data Types in R\nR provides several data types that allow you to represent different kinds of information. Here are some of the key data types in R:\n\nNumeric\nThe numeric data type represents real numbers. It includes both integers and floating-point numbers. In R, both the “numeric” and “double” data types essentially represent numeric values, but there is a subtle difference in how they are stored internally and how they handle decimal precision. Let’s delve into the specifics of each:\nNumeric Data Type:\n\nThe “numeric” data type in R is the more general term used for any numerical data, including both integers and floating-point numbers (doubles).\nIt is typically used when you don’t need to specify a particular type, and R will automatically assign the “numeric” data type to variables containing numbers.\nNumeric values can include integers, such as 1, 42, or 1000, but they can also include decimal values, such as 3.14 or -0.005.\nNumeric variables can have values with varying levels of precision depending on the specific number. For example, integers are represented precisely, while floating-point numbers might have slight inaccuracies due to the limitations of binary representation.\nNumeric data is stored as 64-bit floating-point numbers (doubles) by default in R, which means they can represent a wide range of values with decimal places. However, this storage method may result in very small rounding errors when performing certain operations.\n\nTo define a single number:, you can do the following:\n\nnum_var &lt;- 3.14\n\nDouble Data Type:\n\nThe “double” data type in R specifically refers to double-precision floating-point numbers. It is a subset of the “numeric” data type.\nDouble-precision means that these numbers are stored in a 64-bit format, providing high precision for decimal values.\nWhile the “numeric” data type can include both integers and doubles, the “double” data type is used when you want to explicitly specify that a variable should be stored as a 64-bit double-precision floating-point number.\nUsing “double” can be beneficial in cases where precision is critical, such as scientific computations or when working with very large or very small numbers.\n\n\ndouble_var &lt;- 3.14\n\nIn fact, we gave the same example for both data types. So how do we tell the difference then? To learn the class of objects in R, there are two functions: class() and typeof()\n\nclass(num_var)\n\n[1] \"numeric\"\n\nclass(double_var)\n\n[1] \"numeric\"\n\ntypeof(num_var)\n\n[1] \"double\"\n\ntypeof(double_var)\n\n[1] \"double\"\n\n\nThe two functions produced different results. While the result of class function is numeric, for the same number the result of type of is double. In R, both the class() and typeof() functions are used to inspect the data type or structure of objects, but they serve different purposes and provide different levels of information about the objects. Here’s a breakdown of the differences between these two functions:\nclass():\n\nThe class() function in R is used to determine the class or type of an object in terms of its high-level data structure. It tells you how R treats the object from a user’s perspective, which is often more meaningful for data analysis and manipulation.\nThe class() function returns a character vector containing one or more class names associated with the object. It can return multiple class names when dealing with more complex objects that inherit properties from multiple classes.\nFor example, if you have a data frame called my_df, you can use class(my_df) to determine that it has the class “data.frame.”\nThe class() function is especially useful for understanding the semantics and behaviors associated with R objects. It helps you identify whether an object is a vector, matrix, data frame, factor, etc.\n\ntypeof():\n\nThe typeof() function in R is used to determine the fundamental data type of an object at a lower level. It provides information about the internal representation of the data.\nThe typeof() function returns a character string representing the basic data type of the object. Common results include “double” for numeric data, “integer” for integers, “character” for character strings, and so on.\nUnlike the class() function, which reflects how the object behaves, typeof() reflects how the object is stored in memory.\nThe typeof() function is more low-level and is often used for programming and memory management purposes. It can be useful in situations where you need to distinguish between different internal representations of data, such as knowing whether an object is stored as a double-precision floating-point number or an integer.\n\n\n\n\n\n\n\nTip\n\n\n\nThe key difference between class() and typeof() in R is their level of abstraction. class() provides a high-level view of an object’s data structure and behavior, while typeof() provides a low-level view of its fundamental data type in terms of how it’s stored in memory. Depending on your needs, you may use one or both of these functions to gain insights into your R objects.\n\n\nIn summary, the main difference between the “numeric” and “double” data types in R is that “numeric” is a broader category encompassing both integers and doubles, while “double” explicitly specifies a double-precision floating-point number. For most general purposes, you can use the “numeric” data type without worrying about the specifics of storage precision. However, if you require precise control over decimal precision, you can use “double” to ensure that variables are stored as 64-bit double-precision numbers.\n\n\nIntegers\nIn mathematics, integers are whole numbers that do not have a fractional or decimal part. They include both positive and negative whole numbers, as well as zero. In R, integers are represented as a distinct data type called “integer.”\nHere are some examples of integers in R:\n\nPositive integers: 1, 42, 1000\nNegative integers: -5, -27, -100\nZero: 0\n\nYou can create integer variables in R using the as.integer() function or by simply assigning a whole number to a variable. Let’s look at examples of both methods:\n\n# Using as.integer()\nx &lt;- as.integer(5)\ntypeof(x)\n\n[1] \"integer\"\n\n# Direct assignment\ny &lt;- 10L  # The 'L' suffix denotes an integer\ntypeof(y)\n\n[1] \"integer\"\n\n\nIn the second example, we added an ‘L’ suffix to the number to explicitly specify that it should be treated as an integer. While this suffix is optional, it can help clarify your code.\nIntegers in R have several key characteristics:\n\nExact Representation: Integers are represented exactly in R without any loss of precision. Unlike double-precision floating-point numbers, which may have limited precision for very large or very small numbers, integers can represent whole numbers precisely.\nConversion: You can convert other data types to integers using the as.integer() function. For instance, you can convert a double to an integer, which effectively rounds the number down to the nearest whole number.\n\n\ndouble_number &lt;- 3.99\ninteger_result &lt;- as.integer(double_number)  # Rounds down to 3\ninteger_result\n\n[1] 3\n\n\n\n\nCharacter\nIn computing, character data types (often referred to as “strings”) are used to represent sequences of characters, which can include letters, numbers, symbols, and even spaces. In R, character data types are used for handling text-based information, such as names, descriptions, and textual data extracted from various sources.\nIn R, you can create character variables by enclosing text within either single quotes (') or double quotes (\"). It’s essential to use matching quotes at the beginning and end of the text to define character data correctly. Here are examples of creating character variables:\n\n# Using single quotes\nmy_name &lt;- 'Fatih'\n\n# Using double quotes\nfavorite_fruit &lt;- \"Banana\"\n\n\n\n\n\n\n\nTip\n\n\n\nR doesn’t distinguish between single quotes and double quotes when defining character data; you can choose either, based on your preference.\n\n\nTo convert something to a character you can use the as.character() function. Also it is possible to convert a character to a numeric.\n\na &lt;- 1.234\nclass(a)\n\n[1] \"numeric\"\n\nclass(as.character(a)) # convert to character\n\n[1] \"character\"\n\nb &lt;- \"1.234\"\nclass(b)\n\n[1] \"character\"\n\nclass(as.numeric(b)) # convert to numeric\n\n[1] \"numeric\"\n\n\nCharacter data types in R possess the following characteristics:\n\nTextual Representation: Characters represent text-based information, allowing you to work with words, sentences, paragraphs, or any sequence of characters.\nImmutable: Once created, character data cannot be modified directly. You can create modified versions of character data through string manipulation functions, but the original character data remains unchanged.\nString Manipulation: R offers a wealth of string manipulation functions that enable you to perform operations like concatenation, substring extraction, replacement, and formatting on character data.\n\n# Concatenating two strings\ngreeting &lt;- \"Hello, \"\nname &lt;- \"Fatih\"\nfull_greeting &lt;- paste(greeting, name)\nfull_greeting\n\n[1] \"Hello,  Fatih\"\n\n# Extracting a substring\ntext &lt;- \"R Programming\"\nsub_text &lt;- substr(text, start = 1, stop = 1)  # Extracts the first character\nsub_text\n\n[1] \"R\"\n\n\nText-Based Operations: Character data types are invaluable for working with textual data, including cleaning and preprocessing text, tokenization, and natural language processing (NLP) tasks.\n\nCharacter data types are indispensable for numerous tasks in R:\n\nData Cleaning: When working with datasets, character data is used for cleaning and standardizing text fields, ensuring uniformity in data.\nData Extraction: Character data is often used to extract specific information from text, such as parsing dates, email addresses, or URLs from unstructured text.\nText Analysis: In the field of natural language processing, character data plays a central role in text analysis, sentiment analysis, and text classification.\nString Manipulation: When dealing with data transformation and manipulation, character data is used to create new variables or modify existing ones based on specific patterns or criteria.\n\nCharacter data types in R are essential for handling text-based information and conducting various data analysis tasks. They provide the means to represent, manipulate, and analyze textual data, making them a crucial component of any data scientist’s toolkit. Understanding how to create, manipulate, and work with character data is fundamental to effectively process and analyze text-based information in R programming.\n\n\nLogical\nLogical data types in R, also known as Boolean data types, are used to represent binary or Boolean values: true or false. These data types are fundamental for evaluating conditions, making decisions, and controlling the flow of program execution.\nIn R, logical values are denoted by two reserved keywords: TRUE (representing true) and FALSE (representing false). Logical data types are primarily used in comparisons, conditional statements, and logical operations.\nYou can create logical variables in R in several ways:\n\nDirect Assignment:\n\nis_raining &lt;- TRUE\nis_raining\n\n[1] TRUE\n\nis_sunny &lt;- FALSE\nis_sunny\n\n[1] FALSE\n\nclass(is_raining)\n\n[1] \"logical\"\n\n\nComparison Operators:\nLogical values often arise from comparisons using operators like &lt;, &lt;=, &gt;, &gt;=, ==, and !=. The result of a comparison operation is a logical value.\n\ntemperature &lt;- 25\nis_hot &lt;- temperature &gt; 30  # Evaluates to FALSE\nis_hot\n\n[1] FALSE\n\n\nLogical Functions:\nR provides logical functions like logical(), isTRUE(), isFALSE(), any() and all() that can be used to create logical values.\n\nis_even &lt;- logical(1)  # Creates a logical vector with one TRUE value\nis_even\n\n[1] FALSE\n\nall_positive &lt;- all(c(TRUE, TRUE, TRUE))  # Checks if all values are TRUE\nall_positive\n\n[1] TRUE\n\nany_positive &lt;- any(c(TRUE,FALSE)) #checks whether any of the vector’s elements are TRUE\nany_positive\n\n[1] TRUE\n\nc &lt;- 4 &gt; 3\nisTRUE(c) # cheks if a variable is TRUE\n\n[1] TRUE\n\n!isTRUE(c) # cheks if a variable is FALSE\n\n[1] FALSE\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe ! operator indicates negation, so the above expression could be translated as is c not TRUE. !isTRUE(c) is equivalent to isFALSE(c).\n\n\nLogical data types in R have the following characteristics:\n\nBinary Representation: Logical values can only take two values: TRUE or FALSE. These values are often used to express the truth or falsity of a statement or condition.\nConditional Evaluation: Logical values are integral to conditional statements like if, else, and else if. They determine which branch of code to execute based on the truth or falsity of a condition.\n\nif (is_raining) {\n  cat(\"Don't forget your umbrella!\\n\")\n} else {\n  cat(\"Enjoy the sunshine!\\n\")\n}\n\nDon't forget your umbrella!\n\n\nLogical Operations: Logical data types can be combined using logical operators such as & (AND), | (OR), and ! (NOT) to create more complex conditions.\n\n3 &lt; 5 & 8 &gt; 7 # If TRUE in both cases, the result returns TRUE\n\n[1] TRUE\n\n3 &lt; 5 & 6 &gt; 7 # If one case is FALSE and the other case is TRUE, the result is FALSE.\n\n[1] FALSE\n\n6 &lt; 5 & 6 &gt; 7 # If FALSE in both cases, the result returns FALSE\n\n[1] FALSE\n\n(5==4) | (3!=4) # If either condition is TRUE,returns TRUE\n\n[1] TRUE\n\n\n\nLogical data types are widely used in various aspects of R programming and data analysis:\n\nConditional Execution: Logical values are crucial for writing code that executes specific blocks or statements conditionally based on the evaluation of logical expressions.\nFiltering Data: Logical vectors are used to filter rows or elements in data frames, matrices, or vectors based on specified conditions.\nValidation: Logical data types are employed for data validation and quality control, ensuring that data meets certain criteria or constraints.\nBoolean Indexing: Logical indexing allows you to access elements in data structures based on logical conditions.\n\nLogical data types in R, represented by the TRUE and FALSE values, are fundamental for making decisions, controlling program flow, and evaluating conditions. They enable you to express binary choices and create complex logical expressions using logical operators. Understanding how to create, manipulate, and utilize logical data types is essential for effective programming and data analysis in R, as they play a central role in decision-making processes and conditional execution.\n\n\nDate and Time\nIn R, date and time data are represented using several data types, including:\n\nDate: The Date class in R is used to represent calendar dates. It is suitable for storing information like birthdays, data collection timestamps, and events associated with specific days.\nPOSIXct: The POSIXct class represents date and time values as the number of seconds since the UNIX epoch (January 1, 1970). It provides high precision and is suitable for timestamp data when sub-second accuracy is required.\nPOSIXlt: The POSIXlt class is similar to POSIXct but stores date and time information as a list of components, including year, month, day, hour, minute, and second. It offers human-readable representations but is less memory-efficient than POSIXct.\n\nYou can create date and time objects in R using various functions and formats:\n\nDate Objects: The as.Date() function is used to convert character strings or numeric values into date objects.\n\n# Creating a Date object\nmy_date &lt;- as.Date(\"2023-09-26\")\nclass(my_date)\n\n[1] \"Date\"\n\n\nPOSIXct Objects: The as.POSIXct() function converts character strings or numeric values into POSIXct objects. Timestamps can be represented in various formats.\n\n# Creating a POSIXct object\ntimestamp &lt;- as.POSIXct(\"2023-09-26 14:01:00\", format = \"%Y-%m-%d %H:%M:%S\")\ntimestamp\n\n[1] \"2023-09-26 14:01:00 +03\"\n\nclass(timestamp)\n\n[1] \"POSIXct\" \"POSIXt\" \n\n\nSys.time(): The Sys.time() function returns the current system time as a POSIXct object, which is often used for timestamping data.\n\n# Get the current system time\ncurrent_time &lt;- Sys.time()\ncurrent_time\n\n[1] \"2025-06-27 17:19:14 +03\"\n\n\n\nDate and time data types in R exhibit the following characteristics:\n\nGranularity: R allows you to work with dates and times at various levels of granularity, from years and months down to fractions of a second. This flexibility enables precise temporal analysis.\nArithmetic Operations: You can perform arithmetic operations with date and time objects, such as calculating the difference between two timestamps or adding a duration to a date.\n\n# Calculate the difference between two timestamps\n\nduration &lt;- current_time - timestamp\nduration\n\nTime difference of 640.1377 days\n\n# Add 3 days to a date\nnew_date &lt;- my_date + 3\nnew_date\n\n[1] \"2023-09-29\"\n\n\nFormatting and Parsing: R provides functions for formatting date and time objects as character strings and parsing character strings into date and time objects.\n\n# Formatting a date as a character string\nformatted_date &lt;- format(my_date, format = \"%Y/%m/%d\")\nformatted_date\n\n[1] \"2023/09/26\"\n\n# Parsing a character string into a date object\nparsed_date &lt;- as.Date(\"2023-09-26\", format = \"%Y-%m-%d\")\nparsed_date\n\n[1] \"2023-09-26\"\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf you want to learn details about widely avaliable formats, you can visit the help page of strptime() function.\n\n\nDate and time data types are integral to various data analysis and programming tasks in R:\n\nTime Series Analysis: Time series data, consisting of sequential data points recorded at regular intervals, are commonly analyzed in R for forecasting, trend analysis, and anomaly detection.\nData Aggregation: Date and time data enable you to group and aggregate data by time intervals, such as daily, monthly, or yearly summaries.\nEvent Tracking: Tracking and analyzing events with specific timestamps is essential for understanding patterns and trends in data.\nData Visualization: Effective visualization of temporal data helps in conveying insights and trends to stakeholders.\nData Filtering and Subsetting: Date and time objects are used to filter and subset data based on time criteria, allowing for focused analysis.\n\nDate and time data types in R are indispensable tools for handling temporal information in data analysis and programming tasks. Whether you’re working with time series data, event tracking, or simply timestamping your data, R’s extensive support for date and time operations makes it a powerful choice for temporal analysis. Understanding how to create, manipulate, and leverage date and time data is essential for effective data analysis and modeling in R, as it allows you to uncover valuable insights from temporal patterns and trends.\n\n\nComplex\nComplex numbers are an extension of real numbers, introducing the concept of an imaginary unit denoted by i or j. A complex number is typically expressed in the form a + bi, where a represents the real part, b the imaginary part, and i the imaginary unit.\nIn R, you can create complex numbers using the complex() function or simply by combining a real and imaginary part with the + operator.\n\n# Creating complex numbers\nz1 &lt;- complex(real = 3, imaginary = 2)\nz1\n\n[1] 3+2i\n\nclass(z1)\n\n[1] \"complex\"\n\nz2 &lt;- 1 + 4i\nz2\n\n[1] 1+4i\n\nclass(z2)\n\n[1] \"complex\"\n\n\nComplex numbers in R are often used in mathematical modeling, engineering, physics, signal processing, and various scientific disciplines where calculations involve imaginary and complex values."
  },
  {
    "objectID": "posts/2023-09-25_data_types/index.html#conclusion",
    "href": "posts/2023-09-25_data_types/index.html#conclusion",
    "title": "Understanding Data Types in R",
    "section": "Conclusion",
    "text": "Conclusion\nIn R programming, understanding data types is essential for effective data manipulation and analysis. Whether you’re working with numeric data, text, logical values, or complex structures, R provides the necessary tools to handle a wide range of data types. By mastering these data types, you’ll be better equipped to tackle data-related tasks, from data cleaning and preprocessing to statistical analysis and visualization. Whether you’re a data scientist, analyst, or programmer, a strong foundation in R’s data types is a valuable asset for your data-driven projects."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A Statistician's R Notebook",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nData Leakage in R: Why Correct Evaluation Matters Even When Metrics Do Not Change\n\n\n\nData Leakage\n\nR Programming\n\nData Preprocessing\n\nModel Evaluation\n\nStatistical Learning\n\nMachine Learning Pitfalls\n\nReproducible Research\n\n\n\n\n\n\n\nM. Fatih Tüzen\n\n\nJan 22, 2026\n\n\n\n\n\n\n\n\n\n\n\nData Normalization in R: When, Why, and How to Scale Your Data Correctly\n\n\n\nData Preprocessing\n\nR Programming\n\nData Science\n\nMachine Learning\n\n\n\n\n\n\n\nM. Fatih Tüzen\n\n\nJan 2, 2026\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Data Import and Export in R: Working with CSV and Excel Files\n\n\n\nR Programming\n\nData Analysis\n\nData Science\n\nCSV\n\nExcel\n\nData Import\n\nData Export\n\n\n\n\n\n\n\nM. Fatih Tüzen\n\n\nDec 26, 2025\n\n\n\n\n\n\n\n\n\n\n\nOutliers in Data Analysis: Detecting Extreme Values Before Modeling in R with İstanbul Airbnb Data\n\n\n\nR\n\nStatistics\n\nData Analysis\n\nData Science\n\nData Preprocessing\n\nOutliers\n\nInter Quartile Range\n\nZ-Score\n\nData Cleaning\n\n\n\n\n\n\n\nM. Fatih Tüzen\n\n\nDec 19, 2025\n\n\n\n\n\n\n\n\n\n\n\nHandling Missing Data in R: A Comprehensive Guide\n\n\n\nR\n\nStatistics\n\nData Analysis\n\nData Science\n\nData Preprocessing\n\nMissing Data\n\nData Cleaning\n\nImputation\n\n\n\n\n\n\n\nM. Fatih Tüzen\n\n\nAug 18, 2025\n\n\n\n\n\n\n\n\n\n\n\nStandard Deviation vs. Standard Error: Meaning, Misuse, and the Math Behind the Confusion\n\n\n\nR\n\nStatistics\n\nData Analysis\n\nStandard Deviation\n\nStandard Error\n\n\n\n\n\n\n\nM. Fatih Tüzen\n\n\nJul 11, 2025\n\n\n\n\n\n\n\n\n\n\n\nCorrelation vs Causation: Understanding the Difference\n\n\n\nStatistics\n\nCorrelation\n\nCausation\n\nEconometrics\n\nCausal Inference\n\nPublic Health\n\n\n\n\n\n\n\nM. Fatih Tüzen\n\n\nJun 4, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nExplained vs. Predictive Power: R², Adjusted R², and Beyond\n\n\n\nR\n\nStatistics\n\nMachine Learning\n\nr-squared\n\nadjusted-r-squared\n\npredictive-modeling\n\ntidymodels\n\nmodel-evaluation\n\n\n\n\n\n\n\nM. Fatih Tüzen\n\n\nApr 30, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnderrated Gems in R: Must-Know Functions You’re Probably Missing Out On\n\n\n\nreduce\n\nvapply\n\ndo.call\n\nclean_names\n\n\n\n\n\n\n\nM. Fatih Tüzen\n\n\nMar 11, 2025\n\n\n\n\n\n\n\n\n\n\n\nUnlocking CBRT Data in R: A Guide to the CBRT R Package\n\n\n\nR Programming\n\nCBRT\n\nEVDS\n\nImport\n\nAPI\n\n\n\n\n\n\n\nM. Fatih Tüzen\n\n\nDec 31, 2024\n\n\n\n\n\n\n\n\n\n\n\nExtracting Data from OECD Databases in R: Using the oecd and rsdmx Packages\n\n\n\nR Programming\n\nOECD\n\nrsdmx\n\nImport\n\nAPI\n\n\n\n\n\n\n\nM. Fatih Tüzen\n\n\nDec 16, 2024\n\n\n\n\n\n\n\n\n\n\n\nCreating Professional Excel Reports with R: A Comprehensive Guide to openxlsx Package\n\n\n\nR Programming\n\nReport Automation\n\nopenxlsx\n\nExcel\n\n\n\n\n\n\n\nM. Fatih Tüzen\n\n\nNov 4, 2024\n\n\n\n\n\n\n\n\n\n\n\nMastering Date and Time Data in R with lubridate\n\n\n\nR Programming\n\nlubridate\n\ntime series\n\ntime manipulation\n\ndate handling\n\n\n\n\n\n\n\nM. Fatih Tüzen\n\n\nSep 30, 2024\n\n\n\n\n\n\n\n\n\n\n\nMastering Data Transformation in R with pivot_longer and pivot_wider\n\n\n\nR Programming\n\ntidyr\n\npivot_wider\n\npivot_longer\n\ndata transformation\n\n\n\n\n\n\n\nM. Fatih Tüzen\n\n\nSep 19, 2024\n\n\n\n\n\n\n\n\n\n\n\nText Data Analysis in R: Understanding grep, grepl, sub and gsub\n\n\n\nR Programming\n\ngrep\n\ngrepl\n\nsub\n\ngsub\n\nregex\n\ntext analysis\n\n\n\n\n\n\n\nM. Fatih Tüzen\n\n\nJul 9, 2024\n\n\n\n\n\n\n\n\n\n\n\nExploring apply, sapply, lapply, and map Functions in R\n\n\n\nR Programming\n\napply\n\nsapply\n\nlapply\n\nmap\n\n\n\n\n\n\n\nM. Fatih Tüzen\n\n\nApr 15, 2024\n\n\n\n\n\n\n\n\n\n\n\nR Function Writing 101:A Journey Through Syntax, Best Practices, and More\n\n\n\nR Programming\n\nFunctions\n\n\n\n\n\n\n\nM. Fatih Tüzen\n\n\nJan 23, 2024\n\n\n\n\n\n\n\n\n\n\n\nCracking the Code of Categorical Data: A Guide to Factors in R\n\n\n\nR Programming\n\ndata types\n\nfactor\n\ncategorical data\n\n\n\n\n\n\n\nM. Fatih Tüzen\n\n\nJan 11, 2024\n\n\n\n\n\n\n\n\n\n\n\nUnraveling DataFrames in R: A Comprehensive Guide\n\n\n\nR Programming\n\ndata types\n\ndataframe\n\ntibble\n\n\n\n\n\n\n\nM. Fatih Tüzen\n\n\nDec 29, 2023\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Lists in R Programming\n\n\n\nR Programming\n\ndata types\n\nlists\n\n\n\n\n\n\n\nM. Fatih Tüzen\n\n\nDec 19, 2023\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Matrices in R Programming\n\n\n\nR Programming\n\ndata types\n\nmatrices\n\n\n\n\n\n\n\nM. Fatih Tüzen\n\n\nNov 20, 2023\n\n\n\n\n\n\n\n\n\n\n\nExploring Vectors in R Programming: The Fundamental Building Blocks\n\n\n\nR Programming\n\ndata types\n\nvectors\n\n\n\n\n\n\n\nM. Fatih Tüzen\n\n\nOct 3, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Data Types in R\n\n\n\nR Programming\n\ndata types\n\n\n\n\n\n\n\nM. Fatih Tüzen\n\n\nSep 26, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Gentle Introduction to R Programming\n\n\n\nR Programming\n\nR Studio\n\n\n\n\n\n\n\nM. Fatih Tüzen\n\n\nAug 15, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Muhammed Fatih TÜZEN",
    "section": "",
    "text": "Welcome to my blog about R and Statistics. I believe that it is possible to learn in every moment of life and that success comes from constant work. That’s why I always think it’s valuable to learn something new and share what I’ve learned. I am creating such a blog page to share my knowledge and experience about R and Statistics that I have gained over many years. I think the R programming language is like a full-fledged toolbox for anyone who deals with data. I will try to present the contents of this toolbox as much as I know and as much as I can.\n\nBio\nM. Fatih Tüzen is an expert working in the Methodology Department of the Turkish Statistical Institute (TurkStat). He holds a PhD in Statistics and has nearly 20 years of experience in official statistics. Fatih works on seasonal adjustment of short-term statistics, forecasting, backcasting, multivariate time series analysis, business cycles and data analysis. Skilled in R programming language, Fatih automates business processes, especially using tools such as Shiny. Beyond his professional role, he passionately teaches R programming to transfer his knowledge to his colleagues at TurkStat and prepares documentation with Quarto.\n\n\nEducation\nGazi University, Ankara, Türkiye, Doctor of Philosophy (Ph.D.) in Statistics, 2013 - 2018\nKafkas University, Kars, Türkiye, Master of Science (M.S.) in Business, 2010 - 2012\n19 May University, Samsun, Türkiye, Bachelor of Science (B.S.) in Statistics, 2000 - 2004\n\n\nExperience\nTurkish Statistical Institution, Ankara, Expert, Oct 2015- Present\nSocial Security Institution, Ankara, Statistician, Agu 2012 - Oct 2015\nTurkish Statistical Institution, Kars, Kars Regional Office, Statistician, Dec 2006 - Agu 2012\nTurkish Statistical Institution, Samsun, Samsun Regional Office, Field Interviewer, Agu 2005 - Dec 2006"
  },
  {
    "objectID": "posts/2023-08-15_r-intro/index.html",
    "href": "posts/2023-08-15_r-intro/index.html",
    "title": "A Gentle Introduction to R Programming",
    "section": "",
    "text": "Hello everyone! For my first post on my blog, I would like to make an introduction about R. Before we start coding with R, it’s not a bad idea to know a little about this program and learn what we can do. I will try to answer questions such as why do we need R, how can I install R on my computer, what are the useful resources about R. So, let’s get started."
  },
  {
    "objectID": "posts/2023-08-15_r-intro/index.html#what-is-r",
    "href": "posts/2023-08-15_r-intro/index.html#what-is-r",
    "title": "A Gentle Introduction to R Programming",
    "section": "What is R?",
    "text": "What is R?\n\n\n\n\n\nR is a programming language and open-source software environment specifically designed for statistical computing and data analysis. It was created by Ross Ihaka and Robert Gentleman at the University of Auckland, New Zealand, in the early 1990s. R is widely used by statisticians, data analysts, researchers, and data scientists to manipulate, visualize, and analyze data.\nKey features and characteristics of R programming include:\n\nStatistical Analysis: R provides a wide range of statistical functions and libraries that enable users to perform various statistical analyses, including regression, hypothesis testing, clustering, and more.\nData Visualization: R offers powerful data visualization capabilities through packages like ggplot2, lattice, and base graphics. These packages allow users to create a wide variety of plots and charts to visualize their data.\nData Manipulation: R provides functions and libraries for cleaning, transforming, and manipulating data. The dplyr and tidyr packages are popular choices for data manipulation tasks.\nExtensibility: Users can create and share their own functions, packages, and extensions, which contributes to the vibrant and active R community. This extensibility allows R to be adapted to various domains and applications.\nData Import and Export: R supports reading and writing data in various formats, including CSV, Excel, databases, and more. This flexibility makes it easy to work with data from different sources.\nInteractive Environment: R provides an interactive environment where users can execute commands, scripts, and analyses step by step. This is particularly useful for exploring data and experimenting with different approaches.\nCommunity and Packages: The R community has developed a vast ecosystem of packages that extend R’s functionality. CRAN (Comprehensive R Archive Network) is the central repository for R packages, where users can find and install packages for various tasks.\nScripting and Programming: R is a full-fledged programming language with support for control structures, loops, functions, and other programming constructs. This makes it suitable for both simple data analysis tasks and complex data science projects.\nOpen Source: R is released under an open-source license, which means that anyone can use, modify, and distribute the software. This openness has contributed to the growth and popularity of R in the data science community.\n\nR is commonly used in academia, research, and industries such as finance, healthcare, marketing, and more. Its flexibility, extensive packages, and active community support make it a valuable tool for a wide range of data-related tasks."
  },
  {
    "objectID": "posts/2023-08-15_r-intro/index.html#why-should-i-use-r",
    "href": "posts/2023-08-15_r-intro/index.html#why-should-i-use-r",
    "title": "A Gentle Introduction to R Programming",
    "section": "Why Should I Use R?",
    "text": "Why Should I Use R?\nThere are several compelling reasons to consider using R for your data analysis, statistical computing, and programming needs. Here are some key benefits of using R:\n\nStatistical Analysis: R was specifically designed for statistical analysis and provides a wide range of statistical functions, algorithms, and libraries. It’s an excellent choice for conducting complex statistical analyses, hypothesis testing, regression modeling, and more.\nData Visualization: R offers powerful data visualization capabilities through packages like ggplot2, which allow you to create customized and publication-quality visualizations. Visualizing data is crucial for understanding patterns, trends, and relationships.\nRich Ecosystem of Packages: R has a vibrant and active community that has developed thousands of packages to extend its functionality. These packages cover various domains, from machine learning and data manipulation to text analysis and bioinformatics.\nReproducibility: R promotes reproducible research by allowing you to write scripts that document your data analysis process step by step. This makes it easier to share your work with others and reproduce your results.\nCommunity and Resources: R has a large and supportive community of users and experts who share their knowledge through forums, blogs, and tutorials. This community support can be invaluable when you encounter challenges.\nOpen Source: R is open-source software, meaning it’s free to use and open for anyone to modify and contribute to. This accessibility has led to its widespread adoption across academia, research, and industries.\nFlexibility: R is a versatile programming language that supports both interactive analysis and script-based programming. It’s well-suited for a wide range of tasks, from exploratory data analysis to building complex data science models.\nIntegration with Other Tools: R can be integrated with other tools and platforms, such as databases, big data frameworks (like Hadoop and Spark), and APIs, allowing you to work with data from various sources.\nData Manipulation: Packages like dplyr and tidyr provide powerful tools for efficiently cleaning, transforming, and reshaping data, making data preparation easier and more efficient.\nAcademic and Research Use: R is widely used in academia and research, making it a valuable skill for students, researchers, and professionals in fields such as statistics, social sciences, and natural sciences.\nData Science and Machine Learning: R has a strong presence in the data science and machine learning communities. Packages like caret, randomForest, and xgboost provide tools for building predictive models.\nComprehensive Documentation: R provides comprehensive documentation and help resources, including function documentation, manuals, and online guides.\n\nUltimately, the decision to use R depends on your specific needs, your familiarity with the language, and the types of analyses and projects you’re involved in. If you’re working with data analysis, statistics, or data science, R can be a powerful tool that empowers you to explore, analyze, and visualize data effectively."
  },
  {
    "objectID": "posts/2023-08-15_r-intro/index.html#useful-resources-for-r-programming",
    "href": "posts/2023-08-15_r-intro/index.html#useful-resources-for-r-programming",
    "title": "A Gentle Introduction to R Programming",
    "section": "Useful Resources for R Programming",
    "text": "Useful Resources for R Programming\nThere are numerous useful resources available for learning and mastering R programming. Whether you’re a beginner or an experienced user, these resources can help you enhance your R skills. My intention is to share resources that I think are useful and some of which I use myself, rather than advertising some people or organizations. Here’s a list of some valuable R programming resources:\n\nOnline Courses and Tutorials:\n\nCoursera: Offers a variety of R programming courses, including “R Programming” by Johns Hopkins University.\nedX: Provides courses like “Introduction to R for Data Science” by Microsoft.\nDataCamp: Offers interactive R tutorials and courses for all skill levels.\nRStudio Education: Provides free and interactive tutorials on using R and RStudio.\n\nBooks:\n\n“R for Data Science” by Hadley Wickham, Mine Çetinkaya-Rundel and Garrett Grolemund: A comprehensive guide to using R for data analysis and visualization.\n“Advanced R” by Hadley Wickham: Focuses on more advanced programming concepts and techniques in R.\n“R Graphics Cookbook” by Winston Chang: A guide to creating various types of visualizations using R.\n“Big Book of R”is an open source web page created by Oscar Baruffa. The page functions as an easy-to-navigate, one-stop shop by categorizing books on many topics prepared within the R programming language.\n\nOnline Communities and Forums:\n\nStack Overflow: A popular Q&A platform where you can ask and answer R programming-related questions.\nRStudio Community: RStudio’s official forum for discussing R and RStudio-related topics.\nReddit: The r/rprogramming and r/rstats subreddits are great places for discussions and sharing R resources.\n\nBlogs and Websites:\n\nR-bloggers: Aggregates blog posts from various R bloggers, covering a wide range of topics.\nRStudio Blog: The official blog of RStudio, featuring articles and tutorials on R and RStudio.\nDataCamp Community Blog: DataCamp is an online learning platform, and its community blog features numerous tutorials and articles on R programming, data science, and related topics.\nTidyverse Blog: If you’re a fan of the tidyverse packages (e.g., dplyr, ggplot2), you’ll find useful tips and updates on their blog.\nGithub : GitHub is a web-based platform for version control and collaboration that is widely used by developers and teams for managing and sharing source code and other project-related files. It provides a range of features and tools for software development, including version control, code hosting, collaboration, issue tracking, pull requests, wiki and documentation, integration, community and social features. GitHub is widely used by both individual developers and large organizations for open-source and closed-source projects alike. It has become a central hub for software development, fostering collaboration and code sharing within the global developer community.\n\n\n\n\n\n\n\nWarning\n\n\n\nPlease keep in mind that the availability and popularity of blogs can change, so it’s a good idea to explore these websites and also look for any new blogs or resources that may have emerged since my last update. Additionally, consider following R-related discussions and communities on social media platforms and forums like Stack Overflow for the latest information and discussions related to R programming.\n\n\nPackages and Documentation:\n\nCRAN (Comprehensive R Archive Network): The central repository for R packages. You can find packages for various tasks and their documentation here.\nRDocumentation: Offers searchable documentation for R packages.\n\n\n\n\n\n\n\nTip\n\n\n\nRemember that learning R programming is an ongoing process, so feel free to explore multiple resources and tailor your learning approach to your needs and interests. Apart from these, you can find many channels, communities or people to follow on YouTube and social media. Of course, artificial intelligence-supported chat engines such as chatGPT and Google Bard, which have become popular recently, are also very useful resources."
  },
  {
    "objectID": "posts/2023-08-15_r-intro/index.html#installing-r-on-your-machine",
    "href": "posts/2023-08-15_r-intro/index.html#installing-r-on-your-machine",
    "title": "A Gentle Introduction to R Programming",
    "section": "Installing R on your machine",
    "text": "Installing R on your machine\nIn order to install R and RStudio on your computer, follow these steps:\nInstalling R:\n\nDownload R: Visit the official R website and select a CRAN mirror near you.\nChoose Your Operating System: Click on the appropriate link for your operating system (Windows, macOS, or Linux).\n\nFor Windows: Download the “base” distribution.\nFor macOS: Download the “pkg” file.\nFor Linux: Follow the instructions for your specific distribution (e.g., Ubuntu, Debian, CentOS) provided on the CRAN website.\n\nInstall R:\n\nFor Windows: Run the downloaded installer and follow the installation instructions.\nFor macOS: Open the downloaded .pkg file and follow the installation instructions.\nFor Linux: Follow the installation instructions for your specific Linux distribution.\n\n\nR has now been sucessfully installed on your Windows OS. Open the R GUI to start writing R codes.\nInstalling RStudio:\n\nDownload RStudio: Visit the official RStudio website RStudio website and select the appropriate version of RStudio Desktop for your operating system (Windows, macOS, or Linux).\nInstall RStudio:\n\nFor Windows: Run the downloaded installer and follow the installation instructions.\nFor macOS: Open the downloaded .dmg file and drag the RStudio application to your Applications folder.\nFor Linux: Follow the installation instructions for your specific Linux distribution.\n\n\nRStudio is now successfully installed on your computer.\n\n\n\n\n\nApart from R and Rstudio, you may also need to install Rtools. Rtools is a collection of software tools that are essential for building and compiling packages in the R programming language on Windows operating systems. Here are several reasons why you might need Rtools:\n\nPackage Development: If you plan to develop R packages, you will need Rtools to compile and build those packages. R packages often contain C, C++, or Fortran code, which needs to be compiled into binary form to work with R.\nInstalling Binary Packages: Some R packages are only available in binary form on CRAN (Comprehensive R Archive Network). If you want to install these packages, you may need Rtools to help with package installation and compilation.\nUsing devtools: If you use the devtools package in R to develop or install packages from sources (e.g., GitHub repositories), Rtools is often required for the compilation of code.\nExternal Dependencies: Certain R packages rely on external libraries and tools that are included in Rtools. Without Rtools, these packages may not be able to function correctly.\nCustom Code: If you write custom R code that relies on compiled code in C, C++, or Fortran, you will need Rtools to compile and link your custom code with R.\nCreating RMarkdown Documents: If you use RMarkdown to create documents that involve code chunks needing compilation, Rtools is required to compile these documents into their final format, such as PDF or HTML.\nData Analysis with Specific Packages: Some specialized packages in R, especially those dealing with high-performance computing or specific domains, may require Rtools as a prerequisite.\nBuilding from Source: If you want to install R itself from source code rather than using a pre-built binary version, Rtools is necessary to compile and build R from source.\n\nIn summary, Rtools is crucial for anyone working with R on Windows who intends to compile code, develop packages, or work with packages that rely on compiled code. It provides the necessary toolchain and dependencies for these tasks, ensuring that R functions correctly with code that needs to be compiled.\nInstalling RTools\n\nDownload R Tools: Visit RTools website and download the RTools installer.\nAfter downloading has completed run the installer. Select the default options everywhere."
  },
  {
    "objectID": "posts/2023-10-03_vectors/index.html",
    "href": "posts/2023-10-03_vectors/index.html",
    "title": "Exploring Vectors in R Programming: The Fundamental Building Blocks",
    "section": "",
    "text": "https://www.thoughtco.com/most-basic-building-block-of-matter-608358\n\n\nIn the realm of R programming, vectors serve as the fundamental building blocks that underpin virtually every data analysis and manipulation task. Much like atoms are the smallest units of matter, vectors are the fundamental units of data in R. In this article, we will delve into the world of vectors in R programming, exploring their significance, applications, and some of the most commonly used functions that make them indispensable."
  },
  {
    "objectID": "posts/2023-10-03_vectors/index.html#introduction",
    "href": "posts/2023-10-03_vectors/index.html#introduction",
    "title": "Exploring Vectors in R Programming: The Fundamental Building Blocks",
    "section": "",
    "text": "https://www.thoughtco.com/most-basic-building-block-of-matter-608358\n\n\nIn the realm of R programming, vectors serve as the fundamental building blocks that underpin virtually every data analysis and manipulation task. Much like atoms are the smallest units of matter, vectors are the fundamental units of data in R. In this article, we will delve into the world of vectors in R programming, exploring their significance, applications, and some of the most commonly used functions that make them indispensable."
  },
  {
    "objectID": "posts/2023-10-03_vectors/index.html#what-is-a-vector",
    "href": "posts/2023-10-03_vectors/index.html#what-is-a-vector",
    "title": "Exploring Vectors in R Programming: The Fundamental Building Blocks",
    "section": "What is a Vector?",
    "text": "What is a Vector?\nIn R, a vector is a fundamental data structure that can hold multiple elements of the same data type. These elements can be numbers, characters, logical values, or other types of data. Vectors are one-dimensional, meaning they consist of a single sequence of values. These vectors can be considered as the atomic units of data storage in R, forming the basis for more complex data structures like matrices, data frames, and lists. In essence, vectors are the elemental containers for data elements."
  },
  {
    "objectID": "posts/2023-10-03_vectors/index.html#importance-of-vectors",
    "href": "posts/2023-10-03_vectors/index.html#importance-of-vectors",
    "title": "Exploring Vectors in R Programming: The Fundamental Building Blocks",
    "section": "Importance of Vectors",
    "text": "Importance of Vectors\nVectors play a pivotal role in R programming for several reasons:\n\nEfficient Data Storage: Vectors efficiently store homogeneous data, saving memory and computational resources.\nVectorized Operations: One of the most powerful aspects of R is its ability to perform operations on entire vectors efficiently, a concept known as vectorization. R is designed for vectorized operations, meaning you can perform operations on entire vectors without the need for explicit loops. This makes code concise and faster.\nCompatibility: Most R functions are designed to work with vectors, making them compatible with many data analysis and statistical techniques.\nSimplicity: Using vectors simplifies code and promotes a more intuitive and readable coding style.\nInteroperability: Vectors can be easily converted into other data structures, such as matrices or data frames, enhancing data manipulation capabilities."
  },
  {
    "objectID": "posts/2023-10-03_vectors/index.html#subsetting-and-indexing-vectors",
    "href": "posts/2023-10-03_vectors/index.html#subsetting-and-indexing-vectors",
    "title": "Exploring Vectors in R Programming: The Fundamental Building Blocks",
    "section": "Subsetting and Indexing Vectors",
    "text": "Subsetting and Indexing Vectors\nSubsetting and indexing are essential operations in R that allow you to access specific elements or subsets of elements from a vector. Subsetting refers to the process of selecting a portion of a vector based on specific conditions or positions. Indexing, on the other hand, refers to specifying the position or positions of the elements you want to access within the vector.\n\n\n\n\n\n\nTip\n\n\n\nSquare brackets ([ ]) is used to access and subset elements in vectors and other data structures like lists and matrices. It allows you to extract specific elements or subsets of elements from a vector.\n\n\nLet’s explore these concepts with interesting examples.\n\nSubsetting Vectors\nSubsetting by Index\nYou can subset a vector by specifying the index positions of the elements you want to access.\n\n# Create a numeric vector\nmy_vector &lt;- c(10, 20, 30, 40, 50)\n\n# Subset the second and fourth elements\nsubset &lt;- my_vector[c(2, 4)]\n\n# Print the result\nprint(subset)\n\n[1] 20 40\n\n\nSubsetting by Condition\nYou can subset a vector based on a condition using logical vectors.\n\n# Create a numeric vector\nmy_vector &lt;- c(10, 20, 30, 40, 50)\n\n# Subset values greater than 30\nsubset &lt;- my_vector[my_vector &gt; 30]\n\n# Print the result\nprint(subset)\n\n[1] 40 50\n\n\n\n\nIndexing Vectors\nSingle Index\nAccess a single element by specifying its index.\n\n# Create a character vector\nfruits &lt;- c(\"apple\", \"banana\", \"cherry\")\n\n# Access the second element\nfruit &lt;- fruits[2]\n\n# Print the result\nprint(fruit)\n\n[1] \"banana\"\n\n\nMultiple Indices\nAccess multiple elements by specifying multiple indices.\n\n# Create a numeric vector\nnumbers &lt;- c(1, 2, 3, 4, 5)\n\n# Access the first and fourth elements\nsubset &lt;- numbers[c(1, 4)]\n\n# Print the result\nprint(subset)\n\n[1] 1 4\n\n\nNegative Indexing\nExclude elements by specifying negative indices.\n\n# Create a numeric vector\nnumbers &lt;- c(1, 2, 3, 4, 5)\n\n# Exclude the second element\nsubset &lt;- numbers[-2]\n\n# Print the result\nprint(subset)\n\n[1] 1 3 4 5\n\n\nThese examples demonstrate how to subset and index vectors in R, allowing you to access specific elements or subsets of elements based on conditions, positions, or logical criteria. These operations are fundamental in data analysis and manipulation tasks in R."
  },
  {
    "objectID": "posts/2023-10-03_vectors/index.html#most-used-functions-with-vectors",
    "href": "posts/2023-10-03_vectors/index.html#most-used-functions-with-vectors",
    "title": "Exploring Vectors in R Programming: The Fundamental Building Blocks",
    "section": "Most Used Functions with Vectors",
    "text": "Most Used Functions with Vectors\nLet’s explore some commonly used functions when working with vectors in R.\n\nc()\nc() function (short for “combine” or “concatenate”) is used for creating a new vector or combining multiple values or vectors into a single vector. It allows you to create a vector by listing its elements within the function.\n1. Combining Numeric Values:\n\n# Creating a numeric vector\nnumeric_vector &lt;- c(1, 2, 3, 4, 5)\nprint(numeric_vector)\n\n[1] 1 2 3 4 5\n\n\n2. Combining Character Strings:\n\n# Creating a character vector\ncharacter_vector &lt;- c(\"apple\", \"banana\", \"cherry\")\nprint(character_vector)\n\n[1] \"apple\"  \"banana\" \"cherry\"\n\n\n3. Combining Different Data Types (Implicit Coercion):\n\n# Combining numeric and character values\n# Numeric values are coerced to character.\nmixed_vector &lt;- c(1, \"two\", 3, \"four\")\nclass(mixed_vector)\n\n[1] \"character\"\n\n\n4. Combining Vectors Recursively:\n\n# Creating nested vectors and combining them recursively\n# The nested vectors are flattened into a single vector.\nnested_vector &lt;- c(1, c(2, 3), c(4, 5, c(6, 7)))\nprint(nested_vector)\n\n[1] 1 2 3 4 5 6 7\n\n\n\n\nseq()\nIn R, the seq() function is used to generate sequences of numbers or other objects. It allows you to create a sequence of values with specified starting and ending points, increments, and other parameters. The seq() function is quite versatile and can be used to generate sequences of integers, real numbers, or even character strings.\nHere is the basic syntax of the seq() function:\n\nseq(from, to, by = (to - from)/(length.out - 1), length.out = NULL)\n\n\nfrom: The starting point of the sequence.\nto: The ending point of the sequence.\nby: The interval between values in the sequence. It is an optional parameter. If not specified, R calculates it based on the from, to, and length.out parameters.\nlength.out: The desired length of the sequence. It is an optional parameter. If provided, R calculates the by parameter based on the desired length.\n\nHere are some examples to illustrate how to use the seq() function:\n\nGenerating a Sequence of Integers\n\n\n# Create a sequence of integers from 1 to 10\nseq(1, 10)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\n\nGenerating a Sequence of Real Numbers with a Specified Increment\n\n\n# Create a sequence of real numbers from 0 to 1 with an increment of 0.2\nseq(0, 1, by = 0.2)\n\n[1] 0.0 0.2 0.4 0.6 0.8 1.0\n\n\n\nGenerating a Sequence with a Specified Length\n\n\n# Create a sequence of 5 values from 2 to 10\nseq(2, 10, length.out = 5)\n\n[1]  2  4  6  8 10\n\n\n\nGenerating a Sequence in Reverse Order\n\n\n# Create a sequence of integers from 10 to 1 in reverse order\nseq(10, 1)\n\n [1] 10  9  8  7  6  5  4  3  2  1\n\n\nThe seq() function is very useful for creating sequences of values that you can use for various purposes, such as creating sequences for plotting, generating data for simulations, or defining custom sequences for indexing elements in vectors or data frames.\n\n\nrep()\nIn R, the rep() function is used to replicate or repeat values to create vectors or arrays of repeated elements. It allows you to duplicate a value or a set of values a specified number of times to form a larger vector or matrix. The rep() function is quite flexible and can be used to repeat both individual elements and entire vectors or lists.\nHere’s the basic syntax of the rep() function:\n\nrep(x, times, each, length.out)\n\n\nx: The value(s) or vector(s) that you want to repeat.\ntimes: An integer specifying how many times x should be repeated. If you provide a vector for x, each element of the vector will be repeated times times.\neach: An integer specifying how many times each element of x (if it’s a vector) should be repeated before moving on to the next element. This is an optional parameter.\nlength.out: An integer specifying the desired length of the result. This is an optional parameter, and it can be used instead of times and each to determine the number of repetitions.\n\nHere are some examples to illustrate how to use the rep() function:\n\nReplicating a Single Value\n\n\n# Repeat the value 3, four times\nrep(3, times = 4)\n\n[1] 3 3 3 3\n\n\n\nReplicating Elements of a Vector\n\n\n# Create a vector\nmy_vector &lt;- c(\"A\", \"B\", \"C\")\n\n# Repeat each element of the vector 2 times\nrep(my_vector, each = 2)\n\n[1] \"A\" \"A\" \"B\" \"B\" \"C\" \"C\"\n\n\n\nReplicating Elements of a Vector with Different Frequencies\n\n\n# Repeat each element of the vector with different frequencies\nrep(c(\"A\", \"B\", \"C\"), times = c(3, 2, 4))\n\n[1] \"A\" \"A\" \"A\" \"B\" \"B\" \"C\" \"C\" \"C\" \"C\"\n\n\n\nControlling the Length of the Result\n\n\n# Repeat the values from 1 to 3 to create a vector of length 10\nrep(1:3, length.out = 10)\n\n [1] 1 2 3 1 2 3 1 2 3 1\n\n\nThe rep() function is useful for tasks like creating data for simulations, repeating elements for plotting, and constructing vectors and matrices with specific patterns or repetitions.\n\n\nlength()\nIn R, the length() function is used to determine the number of elements in a vector. It returns an integer value representing the length of the vector. The length() function is straightforward to use and provides a quick way to check the number of elements in a vector.\nHere’s the basic syntax of the length() function for vectors:\n\nlength(x)\n\n\nx: The vector for which you want to find the length.\n\nHere’s an example of how to use the length() function with vectors:\n\n# Create a numeric vector\nnumeric_vector &lt;- c(1, 2, 3, 4, 5)\n\n# Use the length() function to find the length of the vector\nlength(numeric_vector)\n\n[1] 5\n\n\nThe length() function is particularly useful when you need to perform operations or make decisions based on the size or length of a vector. It is commonly used in control structures like loops to ensure that you iterate through the entire vector or to dynamically adjust the length of vectors in your code.\n\n\nunique()\nThe unique() function is used to extract the unique elements from a vector, returning a new vector containing only the distinct values found in the original vector. It is a convenient way to identify and remove duplicate values from a vector.\nHere’s the basic syntax of the unique() function:\n\nunique(x)\n\n\nx: The vector from which you want to extract unique elements.\n\nHere’s an example of how to use the unique() function with a vector:\n\n# Create a vector with duplicate values\nmy_vector &lt;- c(1, 2, 2, 3, 4, 4, 5)\n\n# Use the unique() function to extract unique elements\nunique(my_vector)\n\n[1] 1 2 3 4 5\n\n\nIn this example, the unique() function is applied to the my_vector, and it returns a new vector containing only the unique values, removing duplicates. The order of the unique values in the result is the same as their order of appearance in the original vector.\nThe unique() function is particularly useful when dealing with data preprocessing or data cleaning tasks, where you need to identify and handle duplicate values in a dataset. It’s also helpful when you want to generate a list of unique categories or distinct values from a categorical variable.\n\n\nduplicated()\nThe duplicated() function in R is a handy tool for identifying and working with duplicate elements in a vector. It returns a logical vector of the same length as the input vector, indicating whether each element in the vector is duplicated or not. You can also use the fromLast argument to control the direction of the search for duplicates.\nHere’s the detailed syntax of the duplicated() function:\n\nduplicated(x, fromLast = FALSE)\n\n\nx: The vector in which you want to identify duplicate elements.\nfromLast: An optional logical parameter (default is FALSE). If set to TRUE, it considers duplicates from the last occurrence of each element instead of the first.\n\nNow, let’s dive into some interesting examples to understand how the duplicated() function works:\n\nIdentifying Duplicate Values\n\n\n# Create a vector with duplicate values\nmy_vector &lt;- c(1, 2, 2, 3, 4, 4, 5)\n\n# Use the duplicated() function to identify duplicate elements\nduplicates &lt;- duplicated(my_vector)\n\n# Print the result\nprint(duplicates)\n\n[1] FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE\n\n# Get the values that are duplicated\nduplicated_values &lt;- my_vector[duplicates]\nprint(duplicated_values)\n\n[1] 2 4\n\n\nIn this example, duplicates is a logical vector indicating whether each element in my_vector is duplicated. TRUE indicates duplication, and FALSE indicates uniqueness. We then extract the duplicated values using indexing.\nIdentifying Duplicates from the Last Occurrence\n\n# Create a vector with duplicate values\nmy_vector &lt;- c(1, 2, 2, 3, 4, 4, 5)\n\n# Use the duplicated() function to identify duplicates from the last occurrence\nduplicates_last &lt;- duplicated(my_vector, fromLast = TRUE)\n\n# Print the result\nprint(duplicates_last)\n\n[1] FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE\n\n# Get the values that are duplicated from the last occurrence\nduplicated_values_last &lt;- my_vector[duplicates_last]\nprint(duplicated_values_last)\n\n[1] 2 4\n\n\nBy setting fromLast = TRUE, we identify duplicates based on their last occurrence in the vector.\n\nRemoving Duplicate Values from a Vector\n\n\n# Create a vector with duplicate values\nmy_vector &lt;- c(1, 2, 2, 3, 4, 4, 5)\n\n# Use the `!` operator to negate the duplicated values and get unique values\nunique_values &lt;- my_vector[!duplicated(my_vector)]\n\n# Print the unique values\nprint(unique_values)\n\n[1] 1 2 3 4 5\n\n\nIn this example, we use the ! operator to negate the result of duplicated() to get unique values in the vector.\n\nIdentifying Duplicates in a Character Vector\n\n\n# Create a character vector with duplicate strings\nmy_strings &lt;- c(\"apple\", \"banana\", \"apple\", \"cherry\", \"banana\")\n\n# Use the duplicated() function to identify duplicate strings\nduplicates_strings &lt;- duplicated(my_strings)\n\n# Print the result\nprint(duplicates_strings)\n\n[1] FALSE FALSE  TRUE FALSE  TRUE\n\n# Get the duplicated strings\nduplicated_strings &lt;- my_strings[duplicates_strings]\nprint(duplicated_strings)\n\n[1] \"apple\"  \"banana\"\n\n\nThe duplicated() function can also be used with character vectors to identify duplicate strings.\nThese examples illustrate how the duplicated() function can be used to identify and work with duplicate elements in a vector, which is useful for data cleaning, analysis, and other data manipulation tasks in R.\n\n\nsort()\nthe sort() function is used to sort the elements of a vector in either ascending or descending order. It is a fundamental function for arranging and organizing data. The sort() function can be applied to various types of vectors, including numeric, character, and factor vectors.\nHere’s the basic syntax of the sort() function:\n\nsort(x, decreasing = FALSE)\n\n\nx: The vector that you want to sort.\ndecreasing: An optional logical parameter (default is FALSE). If set to TRUE, the vector is sorted in descending order; if FALSE, it’s sorted in ascending order.\n\nNow, let’s explore the sort() function with some interesting examples:\n\nSorting a Numeric Vector in Ascending Order\n\n\n# Create a numeric vector\nnumeric_vector &lt;- c(5, 2, 8, 1, 3)\n\n# Sort the vector in ascending order\nsorted_vector &lt;- sort(numeric_vector)\n\n# Print the result\nprint(sorted_vector)\n\n[1] 1 2 3 5 8\n\n\nIn this example, sorted_vector contains the elements of numeric_vector sorted in ascending order.\n\nSorting a Character Vector in Alphabetical Order\n\n\n# Create a character vector\ncharacter_vector &lt;- c(\"apple\", \"banana\", \"cherry\", \"date\", \"grape\")\n\n# Sort the vector in alphabetical order\nsorted_vector &lt;- sort(character_vector)\n\n# Print the result\nprint(sorted_vector)\n\n[1] \"apple\"  \"banana\" \"cherry\" \"date\"   \"grape\" \n\n\nHere, sorted_vector contains the elements of character_vector sorted in alphabetical order.\n\nSorting in Descending Order\n\n\n# Create a numeric vector\nnumeric_vector &lt;- c(5, 2, 8, 1, 3)\n\n# Sort the vector in descending order\nsorted_vector &lt;- sort(numeric_vector, decreasing = TRUE)\n\n# Print the result\nprint(sorted_vector)\n\n[1] 8 5 3 2 1\n\n\nBy setting decreasing = TRUE, we sort numeric_vector in descending order.\n\nSorting a Factor Vector\n\nIn R, a “factor” is a data type that represents categorical or discrete data. Factors are used to store and manage categorical variables in a more efficient and meaningful way. Categorical variables are variables that take on a limited, fixed set of values or levels, such as “yes” or “no,” “low,” “medium,” or “high,” or “red,” “green,” or “blue.” In R, Factors are created using the factor() function.\n\n\n\n\n\n\nNote\n\n\n\nI am planning to write a post about the factors soon.\n\n\n\n# Create a factor vector\nfactor_vector &lt;- factor(c(\"high\", \"low\", \"medium\", \"low\", \"high\"))\n\n# Sort the factor vector in alphabetical order\nsorted_vector &lt;- sort(factor_vector)\n\n# Print the result\nprint(sorted_vector)\n\n[1] high   high   low    low    medium\nLevels: high low medium\n\n\nThe sort() function can also be used with factor vectors, where it sorts the levels in alphabetical order.\n\nSorting with Indexing\n\n\n# Create a numeric vector\nnumeric_vector &lt;- c(5, 2, 8, 1, 3)\n\n# Sort the vector in ascending order and store the index order\nsorted_indices &lt;- order(numeric_vector)\nsorted_vector &lt;- numeric_vector[sorted_indices]\n\n# Print the result\nprint(sorted_vector)\n\n[1] 1 2 3 5 8\n\n\nIn this example, we use the order() function to obtain the index order needed to sort numeric_vector in ascending order. We then use this index order for sorting the vector.\nThe sort() function is a versatile tool for sorting vectors in R, and it is a fundamental part of data analysis and manipulation. It can be applied to various data types, and you can control the sorting order with the decreasing parameter.\n\n\nwhich()\nThe which() function is used to identify the indices of elements in a vector that satisfy a specified condition. It returns a vector of indices where the condition is TRUE.\nHere’s the basic syntax of the which() function:\n\nwhich(x, arr.ind = FALSE)\n\n\nx: The vector in which you want to find indices based on a condition.\narr.ind: An optional logical parameter (default is FALSE). If set to TRUE, the function returns an array of indices with dimensions corresponding to x. This is typically used when x is a multi-dimensional array.\n\nNow, let’s explore the which() function with some interesting examples:\n\nFinding Indices of Elements Greater Than a Threshold\n\n\n# Create a numeric vector\nmy_vector &lt;- c(10, 5, 15, 3, 8)\n\n# Find indices where values are greater than 8\nindices_greater_than_8 &lt;- which(my_vector &gt; 8)\n\n# Print the result\nprint(indices_greater_than_8)\n\n[1] 1 3\n\n\nIn this example, indices_greater_than_8 contains the indices where elements in my_vector are greater than 8.\n\nFinding Indices of Missing Values (NA)\n\n\n# Create a vector with missing values (NA)\nmy_vector &lt;- c(2, NA, 5, NA, 8)\n\n# Find indices of missing values\nindices_of_na &lt;- which(is.na(my_vector))\n\n# Print the result\nprint(indices_of_na)\n\n[1] 2 4\n\n\nHere, indices_of_na contains the indices where my_vector has missing values (NA).\n\n\n\n\n\n\nTip\n\n\n\nThe is.na() function in R is used to identify missing values (NAs) in a vector or a data frame. It returns a logical vector or data frame of the same shape as the input, where each element is TRUE if the corresponding element in the input is NA, and FALSE otherwise.\n\n\n\nFinding Indices of Specific Values\n\n\n# Create a character vector\nmy_vector &lt;- c(\"apple\", \"banana\", \"cherry\", \"banana\", \"apple\")\n\n# Find indices where values are \"banana\"\nindices_banana &lt;- which(my_vector == \"banana\")\n\n# Print the result\nprint(indices_banana)\n\n[1] 2 4\n\n\nHere, indices_banana contains the indices where elements in my_vector are equal to “banana.”\nThe which() function is versatile and can be used for various purposes, such as identifying specific elements, locating missing values, and finding indices based on custom conditions. It’s a valuable tool for data analysis and manipulation in R.\n\n\npaste()\nThe paste() function is used to concatenate (combine) character vectors element-wise into a single character vector. It allows you to join strings or character elements together with the option to specify a separator or collapse them without any separator. The basic syntax of the paste() function is as follows:\n\npaste(..., sep = \" \", collapse = NULL)\n\n\n...: One or more character vectors or objects to be combined.\nsep: A character string that specifies the separator to be used between the concatenated elements. The default is a space.\ncollapse: An optional character string that specifies a separator to be used when collapsing the concatenated elements into a single string. If collapse is not specified, the result will be a character vector.\n\nNow, let’s explore the paste() function with some interesting examples:\n\nConcatenating Character Vectors with Default Separator\n\n\n# Create two character vectors\nfirst_names &lt;- c(\"John\", \"Alice\", \"Bob\")\nlast_names &lt;- c(\"Doe\", \"Smith\", \"Johnson\")\n\n# Use paste() to concatenate them with the default separator (space)\nfull_names &lt;- paste(first_names, last_names)\n\n# Print the result\nprint(full_names)\n\n[1] \"John Doe\"    \"Alice Smith\" \"Bob Johnson\"\n\n\nIn this example, the paste() function concatenates first_names and last_names with the default separator, which is a space.\n\nSpecifying a Custom Separator\n\n\n# Create a character vector\nfruits &lt;- c(\"apple\", \"banana\", \"cherry\")\n\n# Use paste() with a custom separator (comma and space)\nfruits_list &lt;- paste(fruits, collapse = \", \")\n\n# Print the result\nprint(fruits_list)\n\n[1] \"apple, banana, cherry\"\n\n\nHere, we concatenate the elements in the fruits vector with a custom separator, which is a comma followed by a space.\n\nCombining Numeric and Character Values\n\n\n# Create a numeric vector and a character vector\nprices &lt;- c(10, 5, 3)\nfruits &lt;- c(\"apple\", \"banana\", \"cherry\")\n\n# Use paste() to combine them\nitem_description &lt;- paste(prices, \"USD -\", fruits)\n\n# Print the result\nprint(item_description)\n\n[1] \"10 USD - apple\" \"5 USD - banana\" \"3 USD - cherry\"\n\n\nIn this example, we combine numeric values from the prices vector with character values from the fruits vector using paste().\n\nCollapsing a Character Vector\n\n\n# Create a character vector\nsentence &lt;- c(\"This\", \"is\", \"an\", \"example\", \"sentence\")\n\n# Use paste() to collapse the vector into a single string\ncollapsed_sentence &lt;- paste(sentence, collapse = \" \")\n\n# Print the result\nprint(collapsed_sentence)\n\n[1] \"This is an example sentence\"\n\n\nHere, we use paste() to collapse the elements of the sentence vector into a single string with spaces between words.\nThe paste() function is versatile and useful for various data manipulation tasks, such as creating custom labels, formatting output, and constructing complex strings from component parts. It allows you to combine character vectors in a flexible way."
  },
  {
    "objectID": "posts/2023-10-03_vectors/index.html#conclusion",
    "href": "posts/2023-10-03_vectors/index.html#conclusion",
    "title": "Exploring Vectors in R Programming: The Fundamental Building Blocks",
    "section": "Conclusion",
    "text": "Conclusion\nOf course, there are many functions that can be used with vectors and other data structures. You can even create your own functions when you learn how to write functions. I tried to explain some basic and frequently used functions here in order not to make the post too long.\nIn conclusion, vectors are the fundamental building blocks of data in R programming, akin to atoms in the world of matter. They are versatile, efficient, and indispensable for a wide range of data analysis tasks. By understanding their importance and mastering the use of vector-related functions, you can unlock the full potential of R for your data manipulation and analysis endeavors."
  },
  {
    "objectID": "posts/2023-12-19_lists/index.html",
    "href": "posts/2023-12-19_lists/index.html",
    "title": "Understanding Lists in R Programming",
    "section": "",
    "text": "R, a powerful statistical programming language, offers various data structures, and among them, lists stand out for their versatility and flexibility. Lists are collections of elements that can store different data types, making them highly useful for managing complex data. Thinking of lists in R as a shopping basket, imagine you’re at a store with a basket in hand. In this case:\n\nItems in the Basket: Each item you put in the basket represents an element in the list. These items can vary in size, shape, or type, just like elements in a list can be different data structures.\nVersatility in Choices: Just as you can put fruits, vegetables, and other products in your basket, a list in R can contain various data types like numbers, strings, vectors, matrices, or even other lists. This versatility allows you to gather different types of information or data together in one container.\nOrganizing Assortments: Similar to how you organize items in a basket to keep them together, a list helps in organizing different pieces of information or data structures within a single entity. This organization simplifies handling and retrieval, just like a well-organized basket makes it easier for you to find what you need.\nHandling Multiple Items: In a market basket, you might have fruits, vegetables, and other goods separately. Likewise, in R, lists can store outputs from functions that generate multiple results. For instance, a list can hold statistical summaries, model outputs, or simulation results together, allowing for easy access and analysis.\nHierarchy and Nesting: Sometimes, within a basket, you might have smaller bags or containers holding different items. Similarly, lists in R can be hierarchical or nested, containing sub-lists or various data structures within them. This nested structure is handy for representing complex data relationships.\n\nIn essence, just as a shopping basket helps you organize and carry diverse items conveniently while shopping, lists in R serve as flexible containers to organize and manage various types of data efficiently within a single entity. This flexibility enables the creation of hierarchical and heterogeneous structures, making lists one of the most powerful data structures in R."
  },
  {
    "objectID": "posts/2023-12-19_lists/index.html#introduction",
    "href": "posts/2023-12-19_lists/index.html#introduction",
    "title": "Understanding Lists in R Programming",
    "section": "",
    "text": "R, a powerful statistical programming language, offers various data structures, and among them, lists stand out for their versatility and flexibility. Lists are collections of elements that can store different data types, making them highly useful for managing complex data. Thinking of lists in R as a shopping basket, imagine you’re at a store with a basket in hand. In this case:\n\nItems in the Basket: Each item you put in the basket represents an element in the list. These items can vary in size, shape, or type, just like elements in a list can be different data structures.\nVersatility in Choices: Just as you can put fruits, vegetables, and other products in your basket, a list in R can contain various data types like numbers, strings, vectors, matrices, or even other lists. This versatility allows you to gather different types of information or data together in one container.\nOrganizing Assortments: Similar to how you organize items in a basket to keep them together, a list helps in organizing different pieces of information or data structures within a single entity. This organization simplifies handling and retrieval, just like a well-organized basket makes it easier for you to find what you need.\nHandling Multiple Items: In a market basket, you might have fruits, vegetables, and other goods separately. Likewise, in R, lists can store outputs from functions that generate multiple results. For instance, a list can hold statistical summaries, model outputs, or simulation results together, allowing for easy access and analysis.\nHierarchy and Nesting: Sometimes, within a basket, you might have smaller bags or containers holding different items. Similarly, lists in R can be hierarchical or nested, containing sub-lists or various data structures within them. This nested structure is handy for representing complex data relationships.\n\nIn essence, just as a shopping basket helps you organize and carry diverse items conveniently while shopping, lists in R serve as flexible containers to organize and manage various types of data efficiently within a single entity. This flexibility enables the creation of hierarchical and heterogeneous structures, making lists one of the most powerful data structures in R."
  },
  {
    "objectID": "posts/2023-12-19_lists/index.html#creating-lists",
    "href": "posts/2023-12-19_lists/index.html#creating-lists",
    "title": "Understanding Lists in R Programming",
    "section": "Creating Lists",
    "text": "Creating Lists\nCreating a list in R is straightforward. Use the list() function, passing the elements you want to include:\n\n# Creating a list with different data types\nmy_list &lt;- list(name = \"Fatih Tüzen\", age = 40, colors = c(\"red\", \"blue\", \"green\"), matrix_data = matrix(1:4, nrow = 2))"
  },
  {
    "objectID": "posts/2023-12-19_lists/index.html#accessing-elements-in-lists",
    "href": "posts/2023-12-19_lists/index.html#accessing-elements-in-lists",
    "title": "Understanding Lists in R Programming",
    "section": "Accessing Elements in Lists",
    "text": "Accessing Elements in Lists\nAccessing elements within a list involves using double brackets [[ ]] or the $ operator. Double brackets extract individual elements based on their positions, while $ accesses elements by their names (if named).\n\n# Accessing elements in a list\n# Using double brackets\nprint(my_list[[1]])  # Accesses the first element\n\n[1] \"Fatih Tüzen\"\n\nprint(my_list[[3]])  # Accesses the third element\n\n[1] \"red\"   \"blue\"  \"green\"\n\n# Using $ operator for named elements\nprint(my_list$colors)  # Accesses an element named \"name\"\n\n[1] \"red\"   \"blue\"  \"green\"\n\nprint(my_list[[\"matrix_data\"]])\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4"
  },
  {
    "objectID": "posts/2023-12-19_lists/index.html#manipulating-lists",
    "href": "posts/2023-12-19_lists/index.html#manipulating-lists",
    "title": "Understanding Lists in R Programming",
    "section": "Manipulating Lists",
    "text": "Manipulating Lists\n\nAdding Elements\nElements can easily be added to a list using indexing or appending functions like append() or c().\n\n# Adding elements to a list\nmy_list[[5]] &lt;- \"New Element\"\nmy_list &lt;- append(my_list, list(numbers = 0:9))\n\n\n\nRemoving Elements\nRemoving elements from a list can be done using indexing or specific functions like NULL assignment or list subsetting.\n\n# Removing elements from a list\nmy_list[[3]] &lt;- NULL  # Removes the third element\nmy_list\n\n$name\n[1] \"Fatih Tüzen\"\n\n$age\n[1] 40\n\n$matrix_data\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n[[4]]\n[1] \"New Element\"\n\n$numbers\n [1] 0 1 2 3 4 5 6 7 8 9\n\nmy_list &lt;- my_list[-c(2, 4)]  # Removes elements at positions 2 and 4\nmy_list\n\n$name\n[1] \"Fatih Tüzen\"\n\n$matrix_data\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n$numbers\n [1] 0 1 2 3 4 5 6 7 8 9"
  },
  {
    "objectID": "posts/2023-12-19_lists/index.html#use-cases-for-lists",
    "href": "posts/2023-12-19_lists/index.html#use-cases-for-lists",
    "title": "Understanding Lists in R Programming",
    "section": "Use Cases for Lists",
    "text": "Use Cases for Lists\n\nStoring Diverse Data\nLists are ideal for storing diverse data structures within a single container. For instance, in a statistical analysis, a list can hold vectors of different lengths, matrices, and even data frames, simplifying data management and analysis.\n\nExample 1: Dataset Description\nSuppose you’re working with a dataset that contains information about individuals. Using a list can help organize different aspects of this data.\n\n# Creating a list to store diverse data about individuals\nindividual_1 &lt;- list(\n  name = \"Alice\",\n  age = 28,\n  gender = \"Female\",\n  contact = list(\n    email = \"alice@example.com\",\n    phone = \"123-456-7890\"\n  ),\n  interests = c(\"Hiking\", \"Reading\", \"Coding\")\n)\n\nindividual_2 &lt;- list(\n  name = \"Bob\",\n  age = 35,\n  gender = \"Male\",\n  contact = list(\n    email = \"bob@example.com\",\n    phone = \"987-654-3210\"\n  ),\n  interests = c(\"Cooking\", \"Traveling\", \"Photography\")\n)\n\n# List of individuals\nindividuals_list &lt;- list(individual_1, individual_2)\n\nIn this example:\n\nEach individual is represented as a list containing various attributes like name, age, gender, contact, and interests.\nThe contact attribute further contains a sub-list for email and phone details.\nFinally, a individuals_list is a list that holds multiple individuals’ data.\n\n\n\nExample 2: Experimental Results\nConsider conducting experiments where each experiment yields different types of data. Lists can efficiently organize this diverse output.\n\n# Simulating experimental data and storing in a list\nexperiment_1 &lt;- list(\n  parameters = list(\n    temperature = 25,\n    duration = 60,\n    method = \"A\"\n  ),\n  results = matrix(rnorm(12), nrow = 3)  # Simulated experimental results\n)\n\nexperiment_2 &lt;- list(\n  parameters = list(\n    temperature = 30,\n    duration = 45,\n    method = \"B\"\n  ),\n  results = data.frame(\n    measurements = c(10, 15, 20),\n    labels = c(\"A\", \"B\", \"C\")\n  )\n)\n\n# List containing experimental data\nexperiment_list &lt;- list(experiment_1, experiment_2)\n\nIn this example:\n\nEach experiment is represented as a list containing parameters and results.\nparameters include details like temperature, duration, and method used in the experiment.\nresults can vary in structure - it could be a matrix, data frame, or any other data type.\n\n\n\nExample 3: Survey Responses\nImagine collecting survey responses where each respondent provides different types of answers. Lists can organize this diverse set of responses.\n\n# Survey responses stored in a list\nrespondent_1 &lt;- list(\n  name = \"Carol\",\n  age = 22,\n  answers = list(\n    question_1 = \"Yes\",\n    question_2 = c(\"Option B\", \"Option D\"),\n    question_3 = data.frame(\n      response = c(4, 3, 5),\n      category = c(\"A\", \"B\", \"C\")\n    )\n  )\n)\n\nrespondent_2 &lt;- list(\n  name = \"David\",\n  age = 30,\n  answers = list(\n    question_1 = \"No\",\n    question_2 = \"Option A\",\n    question_3 = matrix(1:6, nrow = 2)\n  )\n)\n\n# List of survey respondents\nrespondents_list &lt;- list(respondent_1, respondent_2)\n\nIn this example:\n\nEach respondent is represented as a list containing attributes like name, age, and answers.\nanswers contain responses to various questions where responses can be strings, vectors, data frames, or matrices.\n\n\n\n\nFunction Outputs\nLists are commonly used to store outputs from functions that produce multiple results. This approach keeps the results organized and accessible, enabling easy retrieval and further processing. Here are a few examples of how lists can be used to store outputs from functions that produce multiple results.\n\nExample 1: Statistical Summary\nSuppose you have a dataset and want to compute various statistical measures using a custom function:\n\n# Custom function to compute statistics\ncompute_statistics &lt;- function(data) {\n  stats_list &lt;- list(\n    mean = mean(data),\n    median = median(data),\n    sd = sd(data),\n    summary = summary(data)\n  )\n  return(stats_list)\n}\n\n# Usage of the function and storing outputs in a list\ndata &lt;- c(23, 45, 67, 89, 12)\nstatistics &lt;- compute_statistics(data)\nstatistics\n\n$mean\n[1] 47.2\n\n$median\n[1] 45\n\n$sd\n[1] 31.49921\n\n$summary\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   12.0    23.0    45.0    47.2    67.0    89.0 \n\n\nHere, statistics is a list containing various statistical measures such as mean, median, standard deviation, and summary statistics of the input data.\n\n\nExample 2: Model Fitting Outputs\nConsider a scenario where you fit a machine learning model and want to store various outputs:\n\n# Function to fit a model and store outputs\nfit_model &lt;- function(train_data, test_data) {\n  model &lt;- lm(y ~ x, data = train_data)  # Linear regression model\n  \n  # Compute predictions\n  predictions &lt;- predict(model, newdata = test_data)\n  \n  # Store outputs in a list\n  model_outputs &lt;- list(\n    fitted_model = model,\n    predictions = predictions,\n    coefficients = coef(model)\n  )\n  \n  return(model_outputs)\n}\n\n# Usage of the function and storing outputs in a list\ntrain_data &lt;- data.frame(x = 1:10, y = 2*(1:10) + rnorm(10))\ntest_data &lt;- data.frame(x = 11:15)\nmodel_results &lt;- fit_model(train_data, test_data)\nmodel_results\n\n$fitted_model\n\nCall:\nlm(formula = y ~ x, data = train_data)\n\nCoefficients:\n(Intercept)            x  \n     0.4516       1.8862  \n\n\n$predictions\n       1        2        3        4        5 \n21.20034 23.08659 24.97284 26.85908 28.74533 \n\n$coefficients\n(Intercept)           x \n  0.4516322   1.8862465 \n\n\nIn this example, model_results is a list containing the fitted model object, predictions on the test data, and coefficients of the linear regression model.\n\n\nExample 3: Simulation Outputs\nSuppose you are running a simulation and want to store various outputs for analysis:\n\n# Function to perform a simulation and store outputs\nrun_simulation &lt;- function(num_simulations) {\n  simulation_results &lt;- list()\n  \n  for (i in 1:num_simulations) {\n    # Perform simulation\n    simulated_data &lt;- rnorm(100)\n    \n    # Store simulation outputs in the list\n    simulation_results[[paste0(\"simulation_\", i)]] &lt;- simulated_data\n  }\n  \n  return(simulation_results)\n}\n\n# Usage of the function and storing outputs in a list\nsimulations &lt;- run_simulation(5)\n\nHere, simulations is a list containing the results of five separate simulations, each stored as a vector of simulated data.\nThese examples illustrate how lists can efficiently store multiple outputs from functions, making it easier to manage and analyze diverse results within R."
  },
  {
    "objectID": "posts/2023-12-19_lists/index.html#conclusion",
    "href": "posts/2023-12-19_lists/index.html#conclusion",
    "title": "Understanding Lists in R Programming",
    "section": "Conclusion",
    "text": "Conclusion\nIn conclusion, lists in R are a fundamental data structure, offering flexibility and versatility for managing and manipulating complex data. Mastering their use empowers R programmers to efficiently handle various types of data structures and hierarchies, facilitating seamless data analysis and manipulation."
  },
  {
    "objectID": "posts/2024-01-11_factors/index.html",
    "href": "posts/2024-01-11_factors/index.html",
    "title": "Cracking the Code of Categorical Data: A Guide to Factors in R",
    "section": "",
    "text": "https://allisonhorst.com/everything-else\n\n\nR programming is a versatile language known for its powerful statistical and data manipulation capabilities. One often-overlooked feature that plays a crucial role in organizing and analyzing data is the use of factors. In this blog post, we’ll delve into the world of factors, exploring what they are, why they are important, and how they can be effectively utilized in R programming."
  },
  {
    "objectID": "posts/2024-01-11_factors/index.html#introduction",
    "href": "posts/2024-01-11_factors/index.html#introduction",
    "title": "Cracking the Code of Categorical Data: A Guide to Factors in R",
    "section": "",
    "text": "https://allisonhorst.com/everything-else\n\n\nR programming is a versatile language known for its powerful statistical and data manipulation capabilities. One often-overlooked feature that plays a crucial role in organizing and analyzing data is the use of factors. In this blog post, we’ll delve into the world of factors, exploring what they are, why they are important, and how they can be effectively utilized in R programming."
  },
  {
    "objectID": "posts/2024-01-11_factors/index.html#creation-of-factors",
    "href": "posts/2024-01-11_factors/index.html#creation-of-factors",
    "title": "Cracking the Code of Categorical Data: A Guide to Factors in R",
    "section": "Creation of Factors",
    "text": "Creation of Factors\nCreating factors in R involves converting categorical data into a specific data type that represents distinct levels. The most common method involves using the factor() function.\n\n# Creating a factor from a character vector\ngender_vector &lt;- c(rep(\"Male\",5),rep(\"Female\",7))\ngender_factor &lt;- factor(gender_vector)\n\n# Displaying the factor\nprint(gender_factor)\n\n [1] Male   Male   Male   Male   Male   Female Female Female Female Female\n[11] Female Female\nLevels: Female Male\n\n\nYou can explicitly specify the levels when creating a factor.\n\n# Creating a factor with specified levels\neducation_vector &lt;- c(\"High School\", \"Bachelor's\", \"Master's\", \"PhD\")\neducation_factor &lt;- factor(education_vector, levels = c(\"High School\", \"Bachelor's\", \"Master's\", \"PhD\"))\n\n# Displaying the factor\nprint(education_factor)\n\n[1] High School Bachelor's  Master's    PhD        \nLevels: High School Bachelor's Master's PhD\n\n\nFor ordinal data, factors can be ordered.\n\n# Creating an ordered factor\nrating_vector &lt;-  c(rep(\"Low\",4),rep(\"Medium\",5),rep(\"High\",2))\nrating_factor &lt;- factor(rating_vector, ordered = TRUE, levels = c(\"Low\", \"Medium\", \"High\"))\n\n# Displaying the ordered factor\nprint(rating_factor)\n\n [1] Low    Low    Low    Low    Medium Medium Medium Medium Medium High  \n[11] High  \nLevels: Low &lt; Medium &lt; High\n\n\nYou can change the order of levels. ordered=TRUE indicates that the levels are ordered.\n\nrating_vector_2 &lt;- factor(rating_vector,\n                          levels = c(\"High\",\"Medium\",\"Low\"), \n                          ordered = TRUE)\nprint(rating_vector_2)\n\n [1] Low    Low    Low    Low    Medium Medium Medium Medium Medium High  \n[11] High  \nLevels: High &lt; Medium &lt; Low\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou can also use gl() function in order to generate factors by specifying the pattern of their levels.\nSyntax:\ngl(n, k, length, labels, ordered)\n\nParameters:\nn: Number of levels\nk: Number of replications\nlength: Length of result\nlabels: Labels for the vector(optional)\nordered: Boolean value to order the levels\n\nnew_factor &lt;- gl(n = 3, \n                 k = 4, \n                 labels = c(\"level1\", \"level2\",\"level3\"),\n                 ordered = TRUE)\nprint(new_factor)\n\n [1] level1 level1 level1 level1 level2 level2 level2 level2 level3 level3\n[11] level3 level3\nLevels: level1 &lt; level2 &lt; level3"
  },
  {
    "objectID": "posts/2024-01-11_factors/index.html#understanding-factors",
    "href": "posts/2024-01-11_factors/index.html#understanding-factors",
    "title": "Cracking the Code of Categorical Data: A Guide to Factors in R",
    "section": "Understanding Factors",
    "text": "Understanding Factors\nIn R, a factor is a data type used to categorize and store data. Essentially, it represents a categorical variable and is particularly useful when dealing with variables that have a fixed number of unique values. Factors can be thought of as a way to represent and work with categorical data efficiently.\nFactors in R programming are not merely a data type; they are a powerful tool for elevating the efficiency and interpretability of your code. Whether you are analyzing survey responses, evaluating educational levels, or visualizing temperature categories, factors bring a level of organization and clarity that is indispensable in the data analysis landscape. By embracing factors, you unlock a sophisticated approach to handling categorical data, enabling you to extract deeper insights from your datasets and empowering your R code with a robust foundation for statistical analyses.\nFactors are employed in various scenarios, from handling categorical data, statistical modeling, memory efficiency, maintaining data integrity, creating visualizations, to simplifying data manipulation tasks in R programming.\n\nCategorical Data Representation\nFactors allow you to efficiently represent categorical data in R. Categorical variables, such as gender, education level, or geographic region, are common in many datasets. Factors provide a structured way to handle and analyze these categories. Converting this into a factor not only groups these levels but also standardizes their representation across the dataset, allowing for consistent analysis.\n\n# Sample data as a vector\ngender &lt;- c(\"Male\", \"Female\", \"Male\", \"Male\", \"Female\")\n\n# Converting to factor\ngender_factor &lt;- factor(gender)\n\n# Checking levels\nlevels(gender_factor)\n\n[1] \"Female\" \"Male\"  \n\n# Checking unique values within the factor\nunique(gender_factor)\n\n[1] Male   Female\nLevels: Female Male\n\n\n\n\nStatistical Analysis and Modeling\nStatistical models often require categorical variables to be converted into factors. When performing regression analysis or any statistical modeling in R, factors ensure that categorical variables are correctly interpreted, allowing models to account for categorical variations in the data.\nLet’s examine the example to include two factor variables and showcase their roles in a statistical model. We’ll consider the scenario of exploring the impact of both income levels and education levels on spending behavior.\n\n# Simulated data for spending behavior\nn &lt;- 100\nspending &lt;- runif(n, min = 100, max = 600)\n\nincome_levels &lt;- sample(c(\"Low\", \"High\", \"Medium\"), \n                        size = n, \n                        replace = TRUE)\neducation_levels &lt;- sample(c(\"High School\", \"Graduate\", \"Undergraduate\"), \n                           size = n, \n                           replace = TRUE)\n\n# Creating factor variables for income and education\nincome_factor &lt;- factor(income_levels)\neducation_factor &lt;- factor(education_levels)\n\n# Linear model with both income and education as factor variables\nmodel &lt;- lm(spending ~ income_factor + education_factor)\nsummary(model)\n\n\nCall:\nlm(formula = spending ~ income_factor + education_factor)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-246.077 -111.039    4.602  114.327  256.399 \n\nCoefficients:\n                              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                    389.887     31.169  12.509   &lt;2e-16 ***\nincome_factorLow               -60.107     34.900  -1.722   0.0883 .  \nincome_factorMedium            -28.957     34.440  -0.841   0.4026    \neducation_factorHigh School    -38.522     35.799  -1.076   0.2846    \neducation_factorUndergraduate   -6.563     32.608  -0.201   0.8409    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 140.3 on 95 degrees of freedom\nMultiple R-squared:  0.04182,   Adjusted R-squared:  0.001473 \nF-statistic: 1.037 on 4 and 95 DF,  p-value: 0.3926\n\n\nThe output summary of the model will now provide information about the impact of both income levels and education levels on spending:\n\nCoefficients: Each factor level within income_factor and education_factor will have its own coefficient, indicating its estimated impact on spending.\nInteractions: If there is an interaction term (which we don’t have in this simplified example), it would represent the combined effect of both factors on the response variable.\n\nThe summary output will provide a comprehensive view of how different combinations of income and education levels influence spending behavior. This type of model allows for a more nuanced understanding of the relationships between multiple categorical variables and a continuous response variable.\n\n\nEfficiency in Memory and Performance\nFactors in R are implemented as integers that point to a levels attribute, which contains unique values within the categorical variable. This representation can save memory compared to storing string labels for each observation. It also speeds up some operations as integers are more efficiently handled in computations.\n\n# Creating a large dataset with a categorical variable\nlarge_data &lt;- sample(c(\"A\", \"B\", \"C\", \"D\"), 10^6, replace = TRUE)\n\n# Memory usage comparison\nobject.size(large_data) # Memory usage without factor\n\n8000272 bytes\n\nlarge_data_factor &lt;- factor(large_data)\nobject.size(large_data_factor) # Memory usage with factor\n\n4000688 bytes\n\n\nIn this example:\n\nWe generate a large dataset (large_data) with a categorical variable.\nWe compare the memory usage between the original character vector and the factor representation.\n\nWhen you run the code, you’ll observe that the memory usage of the factor representation is significantly smaller than that of the character vector. This highlights the memory efficiency gained by representing categorical variables as factors.\nThe compact integer representation not only saves memory but also accelerates various operations involving categorical variables. This is particularly advantageous when working with extensive datasets or when dealing with resource constraints.\nEfficient memory usage becomes critical in scenarios where datasets are substantial, such as in big data analytics or machine learning tasks. By leveraging factors, R programmers can ensure that their code runs smoothly and effectively, even when dealing with large and complex datasets.\n\n\nData Integrity and Consistency\nFactors enforce the integrity of categorical data. They ensure that only predefined levels are used within a variable, preventing the introduction of new, unforeseen categories. This maintains consistency and prevents errors in analysis or modeling caused by unexpected categories.\nOne of the key features of factors is their ability to explicitly define and enforce levels within a categorical variable. This ensures that the data conforms to a consistent set of categories, providing a robust framework for analysis.\nConsider a scenario where we have a factor representing temperature categories: ‘Low’, ‘Medium’, and ‘High’. Let’s explore how factors help maintain consistency:\n\n# Creating a factor with specified levels\ntemperature &lt;- c(\"Low\", \"Medium\", \"High\", \"Low\", \"Extreme\")\n\n# Defining specific levels\ntemperature_factor &lt;- factor(temperature, levels = c(\"Low\", \"Medium\", \"High\"))\n\n# Replacing with an undefined level will generate a warning\ntemperature_factor[5] &lt;- \"Extreme High\"\n\nWarning in `[&lt;-.factor`(`*tmp*`, 5, value = \"Extreme High\"): invalid factor\nlevel, NA generated\n\n\nIn this example:\n\nWe create a factor representing temperature categories.\nWe explicitly define specific levels using the levels parameter.\nAn attempt to introduce a new, undefined level (‘Extreme High’) generates a warning.\n\nWhen you run the code, you’ll observe that attempting to replace a level with an undefined value triggers a warning. This emphasizes the role of factors in preserving data integrity and consistency. Any attempt to introduce new or undefined categories is flagged, preventing unintended changes to the data.\nIn real-world scenarios, maintaining data integrity is crucial for accurate analyses and meaningful interpretations. Factors provide a safeguard against inadvertent errors, ensuring that the categorical data remains consistent throughout the analysis process. This is particularly important in collaborative projects or situations where data is sourced from multiple channels.\n\n\nGraphical Representations and Visualizations\nFactors in R contribute significantly to the creation of clear and insightful visualizations. By ensuring proper ordering and labeling of categorical data, factors play a pivotal role in generating meaningful graphs and charts that enhance data interpretation.\nWhen creating visual representations of data, such as bar plots or pie charts, factors provide a structured foundation. They ensure that the categories are appropriately arranged and labeled, allowing for accurate communication of insights.\nLet’s create a simple bar plot using the ggplot2 library, showcasing the distribution of product categories:\n\n# Sample data: product categories\n\ncategories &lt;- sample(c(\"Electronics\", \"Clothing\", \"Food\"),\n                     size = 20 ,\n                     replace = TRUE)\ncategory_factor &lt;- factor(categories)\n\n# Creating a bar plot with factors using ggplot2\nlibrary(ggplot2)\n\n# Creating a data frame for ggplot\ndata &lt;- data.frame(category = category_factor)\n\n# Creating a bar plot\nggplot(data, aes(x = category, fill = category)) +\n  geom_bar() +\n  labs(title = \"Distribution of Product Categories\", \n       x = \"Category\", \n       y = \"Count\")\n\n\n\n\n\n\n\n\nIn this example:\n\nWe have a sample dataset representing different product categories.\nThe variable category_factor is a factor representing these categories.\nWe use ggplot2 to create a bar plot, mapping the factor levels to the x-axis and fill color.\n\nWhen you run the code, you’ll generate a bar plot that effectively visualizes the distribution of product categories. The factor ensures that the categories are properly ordered and labeled, providing a clear representation of the data.\nIn data analysis, effective visualization is often the key to conveying insights to stakeholders. By leveraging factors in graphical representations, R users enhance the clarity and interpretability of their visualizations. This is particularly valuable when dealing with categorical data, where the correct representation of levels is essential for accurate communication."
  },
  {
    "objectID": "posts/2024-01-11_factors/index.html#conclusion",
    "href": "posts/2024-01-11_factors/index.html#conclusion",
    "title": "Cracking the Code of Categorical Data: A Guide to Factors in R",
    "section": "Conclusion",
    "text": "Conclusion\nIn the intricate world of data analysis, where insights hide within categorical nuances, factors in R emerge as indispensable guides, offering a pathway to crack the code of categorical data. Through the exploration of their multifaceted roles, we’ve uncovered how factors bring structure, efficiency, and integrity to the table.\nFactors, as revealed in our journey, stand as the bedrock for efficient data representation and manipulation. They unlock the power of statistical modeling, enabling us to dissect the impact of categorical variables on outcomes with precision. Memory efficiency becomes a notable ally, especially in the face of colossal datasets, where factors shine by optimizing computational performance.\nMaintaining data integrity is a critical aspect of any analytical endeavor, and factors act as vigilant guardians, ensuring that categorical variables adhere to predefined levels. The blog post showcased how factors not only prevent unintended changes but also serve as sentinels against the introduction of undefined categories.\nThe journey through the visualization realm illustrated that factors are not just behind-the-scenes players; they are conductors orchestrating visually compelling narratives. By ensuring proper ordering and labeling, factors elevate the impact of graphical representations, making categorical data come alive in meaningful visual stories.\nAs we conclude our guide to factors in R, we find ourselves equipped with a toolkit to navigate the categorical maze. Whether you’re a seasoned data scientist or an aspiring analyst, embracing factors unlocks a deeper understanding of your data, paving the way for more accurate analyses, clearer visualizations, and robust statistical models.\nCracking the code of categorical data is not merely a technical feat—it’s an art. Factors, in their simplicity and versatility, empower us to decode the richness embedded in categorical variables, turning what might seem like a labyrinth into a comprehensible landscape of insights. So, let the journey with factors in R be your compass, guiding you through the intricate tapestry of categorical data analysis. Happy coding!"
  },
  {
    "objectID": "posts/2024-04-15_apply_map/index.html",
    "href": "posts/2024-04-15_apply_map/index.html",
    "title": "Exploring apply, sapply, lapply, and map Functions in R",
    "section": "",
    "text": "In R programming, Apply functions (apply(), sapply(), lapply()) and the map() function from the purrr package are powerful tools for data manipulation and analysis. In this comprehensive guide, we will delve into the syntax, usage, and examples of each function, including the usage of built-in functions and additional arguments, as well as performance benchmarking."
  },
  {
    "objectID": "posts/2024-04-15_apply_map/index.html#introduction",
    "href": "posts/2024-04-15_apply_map/index.html#introduction",
    "title": "Exploring apply, sapply, lapply, and map Functions in R",
    "section": "",
    "text": "In R programming, Apply functions (apply(), sapply(), lapply()) and the map() function from the purrr package are powerful tools for data manipulation and analysis. In this comprehensive guide, we will delve into the syntax, usage, and examples of each function, including the usage of built-in functions and additional arguments, as well as performance benchmarking."
  },
  {
    "objectID": "posts/2024-04-15_apply_map/index.html#understanding-apply-function",
    "href": "posts/2024-04-15_apply_map/index.html#understanding-apply-function",
    "title": "Exploring apply, sapply, lapply, and map Functions in R",
    "section": "Understanding apply() Function",
    "text": "Understanding apply() Function\nThe apply() function in R is used to apply a specified function to the rows or columns of an array. Its syntax is as follows:\n\napply(X, MARGIN, FUN, ...)\n\n\nX: The input data, typically an array or matrix.\nMARGIN: A numeric vector indicating which margins should be retained. Use 1 for rows, 2 for columns.\nFUN: The function to apply.\n...: Additional arguments to be passed to the function.\n\nLet’s calculate the mean of each row in a matrix using apply():\n\nmatrix_data &lt;- matrix(1:9, nrow = 3)\nrow_means &lt;- apply(matrix_data, 1, mean)\nprint(row_means)\n\n[1] 4 5 6\n\n\nThis example computes the mean of each row in the matrix.\nLet’s calculate the standard deviation of each column in a matrix and specify additional arguments (na.rm = TRUE) using apply():\n\ncolumn_stdev &lt;- apply(matrix_data, 2, sd, na.rm = TRUE)\nprint(column_stdev)\n\n[1] 1 1 1"
  },
  {
    "objectID": "posts/2024-04-15_apply_map/index.html#understanding-sapply-function",
    "href": "posts/2024-04-15_apply_map/index.html#understanding-sapply-function",
    "title": "Exploring apply, sapply, lapply, and map Functions in R",
    "section": "Understanding sapply() Function",
    "text": "Understanding sapply() Function\nThe sapply() function is a simplified version of lapply() that returns a vector or matrix. Its syntax is similar to lapply():\n\nsapply(X, FUN, ...)\n\n\nX: The input data, typically a list.\nFUN: The function to apply.\n...: Additional arguments to be passed to the function.\n\nLet’s calculate the sum of each element in a list using sapply():\n\nnum_list &lt;- list(a = 1:3, b = 4:6, c = 7:9)\nsum_results &lt;- sapply(num_list, sum)\nprint(sum_results)\n\n a  b  c \n 6 15 24 \n\n\nThis example computes the sum of each element in the list.\nLet’s convert each element in a list to uppercase using sapply() and the toupper() function:\n\ntext_list &lt;- list(\"hello\", \"world\", \"R\", \"programming\")\nuppercase_text &lt;- sapply(text_list, toupper)\nprint(uppercase_text)\n\n[1] \"HELLO\"       \"WORLD\"       \"R\"           \"PROGRAMMING\"\n\n\nHere, sapply() applies the toupper() function to each element in the list, converting them to uppercase."
  },
  {
    "objectID": "posts/2024-04-15_apply_map/index.html#understanding-lapply-function",
    "href": "posts/2024-04-15_apply_map/index.html#understanding-lapply-function",
    "title": "Exploring apply, sapply, lapply, and map Functions in R",
    "section": "Understanding lapply() Function",
    "text": "Understanding lapply() Function\nThe lapply() function applies a function to each element of a list and returns a list. Its syntax is as follows:\n\nlapply(X, FUN, ...)\n\n\nX: The input data, typically a list.\nFUN: The function to apply.\n...: Additional arguments to be passed to the function.\n\nLet’s apply a custom function to each element of a list using lapply():\n\nnum_list &lt;- list(a = 1:3, b = 4:6, c = 7:9)\ncustom_function &lt;- function(x) sum(x) * 2\nresult_list &lt;- lapply(num_list, custom_function)\nprint(result_list)\n\n$a\n[1] 12\n\n$b\n[1] 30\n\n$c\n[1] 48\n\n\nIn this example, lapply() applies the custom function to each element in the list.\nLet’s extract the vowels from each element in a list of words using lapply() and a custom function:\n\nword_list &lt;- list(\"apple\", \"banana\", \"orange\", \"grape\")\nvowel_list &lt;- lapply(word_list, function(word) grep(\"[aeiou]\", strsplit(word, \"\")[[1]], value = TRUE))\nprint(vowel_list)\n\n[[1]]\n[1] \"a\" \"e\"\n\n[[2]]\n[1] \"a\" \"a\" \"a\"\n\n[[3]]\n[1] \"o\" \"a\" \"e\"\n\n[[4]]\n[1] \"a\" \"e\"\n\n\nHere, lapply() applies the custom function to each element in the list, extracting vowels from words."
  },
  {
    "objectID": "posts/2024-04-15_apply_map/index.html#understanding-map-function",
    "href": "posts/2024-04-15_apply_map/index.html#understanding-map-function",
    "title": "Exploring apply, sapply, lapply, and map Functions in R",
    "section": "Understanding map() Function",
    "text": "Understanding map() Function\nThe map() function from the purrr package is similar to lapply() but offers a more consistent syntax and returns a list. Its syntax is as follows:\n\nmap(.x, .f, ...)\n\n\n.x: The input data, typically a list.\n.f: The function to apply.\n...: Additional arguments to be passed to the function.\n\nLet’s apply a lambda function to each element of a list using map():\n\nlibrary(purrr)\nnum_list &lt;- list(a = 1:3, b = 4:6, c = 7:9)\nmapped_results &lt;- map(num_list, ~ .x^2)\nprint(mapped_results)\n\n$a\n[1] 1 4 9\n\n$b\n[1] 16 25 36\n\n$c\n[1] 49 64 81\n\n\nIn this example, map() applies the lambda function (squared) to each element in the list.\nLet’s calculate the lengths of strings in a list using map() and the nchar() function:\n\ntext_list &lt;- list(\"hello\", \"world\", \"R\", \"programming\")\nstring_lengths &lt;- map(text_list, nchar)\nprint(string_lengths)\n\n[[1]]\n[1] 5\n\n[[2]]\n[1] 5\n\n[[3]]\n[1] 1\n\n[[4]]\n[1] 11\n\n\nHere, map() applies the nchar() function to each element in the list, calculating the length of each string."
  },
  {
    "objectID": "posts/2024-04-15_apply_map/index.html#understanding-map-function-variants",
    "href": "posts/2024-04-15_apply_map/index.html#understanding-map-function-variants",
    "title": "Exploring apply, sapply, lapply, and map Functions in R",
    "section": "Understanding map() Function Variants",
    "text": "Understanding map() Function Variants\nIn addition to the map() function, the purrr package provides several variants that are specialized for different types of output: map_lgl(), map_int(), map_dbl(), and map_chr(). These variants are particularly useful when you expect the output to be of a specific data type, such as logical, integer, double, or character.\n\nmap_lgl(): This variant is used when the output of the function is expected to be a logical vector.\nmap_int(): Use this variant when the output of the function is expected to be an integer vector.\nmap_dbl(): This variant is used when the output of the function is expected to be a double vector.\nmap_chr(): Use this variant when the output of the function is expected to be a character vector.\n\nThese variants provide stricter type constraints compared to the generic map() function, which can be useful for ensuring the consistency of the output type across iterations. They are particularly handy when working with functions that have predictable output types.\n\nlibrary(purrr)\n\n# Define a list of vectors\nnum_list &lt;- list(a = 1:3, b = 4:6, c = 7:9)\n\n# Use map_lgl() to check if all elements in each vector are even\neven_check &lt;- map_lgl(num_list, function(x) all(x %% 2 == 0))\nprint(even_check)\n\n    a     b     c \nFALSE FALSE FALSE \n\n# Use map_int() to compute the sum of each vector\nvector_sums &lt;- map_int(num_list, sum)\nprint(vector_sums)\n\n a  b  c \n 6 15 24 \n\n# Use map_dbl() to compute the mean of each vector\nvector_means &lt;- map_dbl(num_list, mean)\nprint(vector_means)\n\na b c \n2 5 8 \n\n# Use map_chr() to convert each vector to a character vector\nvector_strings &lt;- map_chr(num_list, toString)\nprint(vector_strings)\n\n        a         b         c \n\"1, 2, 3\" \"4, 5, 6\" \"7, 8, 9\" \n\n\nBy using these specialized variants, you can ensure that the output of your mapping operation adheres to your specific data type requirements, leading to cleaner and more predictable code."
  },
  {
    "objectID": "posts/2024-04-15_apply_map/index.html#performance-comparison",
    "href": "posts/2024-04-15_apply_map/index.html#performance-comparison",
    "title": "Exploring apply, sapply, lapply, and map Functions in R",
    "section": "Performance Comparison",
    "text": "Performance Comparison\nTo compare the performance of these functions, it’s important to note that the execution time may vary depending on the hardware specifications of your computer, the size of the dataset, and the complexity of the operations performed. While one function may perform better in one scenario, it may not be the case in another. Therefore, it’s recommended to benchmark the functions in your specific use case.\nLet’s benchmark the computation of the sum of a large list using different functions:\n\nlibrary(microbenchmark)\n\n# Create a 100 x 100 matrix\nmatrix_data &lt;- matrix(rnorm(10000), nrow = 100)\n\n# Use apply() function to compute the sum for each column\nbenchmark_results &lt;- microbenchmark(\n  apply_sum = apply(matrix_data, 2, sum),\n  sapply_sum = sapply(matrix_data, sum),\n  lapply_sum = lapply(matrix_data, sum),\n  map_sum = map_dbl(as.list(matrix_data), sum),  # We need to convert the matrix to a list for the map function\n  times = 100\n)\n\nprint(benchmark_results)\n\nUnit: microseconds\n       expr    min      lq     mean  median      uq     max neval\n  apply_sum   98.1  122.95  143.123  135.60  153.35   277.8   100\n sapply_sum 2326.7 2429.75 2941.094 2514.85 2852.55 11218.3   100\n lapply_sum 2150.6 2247.55 2860.614 2364.90 2930.80  6556.0   100\n    map_sum 5063.5 5342.45 6009.474 5738.35 6788.35  8139.7   100\n\n\napply_sum demonstrates the fastest processing time among the alternatives,. These results suggest that while apply() function offers the fastest processing time, it’s still relatively slow compared to other options. When evaluating these results, it’s crucial to consider factors beyond processing time, such as usability and functionality, to select the most suitable function for your specific needs.\nOverall, the choice of function depends on factors such as speed, ease of use, and compatibility with the data structure. It’s essential to benchmark different alternatives in your specific use case to determine the most suitable function for your needs."
  },
  {
    "objectID": "posts/2024-04-15_apply_map/index.html#conclusion",
    "href": "posts/2024-04-15_apply_map/index.html#conclusion",
    "title": "Exploring apply, sapply, lapply, and map Functions in R",
    "section": "Conclusion",
    "text": "Conclusion\nApply functions (apply(), sapply(), lapply()) and the map() function from the purrr package are powerful tools for data manipulation and analysis in R. Each function has its unique features and strengths, making them suitable for various tasks.\n\napply() function is versatile and operates on matrices, allowing for row-wise or column-wise operations. However, its performance may vary depending on the size of the dataset and the nature of the computation.\nsapply() and lapply() functions are convenient for working with lists and provide more optimized implementations compared to apply(). They offer flexibility and ease of use, making them suitable for a wide range of tasks.\nmap() function offers a more consistent syntax compared to lapply() and provides additional variants (map_lgl(), map_int(), map_dbl(), map_chr()) for handling specific data types. While it may exhibit slower performance in some cases, its functionality and ease of use make it a valuable tool for functional programming in R.\n\nWhen choosing the most suitable function for your task, it’s essential to consider factors beyond just performance. Usability, compatibility with data structures, and the nature of the computation should also be taken into account. Additionally, the performance of these functions may vary depending on the hardware specifications of your computer, the size of the dataset, and the complexity of the operations performed. Therefore, it’s recommended to benchmark the functions in your specific use case and evaluate them based on multiple criteria to make an informed decision.\nBy mastering these functions and understanding their nuances, you can streamline your data analysis workflows and tackle a wide range of analytical tasks with confidence in R."
  },
  {
    "objectID": "posts/2024-09-19_pivot/index.html",
    "href": "posts/2024-09-19_pivot/index.html",
    "title": "Mastering Data Transformation in R with pivot_longer and pivot_wider",
    "section": "",
    "text": "Artwork by: Shannon Pileggi and Allison Horst"
  },
  {
    "objectID": "posts/2024-09-19_pivot/index.html#introduction",
    "href": "posts/2024-09-19_pivot/index.html#introduction",
    "title": "Mastering Data Transformation in R with pivot_longer and pivot_wider",
    "section": "Introduction",
    "text": "Introduction\nData analysis requires a deep understanding of how to structure data effectively. Often, datasets are not in the format most suitable for analysis or visualization. That’s where data transformation comes in. Converting data between wide (horizontal) and long (vertical) formats is an essential skill for any data analyst or scientist, ensuring that data is correctly organized for tasks such as statistical modeling, machine learning, or visualization.\nThe concept of tidy data plays a crucial role in this process. Tidy data principles advocate for a structure where each variable forms a column and each observation forms a row. This consistent structure facilitates easier and more effective data manipulation, analysis, and visualization. By adhering to these principles, you can ensure that your data is well-organized and suited to various analytical tasks.\nIn this post, we’ll dive into data transformation using the tidyr package in R, specifically focusing on the pivot_longer() and pivot_wider() functions. We’ll explore their theoretical background, use cases, and the importance of reshaping data in data science. Additionally, we’ll discuss when and why we should use wide or long formats, and analyze their advantages and disadvantages."
  },
  {
    "objectID": "posts/2024-09-19_pivot/index.html#why-data-transformation-is-essential",
    "href": "posts/2024-09-19_pivot/index.html#why-data-transformation-is-essential",
    "title": "Mastering Data Transformation in R with pivot_longer and pivot_wider",
    "section": "Why Data Transformation is Essential",
    "text": "Why Data Transformation is Essential\nIn data science, structuring data appropriately can be the difference between smooth analysis and frustrating errors. Here’s why reshaping data matters:\n\nPreparation for modeling: Many machine learning algorithms require data in long format, where each observation is represented by a single row.\nImproved visualization: Libraries like ggplot2 in R are designed to work best with long data, allowing for more flexible and detailed plots.\nData management and reporting: Certain summary statistics or reports are more intuitive when the data is presented in a wide format, making tables easier to interpret.\n\nChoosing the correct format can optimize both data handling and the clarity of your analysis."
  },
  {
    "objectID": "posts/2024-09-19_pivot/index.html#theoretical-overview",
    "href": "posts/2024-09-19_pivot/index.html#theoretical-overview",
    "title": "Mastering Data Transformation in R with pivot_longer and pivot_wider",
    "section": "Theoretical Overview",
    "text": "Theoretical Overview\n\npivot_longer(): Converts wide-format data (where variables are spread across columns) into a long format (where each variable is in a single column). This is particularly useful when you need to simplify your dataset for analysis or visualization.\npivot_wider(): Converts long-format data (where values are repeated across rows) into wide format, useful when data summarization or comparison across categories is required.\n\nFunction Arguments:\n\npivot_longer():\n\ndata: The dataset to be transformed.\ncols: Specifies the columns to pivot from wide to long.\nnames_to: The name of the new column that will store the pivoted column names.\nvalues_to: The name of the new column that will store the pivoted values.\nvalues_drop_na: Drops rows where the pivoted value is NA if set to TRUE.\n\npivot_wider():\n\ndata: The dataset to be transformed.\nnames_from: Specifies which column’s values should become the column names in the wide format.\nvalues_from: The column that contains the values to fill into the new wide-format columns.\nvalues_fill: A value to fill missing entries when transforming to wide format."
  },
  {
    "objectID": "posts/2024-09-19_pivot/index.html#advantages-and-disadvantages-of-wide-vs.-long-formats",
    "href": "posts/2024-09-19_pivot/index.html#advantages-and-disadvantages-of-wide-vs.-long-formats",
    "title": "Mastering Data Transformation in R with pivot_longer and pivot_wider",
    "section": "Advantages and Disadvantages of Wide vs. Long Formats",
    "text": "Advantages and Disadvantages of Wide vs. Long Formats\n\n\n\n\n\n\n\nWide Format\nLong Format\n\n\n\n\nAdvantages: Easier to read for summary tables and simple reports. Can be more efficient for certain statistical summaries (e.g., total sales per month).\nAdvantages: Ideal for detailed analysis and visualization (e.g., time series plots). Allows flexible data manipulation and easier grouping/summarization.\n\n\nDisadvantages: Can become unwieldy with many variables or time points. Not suitable for machine learning or statistical models that expect long data.\nDisadvantages: Harder to interpret at a glance. May require more computational resources when handling large datasets.\n\n\n\nWhen to Use Wide Format: Wide format is best for reporting, as it condenses information into fewer rows and is often more visually intuitive in summary tables.\nWhen to Use Long Format: Long format is essential for most analysis, particularly when working with time-series data, categorical data, or preparing data for machine learning algorithms."
  },
  {
    "objectID": "posts/2024-09-19_pivot/index.html#some-examples",
    "href": "posts/2024-09-19_pivot/index.html#some-examples",
    "title": "Mastering Data Transformation in R with pivot_longer and pivot_wider",
    "section": "Some Examples",
    "text": "Some Examples\n\nBasic Data Transformation Using pivot_longer()\nLet’s revisit the monthly sales data:\n\nlibrary(tidyr)\nsales_data &lt;- data.frame(\n  product = c(\"A\", \"B\", \"C\"),\n  Jan = c(500, 600, 300),\n  Feb = c(450, 700, 320),\n  Mar = c(520, 640, 310)\n)\nsales_data\n\n  product Jan Feb Mar\n1       A 500 450 520\n2       B 600 700 640\n3       C 300 320 310\n\n\nUsing pivot_longer(), we convert it to a long format:\n\nsales_long &lt;- pivot_longer(sales_data, cols = Jan:Mar, \n                           names_to = \"month\", values_to = \"sales\")\nsales_long\n\n# A tibble: 9 × 3\n  product month sales\n  &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt;\n1 A       Jan     500\n2 A       Feb     450\n3 A       Mar     520\n4 B       Jan     600\n5 B       Feb     700\n6 B       Mar     640\n7 C       Jan     300\n8 C       Feb     320\n9 C       Mar     310\n\n\nThis format is perfect for generating time-series visualizations, analyzing trends, or feeding the data into statistical models that expect a single observation per row.\n\n\nReshaping Data with pivot_wider()\nNow, let’s take the long-format data from Example 1 and use pivot_wider() to convert it back to wide format:\n\nsales_wide &lt;- pivot_wider(sales_long, names_from = month, values_from = sales)\nsales_wide\n\n# A tibble: 3 × 4\n  product   Jan   Feb   Mar\n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 A         500   450   520\n2 B         600   700   640\n3 C         300   320   310\n\n\nThis wide format is easier to read when creating summary reports or comparison tables across months.\n\n\nHandling Complex Data with Missing Values\nLet’s extend the example to include regional sales data with missing values:\n\nsales_data &lt;- data.frame(\n  product = c(\"A\", \"A\", \"B\", \"B\", \"C\", \"C\"),\n  region = c(\"North\", \"South\", \"North\", \"South\", \"North\", \"South\"),\n  Jan = c(500, NA, 600, 580, 300, 350),\n  Feb = c(450, 490, NA, 700, 320, 400)\n)\nsales_data\n\n  product region Jan Feb\n1       A  North 500 450\n2       A  South  NA 490\n3       B  North 600  NA\n4       B  South 580 700\n5       C  North 300 320\n6       C  South 350 400\n\n\nUsing pivot_longer(), we can transform this dataset while removing missing values:\n\nsales_long &lt;- pivot_longer(sales_data, cols = Jan:Feb, \n                           names_to = \"month\", values_to = \"sales\", \n                           values_drop_na = TRUE)\n\nsales_long\n\n# A tibble: 10 × 4\n   product region month sales\n   &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt;\n 1 A       North  Jan     500\n 2 A       North  Feb     450\n 3 A       South  Feb     490\n 4 B       North  Jan     600\n 5 B       South  Jan     580\n 6 B       South  Feb     700\n 7 C       North  Jan     300\n 8 C       North  Feb     320\n 9 C       South  Jan     350\n10 C       South  Feb     400\n\n\nThe missing values have been dropped, and the data is now in a form that can be analyzed by month, region, or product."
  },
  {
    "objectID": "posts/2024-09-19_pivot/index.html#importance-of-data-transformation-in-visualization",
    "href": "posts/2024-09-19_pivot/index.html#importance-of-data-transformation-in-visualization",
    "title": "Mastering Data Transformation in R with pivot_longer and pivot_wider",
    "section": "Importance of Data Transformation in Visualization",
    "text": "Importance of Data Transformation in Visualization\nOne of the most significant advantages of transforming data into a long format is the ease of visualizing it. Visualization libraries like ggplot2 in R often require data to be in long format for producing detailed and layered charts. For instance, the ability to map different variables to the aesthetics of a plot (such as color, size, or shape) is much simpler with long-format data.\nConsider the example of monthly sales data. When the data is in wide format, plotting each product’s sales across months can be cumbersome and limited. However, converting the data into long format allows us to easily generate visualizations that compare sales trends across products and months.\nHere’s an example bar plot illustrating the sales data in long format:\n\n# Gerekli paketleri yükle\nlibrary(tidyr)\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n# Veri setini oluştur\nsales_data &lt;- data.frame(\n  product = c(\"A\", \"B\", \"C\"),\n  Jan = c(500, 600, 300),\n  Feb = c(450, 700, 320),\n  Mar = c(520, 640, 310)\n)\n\n# Veriyi uzun formata dönüştür\nsales_long &lt;- pivot_longer(sales_data, cols = Jan:Mar, \n                           names_to = \"month\", values_to = \"sales\")\n\n# Çubuk grafiği oluştur\nggplot(sales_long, aes(x = month, y = sales, fill = product)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(title = \"Sales Data: Long Format Example\", x = \"Month\", y = \"Sales\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\nsales_data: A wide-format dataset containing the sales of products across different months.\npivot_longer(): Used to transform data from a wide format to a long format.\nggplot(): Used to create a bar plot. The aes() function specifies the axes and coloring (for different products).\ngeom_bar(): Draws the bar plot.\nlabs(): Adds titles and axis labels.\ntheme_minimal(): Applies a minimal theme.\nposition = \"dodge\": Draws the bars for products side by side.\n\nThe generated plot would illustrate how pivot_longer() facilitates better visualizations by organizing data in a manner that allows for flexible plotting.\nWhy Visualization Matters:\n\nClear Insights: Long format allows better representation of complex relationships.\nFlexible Aesthetics: With long format data, you can map multiple variables to visual properties (like color or size) more easily.\nLayering Data: Especially in time-series or categorical data, layering information through visual channels becomes more efficient with long data.\n\nWithout reshaping data, creating advanced visualizations for effective storytelling becomes challenging, making data transformation crucial in exploratory data analysis (EDA) and reporting."
  },
  {
    "objectID": "posts/2024-09-19_pivot/index.html#importance-in-data-science",
    "href": "posts/2024-09-19_pivot/index.html#importance-in-data-science",
    "title": "Mastering Data Transformation in R with pivot_longer and pivot_wider",
    "section": "Importance in Data Science",
    "text": "Importance in Data Science\nIn data science, the ability to reshape data is critical for exploratory data analysis (EDA), feature engineering, and model preparation. Many statistical models and machine learning algorithms expect data in long format, with each observation represented as a row. Converting between formats, especially in the cleaning and pre-processing phase, helps to avoid common errors in analysis, improves the quality of insights, and makes data manipulation more intuitive.\n\n\n\n\n\n\nAlternatives to pivot_longer() and pivot_wider()\n\n\n\nWhile pivot_longer() and pivot_wider() are part of the tidyr package and are widely used, there are alternative methods for reshaping data in R.\nHistorically, functions like gather() and spread() from the tidyr package were used for similar tasks before pivot_longer() and pivot_wider() became available. gather() was used to convert data from a wide format to a long format, while spread() was used to convert data from long to wide format. These functions laid the groundwork for the more flexible and consistent pivot_longer() and pivot_wider().\nIn addition to pivot_longer() and pivot_wider(), there are alternative methods for reshaping data in R. The reshape2 package offers melt() and dcast() functions as older but still functional alternatives for reshaping data. Base R also provides the reshape() function, which is more flexible but less intuitive compared to pivot_longer() and pivot_wider()."
  },
  {
    "objectID": "posts/2024-09-19_pivot/index.html#conclusion",
    "href": "posts/2024-09-19_pivot/index.html#conclusion",
    "title": "Mastering Data Transformation in R with pivot_longer and pivot_wider",
    "section": "Conclusion",
    "text": "Conclusion\nData transformation using pivot_longer() and pivot_wider() is fundamental in both everyday analysis and more advanced data science tasks. Choosing the correct data structure—whether wide or long—will optimize your workflow, whether you’re modeling, visualizing, or reporting.\nThe concept of tidy data, which emphasizes a consistent structure where each variable forms a column and each observation forms a row, is crucial in leveraging these functions effectively. By adhering to tidy data principles, you can ensure that your data is well-organized, making it easier to apply transformations and perform analyses. Through pivot_longer() and pivot_wider(), you gain flexibility in reshaping your data to meet the specific needs of your project, facilitating better data manipulation, visualization, and insight extraction.\nUnderstanding when and why to use these transformations, alongside maintaining tidy data practices, will enhance your ability to work with complex datasets and produce meaningful results."
  },
  {
    "objectID": "posts/2024-09-19_pivot/index.html#references",
    "href": "posts/2024-09-19_pivot/index.html#references",
    "title": "Mastering Data Transformation in R with pivot_longer and pivot_wider",
    "section": "References",
    "text": "References\n\nWickham, H. (2016). ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag.\nWickham, H. (2019). Advanced R. Chapman and Hall/CRC.\nWickham, H., Çetinkaya-Rundel, M., & Grolemund, G. (2023). R for data science (2nd ed.). O’Reilly Media."
  },
  {
    "objectID": "posts/2024-11-04_openxlsx/index.html#introduction",
    "href": "posts/2024-11-04_openxlsx/index.html#introduction",
    "title": "Creating Professional Excel Reports with R: A Comprehensive Guide to openxlsx Package",
    "section": "Introduction",
    "text": "Introduction\nThe ability to generate professional Excel reports programmatically is a crucial skill in data analysis and business reporting. In this comprehensive guide, we’ll explore how to use the openxlsx package in R to create sophisticated Excel reports with multiple sheets, custom formatting, and visualizations. This tutorial is designed for beginners to intermediate R users who want to automate their reporting workflows."
  },
  {
    "objectID": "posts/2024-11-04_openxlsx/index.html#why-choose-openxlsx",
    "href": "posts/2024-11-04_openxlsx/index.html#why-choose-openxlsx",
    "title": "Creating Professional Excel Reports with R: A Comprehensive Guide to openxlsx Package",
    "section": "Why Choose openxlsx?",
    "text": "Why Choose openxlsx?\n\nNo Excel Dependency: Unlike some alternatives, openxlsx doesn’t require Excel installation and No Java dependency (unlike XLConnect)\nPerformance: Efficient handling of large datasets\nComprehensive Formatting: Extensive options for cell styling, merging, and formatting\nMultiple Worksheets: Easy management of multiple sheets in a workbook\nCustom Styles: Ability to create and apply custom styles\nMemory Efficient: Better memory management compared to other packages\nActive Development: Regular updates and community support"
  },
  {
    "objectID": "posts/2024-11-04_openxlsx/index.html#getting-started",
    "href": "posts/2024-11-04_openxlsx/index.html#getting-started",
    "title": "Creating Professional Excel Reports with R: A Comprehensive Guide to openxlsx Package",
    "section": "Getting Started",
    "text": "Getting Started\nFirst, install and load the required packages:\n\n# Load packages\nlibrary(openxlsx)\nlibrary(dplyr)\nlibrary(ggplot2)"
  },
  {
    "objectID": "posts/2024-11-04_openxlsx/index.html#basic-functions-and-their-arguments",
    "href": "posts/2024-11-04_openxlsx/index.html#basic-functions-and-their-arguments",
    "title": "Creating Professional Excel Reports with R: A Comprehensive Guide to openxlsx Package",
    "section": "Basic Functions and Their Arguments",
    "text": "Basic Functions and Their Arguments\n\nCore Functions\ncreateWorkbook()\nThe createWorkbook() function is just the starting point and creates a new workbook object. When you run wb &lt;- createWorkbook(), you are creating a new, empty workbook object and assigning it to the variable wb. This workbook will serve as the container for any worksheets, styles, and data you want to add before saving it as an Excel file.\n\nwb &lt;- createWorkbook()\n\naddWorksheet()\nThe addWorksheet() function, part of the openxlsx package in R, is used to add a new worksheet (tab) to an Excel workbook created with createWorkbook().\nKey arguments:\n\nwb: This is the workbook object to which you’re adding a new worksheet. It should be an existing workbook created with createWorkbook().\nsheetName = \"Sales Report\": This argument specifies the name of the new worksheet. In this case, the sheet will be labeled “Sales Report.” The name you choose will appear as the worksheet tab name in the Excel file.\ngridLines = TRUE: This argument controls whether gridlines are visible in the worksheet.\n\nTRUE: Shows gridlines (default setting).\nFALSE: Hides gridlines, which can create a cleaner look in some reports.\n\n\n\naddWorksheet(wb, sheetName = \"Sales Report\", gridLines = TRUE)\n\nwriteData()\nThe writeData() function from the openxlsx package in R is used to add data to a specific worksheet in an Excel workbook. Here’s what each argument in your code does:\n\nwb: This is the workbook object where you want to write data. The workbook should already be created using createWorkbook().\nsheet = 1: This specifies the sheet to which you’re writing data. Here, 1 refers to the first sheet in the workbook. You can also use the sheet’s name (e.g., sheet = \"Sales Report\") if you prefer.\nx = data: This is the data you want to write to the worksheet. data can be a data frame, matrix, or vector.\nstartRow = 1: This specifies the row in the worksheet where the data should start. In this case, data will be written beginning at the first row.\nstartCol = 1: This specifies the column where the data should start. Setting this to 1 will write data starting from the first column (column “A” in Excel).\n\n\nwriteData(wb, sheet = 1, x = data, startRow = 1, startCol = 1)"
  },
  {
    "objectID": "posts/2024-11-04_openxlsx/index.html#step-by-step-report-creation",
    "href": "posts/2024-11-04_openxlsx/index.html#step-by-step-report-creation",
    "title": "Creating Professional Excel Reports with R: A Comprehensive Guide to openxlsx Package",
    "section": "Step-by-Step Report Creation",
    "text": "Step-by-Step Report Creation\nLet’s create a sample sales report with multiple sheets, formatting, and charts.\n\nStep 1: Prepare Sample Data\n\n# Create sample sales data\nset.seed(123)\nsales_data &lt;- data.frame(\n  Date = seq.Date(as.Date(\"2023-01-01\"), as.Date(\"2023-12-31\"), by = \"month\"),\n  Region = rep(c(\"North\", \"South\", \"East\", \"West\"), 3),\n  Sales = round(runif(12, 10000, 50000), 2),\n  Units = round(runif(12, 100, 500)),\n  Profit = round(runif(12, 5000, 25000), 2)\n)\n\nsales_data\n\n         Date Region    Sales Units   Profit\n1  2023-01-01  North 21503.10   371 18114.12\n2  2023-02-01  South 41532.21   329 19170.61\n3  2023-03-01   East 26359.08   141 15881.32\n4  2023-04-01   West 45320.70   460 16882.84\n5  2023-05-01  North 47618.69   198 10783.19\n6  2023-06-01  South 11822.26   117  7942.27\n7  2023-07-01   East 31124.22   231 24260.48\n8  2023-08-01   West 45696.76   482 23045.98\n9  2023-09-01  North 32057.40   456 18814.11\n10 2023-10-01  South 28264.59   377 20909.35\n11 2023-11-01   East 48273.33   356  5492.27\n12 2023-12-01   West 28133.37   498 14555.92\n\n\n\nset.seed(123): This sets the random seed to ensure that any randomly generated numbers in the code are reproducible. This is useful if you want to get the same “random” values each time you run the code.\nsales_data &lt;- data.frame(...): This creates a data frame called sales_data to store the sample sales data. A data frame is a table-like structure in R, suitable for storing datasets.\nDate = seq.Date(...): seq.Date() generates a sequence of dates from January 1, 2023, to December 31, 2023, with one date per month.\n\nas.Date(\"2023-01-01\") and as.Date(\"2023-12-31\") define the start and end dates for the sequence.\nby = \"month\" specifies that the sequence should increment by one month at a time, creating 12 monthly date entries.\n\nRegion = rep(c(\"North\", \"South\", \"East\", \"West\"), 3): rep(c(\"North\", \"South\", \"East\", \"West\"), 3) repeats the four regions (“North”, “South”, “East”, “West”) three times to get a total of 12 values. This column will indicate which region each data entry corresponds to.\nSales = round(runif(12, 10000, 50000), 2):\n\nrunif(12, 10000, 50000) generates 12 random numbers between 10,000 and 50,000, representing the monthly sales figures.\nround(..., 2) rounds these sales figures to two decimal places for readability.\n\nUnits = round(runif(12, 100, 500)):\n\nrunif(12, 100, 500) generates 12 random integers between 100 and 500, representing the number of units sold each month.\nround() rounds these values to the nearest whole number.\n\nProfit = round(runif(12, 5000, 25000), 2):\n\nrunif(12, 5000, 25000) generates 12 random numbers between 5,000 and 25,000, representing monthly profit values.\nround(..., 2) rounds each profit value to two decimal places.\n\n\n\n\nStep 2: Create Workbook and Add Sheets\nFollowing code creates an Excel workbook and prepares it with several worksheets and customized styles for titles and headers. Let’s walk through each part.\n\n# Create new workbook\nwb &lt;- createWorkbook()\n\nThis line initializes a new workbook object (wb) where you’ll add worksheets and data. The workbook is created using createWorkbook() from the openxlsx package.\n\n# Add worksheets\naddWorksheet(wb, \"Summary\")\naddWorksheet(wb, \"Details\")\naddWorksheet(wb, \"Charts\")\n\nThese lines add three worksheets to the workbook, named “Summary,” “Details,” and “Charts.” Each worksheet will be a separate tab in the Excel file.\n\n# Create a title style\ntitle_style &lt;- createStyle(\n  fontSize = 14,\n  fontColour = \"#FFFFFF\",\n  halign = \"center\",\n  fgFill = \"#4F81BD\",\n  textDecoration = \"bold\",\n  border = \"TopBottom\",\n  borderColour = \"#4F81BD\"\n)\n\n\ncreateStyle(): This function defines a custom style that you can apply to specific cells in the workbook. The style here is designed for titles and is stored in title_style.\n\n\nArguments in createStyle() for the Title:\n\nfontSize = 14: Sets the font size to 14 for better visibility of the title.\nfontColour = \"#FFFFFF\": Sets the font color to white, using a hexadecimal color code.\nhalign = \"center\": Horizontally aligns the text to the center within the cell.\nfgFill = \"#4F81BD\": Sets the background fill color (foreground color) of the cell to a shade of blue (#4F81BD).\ntextDecoration = \"bold\": Makes the text bold to emphasize it as a title.\nborder = \"TopBottom\": Adds borders to the top and bottom of the cell to give the title a framed appearance.\nborderColour = \"#4F81BD\": Sets the color of the borders to match the blue fill color.\n\n\n# Create header style\nheader_style &lt;- createStyle(\n  fontSize = 12,\n  fontColour = \"#000000\",\n  halign = \"center\",\n  fgFill = \"#DCE6F1\",\n  textDecoration = \"bold\",\n  border = \"bottom\",\n  borderColour = \"#4F81BD\"\n)\n\n\nThis style is designed for headers in the worksheets, stored in header_style.\n\n\n\nArguments in createStyle() for the Header:\n\nfontSize = 12: Sets a slightly smaller font size than the title.\nfontColour = \"#000000\": Sets the font color to black.\nhalign = \"center\": Centers the text within each cell.\nfgFill = \"#DCE6F1\": Sets a light blue background fill for the header cells to distinguish them visually.\ntextDecoration = \"bold\": Makes the header text bold.\nborder = \"bottom\": Adds a border to the bottom of the cell.\nborderColour = \"#4F81BD\": Sets the color of the bottom border to the same blue as in the title style.\n\n\n\n\nStep 3: Add Summary Data and Formatting\nThis code adds a formatted title and data summary to the “Summary” worksheet in an Excel workbook, then applies styling to headers and numeric data, and adjusts column widths for a polished appearance. Let’s go through each section.\n\n# Write title\nwriteData(wb, \"Summary\", \"Sales Performance Report 2023\", startCol = 1, startRow = 1)\nmergeCells(wb, \"Summary\", cols = 1:5, rows = 1)\naddStyle(wb, \"Summary\", title_style, rows = 1, cols = 1:5)\n\n\nwriteData(wb, \"Summary\", \"Sales Performance Report 2023\", startCol = 1, startRow = 1): This places the text \"Sales Performance Report 2023\" in cell A1 of the “Summary” worksheet.\nmergeCells(wb, \"Summary\", cols = 1:5, rows = 1): Merges cells from columns 1 to 5 (A to E) in the first row, centering the title across these columns to make it look like a unified title.\naddStyle(wb, \"Summary\", title_style, rows = 1, cols = 1:5): Applies the previously defined title_style to the merged title cell. This style includes formatting like font size, color, alignment, and borders, giving the title a professional appearance.\n\n\n# Write data with headers\nwriteData(wb, \"Summary\", sales_data, startCol = 1, startRow = 3)\naddStyle(wb, \"Summary\", header_style, rows = 3, cols = 1:5)\n\n\nwriteData(wb, \"Summary\", sales_data, startCol = 1, startRow = 3): Writes the sales_data data frame starting from cell A3. Row 3 will contain the headers from sales_data, while the rows below will contain the data.\naddStyle(wb, \"Summary\", header_style, rows = 3, cols = 1:5): Applies the header_style to row 3 (columns A to E) to make the headers bold, centered, and colored with a background fill. This improves readability and distinguishes the headers from the data.\n\n\n# Format numbers\nnumber_style &lt;- createStyle(numFmt = \"#,##0.00\")\naddStyle(wb, \"Summary\", number_style, rows = 4:15, cols = 3:5, gridExpand = TRUE)\n\n\nnumber_style &lt;- createStyle(numFmt = \"#,##0.00\"): Defines a style named number_style that formats numbers with commas as thousands separators and two decimal places (e.g., 12,345.67).\naddStyle(wb, \"Summary\", number_style, rows = 4:15, cols = 3:5, gridExpand = TRUE):\n\nApplies this number_style to columns 3 through 5 (Sales, Units, and Profit columns in sales_data) for rows 4 to 15, covering all data rows.\ngridExpand = TRUE ensures the style applies to the entire specified range, not just the first cell in each row or column.\n\n\n\n# Adjust column widths\nsetColWidths(wb, \"Summary\", cols = 1:5, widths = \"auto\")\n\nsetColWidths(wb, \"Summary\", cols = 1:5, widths = \"auto\"): Automatically adjusts the widths of columns 1 through 5 (A to E) based on their content. This ensures that all data, headers, and titles are fully visible without manual adjustment.\n\n\nStep 4: Create and Add Visualizations\nThis code creates a line chart to visualize monthly sales trends and inserts it into an Excel workbook. Here’s a step-by-step explanation of each part.\n\n# Create monthly sales trend chart\nsales_plot &lt;- ggplot(sales_data, aes(x = Date, y = Sales)) +\n  geom_line(color = \"#4F81BD\", size = 1.2) +\n  geom_point(color = \"#4F81BD\", size = 3) +\n  theme_minimal() +\n  labs(title = \"Monthly Sales Trend\",\n       x = \"Month\",\n       y = \"Sales ($)\") +\n  theme(plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"))\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n# Save plot to workbook\ninsertPlot(wb, \"Charts\", width = 8, height = 6,\n          startCol = 1, startRow = 1)\n\ninsertPlot() is an openxlsx function that saves the current plot into a specified worksheet in an Excel workbook.\n\nwb: Specifies the workbook to insert the plot into.\n\"Charts\": Specifies the worksheet where the plot will be placed.\nwidth = 8, height = 6: Sets the width and height of the plot in inches.\nstartCol = 1, startRow = 1: Inserts the plot starting at cell A1 of the “Charts” worksheet.\n\n\n\nStep 5: Add Regional Analysis\nThen let’s create a summary of sales data by region, writes it to the “Details” worksheet in an Excel workbook, and applies styling for a professional presentation.\n\n# Create regional summary\nregional_summary &lt;- sales_data %&gt;%\n  group_by(Region) %&gt;%\n  summarise(\n    Total_Sales = sum(Sales),\n    Avg_Units = mean(Units),\n    Total_Profit = sum(Profit)\n  )\n\n# Write regional summary to Details sheet\nwriteData(wb, \"Details\", \"Regional Performance Summary\", startCol = 1, startRow = 1)\nmergeCells(wb, \"Details\", cols = 1:4, rows = 1)\naddStyle(wb, \"Details\", title_style, rows = 1, cols = 1:4)\n\nwriteData(wb, \"Details\", regional_summary, startCol = 1, startRow = 3)\naddStyle(wb, \"Details\", header_style, rows = 3, cols = 1:4)\n\n\n\nStep 6: Save the Workbook\nLastly with this command finalizes and exports the workbook, preserving all worksheets, data, formatting, and charts created in previous steps. You should see a file named Sales_Report_2023.xlsx in your working directory after this line runs.\n\n# Save the workbook\nsaveWorkbook(wb, \"Sales_Report_2023.xlsx\", overwrite = TRUE)\n\nAfter saving the Excel file with the Summary, Details, and Charts sheets, I opened the file to review the output. Below, I’m sharing screenshots of each sheet to showcase the final report layout, formatting, and visualization.\nIn the Summary sheet, you can see the main title, followed by a detailed table with the monthly sales data. The headers and values are formatted to improve readability and create a professional appearance.\n\n\n\n\n\nThe Details sheet provides a regional breakdown with aggregated sales, average units, and profit for each region. This sheet includes formatted headers and a clear, centered title, making it easy to interpret the regional performance metrics.\n\n\n\n\n\nLastly, the Charts sheet contains a line graph displaying the monthly sales trend. This visualization is useful for spotting sales patterns and seeing how performance changes over the months.\n\n\n\n\n\nThese screenshots illustrate the powerful formatting and customization options available when generating Excel reports in R, making it straightforward to create polished and informative workbooks for reporting."
  },
  {
    "objectID": "posts/2024-11-04_openxlsx/index.html#best-practices-and-tips-for-using-the-openxlsx-package-in-r",
    "href": "posts/2024-11-04_openxlsx/index.html#best-practices-and-tips-for-using-the-openxlsx-package-in-r",
    "title": "Creating Professional Excel Reports with R: A Comprehensive Guide to openxlsx Package",
    "section": "Best Practices and Tips for Using the openxlsx Package in R",
    "text": "Best Practices and Tips for Using the openxlsx Package in R\n\nUse Meaningful Sheet Names\nChoose descriptive and relevant names for your Excel sheets. This helps users understand the content at a glance and enhances navigation within the workbook. For example, instead of generic names like “Sheet1,” use names like “SalesData_Q1” or “CustomerFeedback.”\nImplement Consistent Styling Across Sheets\nMaintain a uniform style throughout your workbook to enhance readability and professionalism. Use consistent fonts, colors, and cell styles. You can set styles using the createStyle() function and apply them to multiple sheets to ensure uniformity.\nInclude Proper Documentation in Your Code\nDocument your R code with clear comments explaining the purpose of each section and any specific styling or formatting choices made with the openxlsx functions. This will make your code easier to understand and maintain, especially for others who may work with it later.\nUse Appropriate Number Formatting for Different Data Types\nApply relevant number formats for various data types, such as currency, percentages, or dates. Utilize the addStyle() function to format cells appropriately, which improves data clarity and presentation in your reports.\nTest the Report with Different Data Sizes\nBefore finalizing your report, test it with datasets of varying sizes to ensure it renders correctly and performs well. This will help you identify any potential issues, such as layout problems or performance slowdowns, before distribution.\nInclude Error Handling for Robust Reports\nImplement error handling in your R code to gracefully manage potential issues, such as missing data or formatting errors. Use tryCatch() to catch errors during report generation, ensuring that your report generation process is robust and user-friendly."
  },
  {
    "objectID": "posts/2024-11-04_openxlsx/index.html#conclusion",
    "href": "posts/2024-11-04_openxlsx/index.html#conclusion",
    "title": "Creating Professional Excel Reports with R: A Comprehensive Guide to openxlsx Package",
    "section": "Conclusion",
    "text": "Conclusion\nThe openxlsx package is a powerful and flexible tool for generating professional Excel reports directly from R. By leveraging its capabilities, you can create sophisticated reports that include multiple sheets, tailored formatting, and integrated visualizations. This package allows for extensive customization, enabling you to apply styles, set column widths, and format numbers to meet your specific requirements.\nAs you create your reports, take advantage of features such as conditional formatting, data validation, and the ability to add hyperlinks. These functionalities can enhance the interactivity and usability of your reports, making them not only visually appealing but also more functional.\nDon’t hesitate to experiment with various formatting options, as openxlsx offers a range of functions to help you manipulate the appearance of your sheets. Adapting the code to fit your reporting needs is crucial; consider how you can automate repetitive tasks or incorporate dynamic elements that reflect changes in your data.\nAdditionally, always keep performance in mind—testing your reports with datasets of varying sizes will ensure that they function smoothly and remain responsive, regardless of the data complexity. Finally, robust error handling will help you create reliable reports that can withstand unexpected data issues, thereby enhancing the user experience.\nBy following the best practices outlined in this guide, you will be well-equipped to utilize the openxlsx package to its fullest potential, producing high-quality, professional reports that effectively communicate your insights and findings."
  },
  {
    "objectID": "posts/2024-11-04_openxlsx/index.html#about-openxlsx2-package",
    "href": "posts/2024-11-04_openxlsx/index.html#about-openxlsx2-package",
    "title": "Creating Professional Excel Reports with R: A Comprehensive Guide to openxlsx Package",
    "section": "About openxlsx2 Package",
    "text": "About openxlsx2 Package\nWhile openxlsx is a powerful package for Excel reporting, its successor, openxlsx2, brings significant enhancements and additional features:\n\nImproved Performance:\nopenxlsx2 is optimized for speed and efficiency, making it faster when handling large datasets or generating complex Excel files.\nEnhanced Compatibility:\nThe package offers better compatibility with modern Excel formats and supports advanced features such as conditional formatting and improved table styles.\nSimplified Syntax:\nFunctions in openxlsx2 have been refined for easier use, with clearer argument names and enhanced documentation.\nBackward Compatibility:\nopenxlsx2 maintains most of the functionality of openxlsx, allowing users to transition seamlessly while benefiting from the new features.\n\nFor users who require advanced functionality or improved performance, openxlsx2 is an excellent alternative. You can explore the package and its documentation on CRAN and github."
  },
  {
    "objectID": "posts/2024-11-04_openxlsx/index.html#references",
    "href": "posts/2024-11-04_openxlsx/index.html#references",
    "title": "Creating Professional Excel Reports with R: A Comprehensive Guide to openxlsx Package",
    "section": "References",
    "text": "References\n\nopenxlsx GitHub Repository\nExplore the source code, issues, and development updates for the openxlsx package. Available at: openxlsx GitHubRepository\nopenxlsx Documentation\nAccess the official documentation for detailed information on functions, usage, and examples for the openxlsx package. Available at: openxlsx Documentation\nCRAN Package Page\nFind installation instructions, news, and package information from the Comprehensive R Archive Network (CRAN). Available at: openxlsx CRAN Page"
  },
  {
    "objectID": "posts/2024-12-31_cbrt/index.html",
    "href": "posts/2024-12-31_cbrt/index.html",
    "title": "Unlocking CBRT Data in R: A Guide to the CBRT R Package",
    "section": "",
    "text": "The Central Bank of the Republic of Turkey (CBRT) provides a wealth of economic data crucial for researchers, analysts, and policymakers. Through the Electronic Data Delivery System (EVDS ), users can access time-series data on various economic indicators. With the CBRT R package this process becomes streamlined, empowering users to integrate CBRT data directly into their R workflows. This blog post delves into the details of accessing CBRT data using the package, explaining everything from obtaining an API key to practical examples of retrieving economic series."
  },
  {
    "objectID": "posts/2024-12-31_cbrt/index.html#introduction",
    "href": "posts/2024-12-31_cbrt/index.html#introduction",
    "title": "Unlocking CBRT Data in R: A Guide to the CBRT R Package",
    "section": "Introduction",
    "text": "Introduction\nThe CBRT serves as Turkey’s central bank, tasked with implementing monetary policies and maintaining financial stability. The EVDS (Elektronik Veri Dağıtım Sistemi) is the CBRT’s online data delivery platform, providing access to a vast repository of economic data, including price indices, exchange rates, monetary aggregates, and more. EVDS supports API-based data retrieval, allowing programmatic access to its datasets."
  },
  {
    "objectID": "posts/2024-12-31_cbrt/index.html#evds",
    "href": "posts/2024-12-31_cbrt/index.html#evds",
    "title": "Unlocking CBRT Data in R: A Guide to the CBRT R Package",
    "section": "EVDS",
    "text": "EVDS\nThe Electronic Data Delivery System (EVDS) is a dynamic and interactive system that presents statistical time series data produced by the CBRT and/or data produced by other institutions and compiled by the CBRT. These data are published on dynamic web pages. They can also be reported in the xls format or through the web service client (json, csv, xml), viewed in the graphics format, and received via e-mail by subscribing to the system. The EVDS was first introduced in 1995 and is available in Turkish and English.\nThe system provides a rich range of economic data and information to support economic education and foster economic research. Its technical infrastructure was revised in October 2017. The EVDS serves the public with its new facilities and content such as the REST web service, Customization, Reports, Interactive Charts, Frequently Used Data Groups, Recently Updated Data Groups, and data displayed on Turkey and world maps."
  },
  {
    "objectID": "posts/2024-12-31_cbrt/index.html#setting-up-access-the-api-key",
    "href": "posts/2024-12-31_cbrt/index.html#setting-up-access-the-api-key",
    "title": "Unlocking CBRT Data in R: A Guide to the CBRT R Package",
    "section": "Setting Up Access: The API Key",
    "text": "Setting Up Access: The API Key\nTo access EVDS data programmatically, you need an API key, which serves as a unique identifier for authenticating your requests.\n\nRequesting an API Key:\nVisit EVDS and create an account. Once logged in, navigate to the API access section to generate your personal API key.\nStoring Your API Key Securely:\nAvoid hardcoding your API key in scripts. Instead, save it in a .txt file and read it into your R session. For example:\n\n\napi_key &lt;- readLines(\"path/to/your_api_key.txt\")"
  },
  {
    "objectID": "posts/2024-12-31_cbrt/index.html#cbrt-package",
    "href": "posts/2024-12-31_cbrt/index.html#cbrt-package",
    "title": "Unlocking CBRT Data in R: A Guide to the CBRT R Package",
    "section": "CBRT Package",
    "text": "CBRT Package\nThe CBRT R package, developed by Prof. Dr. Erol Taymaz from Middle East Technical University, is a powerful tool designed to simplify data retrieval from the Central Bank of the Republic of Turkey’s (CBRT) Electronic Data Delivery System (EVDS). This package enables users to efficiently access and analyze economic indicators by providing functions for querying data series, retrieving metadata, and searching for relevant datasets through the EVDS API. he CBRT package includes functions for finding, and downloading data from the Central Bank of the Republic of Türkiye’s database. The CBRT database covers more than 40,000 time series variables. For detailed documentation and further insights into the package, you can visit this link.\nThe package is now available at CRAN (November 13, 2024), and can be installed by\n\ninstall.packages(\"CBRT\")"
  },
  {
    "objectID": "posts/2024-12-31_cbrt/index.html#core-functions",
    "href": "posts/2024-12-31_cbrt/index.html#core-functions",
    "title": "Unlocking CBRT Data in R: A Guide to the CBRT R Package",
    "section": "Core Functions",
    "text": "Core Functions\nAll data series (variables) are classified into data groups, and data groups into data categories. There are 44 data categories (including the archieved ones), 499 data groups, and 40,826 data series.\n\ngetAllCategoriesInfo\nThe getAllCategoriesInfo function in the CBRT R package provides a convenient way to access information about the main data categories available in the Central Bank of the Republic of Türkiye’s (CBRT) Electronic Data Delivery System (EVDS). This function requires a valid API key as an argument to authenticate your request. By retrieving a structured list of these categories, users can explore the high-level organization of economic data offered by the EVDS API.\n\nlibrary(CBRT)\nmy_api_key &lt;- readLines(\"D:/evds_api_key.txt\",warn=FALSE)\n\nCategories &lt;- getAllCategoriesInfo(CBRTKey = my_api_key)\nhead(Categories)\n\n   cid                                    topic\n1:   1                       MARKET DATA (CBRT)\n2:   2                    EXCHANGE RATES (CBRT)\n3:   3          INTEREST RATE STATISTICS (CBRT)\n4:   4 MONTHLY MONEY AND BANK STATISTICS (CBRT)\n5:   5             SECURITIES STATISTICS (CBRT)\n6:  12         FINANCIAL SERVICES SURVEY (CBRT)\n\n\n\n\ngetAllGroupsInfo\nThe CBRT R package offers the getAllGroupsInfo function, which allows users to access detailed information about the groups within specific categories in the Central Bank of the Republic of Turkey’s (CBRT) Electronic Data Delivery System (EVDS). Similar to getAllCategoriesInfo, this function requires a valid API key for authentication. The groups represent subcategories or finer classifications of data within the broader main categories. By leveraging the cid (category ID) variable from the categories table, users can establish a relationship between categories and their corresponding groups. This functionality provides a structured approach to exploring the hierarchy of economic data in EVDS, enabling users to efficiently navigate and identify the datasets most relevant to their research or analysis.\n\nGroups &lt;- getAllGroupsInfo(CBRTKey = my_api_key)\nhead(Groups)\n\n   cid    groupCode\n1:   1   bie_pyrepo\n2:   0   bie_mkbral\n3:  25 bie_mkaltytl\n4:   1   bie_ppibsm\n5:   1 bie_pyintbnk\n6:   1    bie_tldov\n                                                                              groupName\n1:                                       Open Market Repo and Reverse Repo Transactions\n2:                                           Istanbul Gold Exchange (TRY_USD) (Archive)\n3:                                           Gold Prices (Averaged) - Free Market (TRY)\n4: Free Deposits of Banks and Total Liquidity (Beginning of the Day)(Million TRY)(CBRT)\n5:                      Interbank Money Market Transactions Summary (TRY Thousand or %)\n6:     Total Volume of FX Transaction of Banks Against Turkish Lira (Million USD, CBRT)\n   freq         source sourceLink revisionPolicy appLink  firstDate   lastDate\n1:    5           CBRT                                   01-08-2020 01-05-1989\n2:    2 BORSA ISTANBUL                                   29-06-2018 27-07-1995\n3:    5         Market                                   01-02-2025 01-12-1950\n4:    2           CBRT                                   07-03-2025 04-01-2007\n5:    2           CBRT                                   06-03-2025 04-11-1996\n6:    2           CBRT                                   27-02-2025 14-10-2002\n\n\nAdditionally, the groups table contains valuable metadata, including the date ranges for available data, data frequency, and data sources. The frequency of the data is indicated by predefined frequency codes:\n\nDaily\nWorkday\nWeekly\nBiweekly\nMonthly\nQuarterly\nSemiannual\nAnnual\n\n\n\ngetAllSeriesInfo\nThe getAllSeriesInfo function in the CBRT R package enables users to retrieve up-to-date metadata for data series available in the Central Bank of the Republic of Turkey’s (CBRT) Electronic Data Delivery System (EVDS). This function, like others in the package, requires a valid API key for authentication. The metadata includes essential details such as group codes, series names, and other relevant information about the datasets within a chosen topic. These details help users identify and filter specific series of interest. Furthermore, by utilizing key variables, the series metadata can be linked to the categories and groups tables, allowing users to establish relationships across the data hierarchy. This capability ensures a structured and interconnected exploration of economic datasets, simplifying the process of locating and analyzing relevant data for research or analysis.\n\nSeries &lt;- getAllSeriesInfo(CBRTKey = my_api_key)\n\n\nhead(Series)\n\n   cid                                                                topic\n1:   7 DEPOSITS AND PARTICIPATION FUNDS SUBJECT TO REQUIRED RESERVES (CBRT)\n2:   7 DEPOSITS AND PARTICIPATION FUNDS SUBJECT TO REQUIRED RESERVES (CBRT)\n3:   7 DEPOSITS AND PARTICIPATION FUNDS SUBJECT TO REQUIRED RESERVES (CBRT)\n4:   7 DEPOSITS AND PARTICIPATION FUNDS SUBJECT TO REQUIRED RESERVES (CBRT)\n5:   7 DEPOSITS AND PARTICIPATION FUNDS SUBJECT TO REQUIRED RESERVES (CBRT)\n6:   7 DEPOSITS AND PARTICIPATION FUNDS SUBJECT TO REQUIRED RESERVES (CBRT)\n       groupCode\n1: bie_KYBKATFON\n2: bie_KYBKATFON\n3: bie_KYBKATFON\n4: bie_KYBKATFON\n5: bie_KYBKATFON\n6: bie_KYBKATFON\n                                                          groupName freq\n1: Breakdown of Participation Funds Subject to Reserve Requirements    3\n2: Breakdown of Participation Funds Subject to Reserve Requirements    3\n3: Breakdown of Participation Funds Subject to Reserve Requirements    3\n4: Breakdown of Participation Funds Subject to Reserve Requirements    3\n5: Breakdown of Participation Funds Subject to Reserve Requirements    3\n6: Breakdown of Participation Funds Subject to Reserve Requirements    3\n         seriesCode                          seriesName      start        end\n1: TP.KYBKATFON.KB1  Turkish Lira_Turkish Lira Accounts 10-05-2002 13-12-2024\n2: TP.KYBKATFON.KB2                   Fx Accounts (USD) 10-05-2002 13-12-2024\n3: TP.KYBKATFON.KB3                  Fx Accounts (EURO) 10-05-2002 13-12-2024\n4: TP.KYBKATFON.KB4 Fx Accounts Precious Minerals (USD) 14-10-2011 13-12-2024\n5: TP.KYBKATFON.KB5             Fx Accounts Other (USD) 10-05-2002 13-12-2024\n6: TP.KYBKATFON.KB6             Total Fx Accounts (USD) 10-05-2002 13-12-2024\n   aggMethod freqname  tag\n1:      last     Week &lt;NA&gt;\n2:      last     Week &lt;NA&gt;\n3:      last     Week &lt;NA&gt;\n4:      last     Week &lt;NA&gt;\n5:      last     Week &lt;NA&gt;\n6:      last     Week &lt;NA&gt;\n\n\n\n\nsearchCBRT\nThe searchCBRT function in the CBRT R package provides a powerful tool for searching any category, group, or series name within the Central Bank of the Republic of Turkey’s (CBRT) Electronic Data Delivery System (EVDS). By specifying keywords and the desired field to search in, users can efficiently locate relevant datasets. This function simplifies the process of finding specific information within the extensive EVDS repository, enabling direct access to the desired table or dataset. Whether searching for broad topics, specific groups, or individual data series, searchCBRT offers a flexible and efficient way to navigate the system and pinpoint the data needed for analysis.\nSuppose we want to find datasets related to “Consumer Prices” within the EVDS system. Using the searchCBRT function, we can search for this keyword in relevant fields to locate the desired tables or series. Here’s how to do it:\n\nsearchCBRT(\"consumer price\", field = \"series\")\n\n         seriesCode\n1:  TP.ENFBEK.TEA12\n2: TP.ENFBEK.TEA345\n3:     TP.FE.OKTG01\n4:        TP.FG.A09\n5:        TP.FG.A10\n6:       TP.TG2.Y14\n7:       TP.TG2.Y15\n8:        TP.FG.F19\n9:        TP.FG.F20\n                                                                                                         seriesName\n1:              Percentage of households expecting consumer prices to increase more rapidly or at the same rate (%)\n2: Percentage of households expecting consumer prices to stay about the same, fall or increase at a slower rate (%)\n3:                                                                                             Consumer Price Index\n4:                                                                        Consumer Prices Index of Ankara (Archive)\n5:                                                                      Consumer Prices Index of Istanbul (Archive)\n6:                                              Assessment on Consumer prices change rate (over the last 12 months)\n7:             Expectation for consumer prices change rate (over the next 12 months compared to the past 12 months)\n8:                                                                            Ankara Consumer Price Index (Archive)\n9:                                                                          Istanbul Consumer Price Index (Archive)\n       groupCode\n1:    bie_enfbek\n2:    bie_enfbek\n3:    bie_feoktg\n4: bie_fgtukfiy2\n5: bie_fgtukfiy2\n6:   bie_mbgven2\n7:   bie_mbgven2\n8:   bie_tukfiy1\n9:   bie_tukfiy1\n                                                                                                   groupName\n1:                                                                           Sectoral Inflation Expectations\n2:                                                                           Sectoral Inflation Expectations\n3: TURKSTAT- Consumer Price Index-Indicators for the CPI's Having Specified Coverages (2003=100)(New Series)\n4:                                                      Consumer Price Index (1987=100) (TURKSTAT) (Archive)\n5:                                                      Consumer Price Index (1987=100) (TURKSTAT) (Archive)\n6:     Seasonally unadjusted Consumer Confidence Index and Indices of Consumer Tendency Survey Questions (*)\n7:     Seasonally unadjusted Consumer Confidence Index and Indices of Consumer Tendency Survey Questions (*)\n8:                                                 Consumer Price Index (1978_1979=100) (TURKSTAT) (Archive)\n9:                                                 Consumer Price Index (1978_1979=100) (TURKSTAT) (Archive)\n\n\n\n\ngetDataSeries\nThe getDataSeries function in the CBRT R package is a versatile tool for importing one or more time series directly from the EVDS. This function provides users with several advanced features to customize their data retrieval. For example, users can specify the frequency level (freq), such as daily, weekly, or monthly, and set a date range using the startDate and endDate arguments in the format DD-MM-YYYY. If the endDate is not specified, the function automatically retrieves data up to the latest available point.\nAn additional feature of getDataSeries is its ability to aggregate higher-frequency data into lower-frequency formats using the aggType argument. Supported aggregation methods include:\n\navg: Average value,\nfirst: First observation,\nlast: Last observation,\nmax: Maximum value,\nmin: Minimum value,\nsum: Summation of values.\n\nFor instance, if weekly data is aggregated to a monthly frequency, the aggregation method is applied to compute the resulting values. Furthermore, the na.rm argument allows users to drop all missing dates, ensuring clean and continuous time series data.\nHere’s an example demonstrating its use:\n\n# Import a time series (e.g., CPI data) with specific parameters\ncpi_data &lt;- getDataSeries(\n  series = c(\"TP.FE.OKTG01\"),       # Example series ID\n  CBRTKey = my_api_key,            # Your API key\n  freq = 5,                     # Monthly frequency\n  startDate = \"01-01-2010\",     # Start date\n  endDate = \"31-12-2023\",       # End date\n  na.rm = TRUE                  # Remove missing dates\n)\n\n# View the imported data\nhead(cpi_data)\n\n         time TP.FE.OKTG01\n1: 2010-01-15       174.07\n2: 2010-02-15       176.59\n3: 2010-03-15       177.62\n4: 2010-04-15       178.68\n5: 2010-05-15       178.04\n6: 2010-06-15       177.04\n\n\nFor example, we want to fetch exchange rates for USD, EUR, and GBP against the Turkish Lira (TRY) for a specific time period in monthly frequency.\n\n# Define the series IDs for USD, EUR, and GBP (Sales rate against TRY)\nusd_series &lt;- \"TP.DK.USD.S\"\neur_series &lt;- \"TP.DK.EUR.S\"\ngbp_series &lt;- \"TP.DK.GBP.S\"\n\n# Define the frequency method\nfreq &lt;- 5  # Monthly frequency\n\n# Define the date range for the data (e.g., from 01-01-2020 to 31-12-2024)\nstartDate &lt;- \"01-01-2020\"\nendDate &lt;- \"31-12-2024\"\n\n# Fetch the data for USD, EUR, and GBP exchange rates\nexchange_data &lt;- getDataSeries(\n  series = c(usd_series,eur_series,gbp_series),\n  CBRTKey = my_api_key,\n  freq = freq,\n  startDate = startDate,\n  endDate = endDate,\n  na.rm = TRUE\n)\n\nhead(exchange_data)\n\n         time TP.DK.USD.S TP.DK.EUR.S TP.DK.GBP.S\n1: 2020-01-15    5.928827    6.586905    7.763218\n2: 2020-02-15    6.055370    6.605785    7.872095\n3: 2020-03-15    6.325805    7.001341    7.858764\n4: 2020-04-15    6.831252    7.430133    8.493257\n5: 2020-05-15    6.964488    7.573124    8.588112\n6: 2020-06-15    6.821091    7.676245    8.560195"
  },
  {
    "objectID": "posts/2024-12-31_cbrt/index.html#conclusion",
    "href": "posts/2024-12-31_cbrt/index.html#conclusion",
    "title": "Unlocking CBRT Data in R: A Guide to the CBRT R Package",
    "section": "Conclusion",
    "text": "Conclusion\nThe CBRT R package is a powerful tool for accessing and analyzing Turkish economic data. By combining the package’s functionality with R’s robust analytical tools, users can unlock insights and streamline their research. Whether you’re tracking inflation trends, analyzing monetary policy impacts, or studying exchange rates, the CBRT package offers a seamless experience.\n\nReferences\n\nTaymaz, E. (2024). CBRT R Package. Retrieved from CBRT PackageDocumentation\nCentral Bank of the Republic of Turkey. Electronic Data Delivery System (EVDS). Retrieved from EVDS"
  },
  {
    "objectID": "posts/2025-04-30_rsquared/index.html#what-is-r²",
    "href": "posts/2025-04-30_rsquared/index.html#what-is-r²",
    "title": "Explained vs. Predictive Power: R², Adjusted R², and Beyond",
    "section": "2.1 What is R²?",
    "text": "2.1 What is R²?\nThe coefficient of determination, R², is defined as:\n\\[R^2 = 1 - \\frac{\\text{SS}_{\\text{res}}}{\\text{SS}_{\\text{tot}}}\\]\nWhere:\n\n\\(\\text{SS}_{\\text{res}}\\) = Sum of squares of residuals = \\(\\sum (y_i - \\hat{y}_i)^2\\)\n\\(\\text{SS}_{\\text{tot}}\\) = Total sum of squares = \\(\\sum (y_i - \\bar{y})^2\\)\n\nIt tells us the proportion of variance explained by the model. An R² of 0.80 implies that 80% of the variability in the dependent variable is explained by the model.\nBut beware — it only measures fit to training data, not the model’s ability to generalize."
  },
  {
    "objectID": "posts/2025-04-30_rsquared/index.html#adjusted-r²",
    "href": "posts/2025-04-30_rsquared/index.html#adjusted-r²",
    "title": "Explained vs. Predictive Power: R², Adjusted R², and Beyond",
    "section": "2.2 Adjusted R²",
    "text": "2.2 Adjusted R²\nWhen we add predictors to a regression model, R² will never decrease — even if the added variables are irrelevant.\nAdjusted R² corrects this by penalizing the number of predictors: \\[R^2_{\\text{adj}} = 1 - \\left(1 - R^2\\right) \\cdot \\left(\\frac{n - 1}{n - p - 1}\\right)\\]\nWhere:\n\nn : number of observations\np : number of predictors\n\nThus, Adjusted R² will only increase if the new predictor improves the model more than expected by chance."
  },
  {
    "objectID": "posts/2025-04-30_rsquared/index.html#predicted-r²",
    "href": "posts/2025-04-30_rsquared/index.html#predicted-r²",
    "title": "Explained vs. Predictive Power: R², Adjusted R², and Beyond",
    "section": "2.3 Predicted R²",
    "text": "2.3 Predicted R²\nPredicted R² (or cross-validated R²) is the most honest estimate of model utility. It answers the question:\n\nHow well will this model predict new, unseen data?\n\nThis is typically calculated using cross-validation, and unlike regular R², it reflects out-of-sample performance.\nYou can also view it as:\n\\[R^2_{\\text{pred}} = 1 - \\frac{\\text{PRESS}}{\\text{SS}_{\\text{tot}}}\\]\nWhere PRESS is the Prediction Error Sum of Squares based on cross-validation."
  },
  {
    "objectID": "posts/2025-04-30_rsquared/index.html#data-splitting-and-preprocessing",
    "href": "posts/2025-04-30_rsquared/index.html#data-splitting-and-preprocessing",
    "title": "Explained vs. Predictive Power: R², Adjusted R², and Beyond",
    "section": "5.1 Data Splitting and Preprocessing",
    "text": "5.1 Data Splitting and Preprocessing\nWe begin by splitting the dataset into training and testing sets. The training set will be used to fit the model, and the test set will evaluate its generalization performance.\n\nset.seed(42)\nsplit &lt;- initial_split(boston, prop = 0.8)\ntrain &lt;- training(split)\ntest &lt;- testing(split)\n\nrec &lt;- recipe(medv ~ ., data = train)\nmodel &lt;- linear_reg() %&gt;% set_engine(\"lm\")\nworkflow &lt;- workflow() %&gt;% add_recipe(rec) %&gt;% add_model(model)"
  },
  {
    "objectID": "posts/2025-04-30_rsquared/index.html#model-fitting",
    "href": "posts/2025-04-30_rsquared/index.html#model-fitting",
    "title": "Explained vs. Predictive Power: R², Adjusted R², and Beyond",
    "section": "5.2 Model Fitting",
    "text": "5.2 Model Fitting\nWe now fit the model to the training data:\n\nfit &lt;- fit(workflow, data = train)"
  },
  {
    "objectID": "posts/2025-04-30_rsquared/index.html#evaluating-the-model-on-the-training-set",
    "href": "posts/2025-04-30_rsquared/index.html#evaluating-the-model-on-the-training-set",
    "title": "Explained vs. Predictive Power: R², Adjusted R², and Beyond",
    "section": "5.3 Evaluating the Model on the Training Set",
    "text": "5.3 Evaluating the Model on the Training Set\nLet’s extract the R² and Adjusted R² values from the fitted model:\n\ntraining_summary &lt;- glance(extract_fit_parsnip(fit))\ntraining_summary %&gt;% dplyr::select(r.squared, adj.r.squared)\n\n# A tibble: 1 × 2\n  r.squared adj.r.squared\n      &lt;dbl&gt;         &lt;dbl&gt;\n1     0.726         0.717\n\n\n🔍 Interpretation:\n\nR² measures the proportion of variance in medv explained by the predictors in the training set.\nAdjusted R² adjusts this value by penalizing for the number of predictors, making it more reliable in multi-variable contexts.\n\nIf R² and Adjusted R² differ significantly, it indicates that some predictors may not be contributing meaningfully to the model.\n\nExample: A model with 12 predictors might show R² = 0.76, but Adjusted R² = 0.72 — suggesting some predictors are adding complexity without real explanatory power."
  },
  {
    "objectID": "posts/2025-04-30_rsquared/index.html#test-set-performance",
    "href": "posts/2025-04-30_rsquared/index.html#test-set-performance",
    "title": "Explained vs. Predictive Power: R², Adjusted R², and Beyond",
    "section": "5.4 Test Set Performance",
    "text": "5.4 Test Set Performance\nNow we assess the model on the unseen test data:\n\npreds &lt;- predict(fit, test) %&gt;% bind_cols(test)\nmetrics(preds, truth = medv, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       4.79 \n2 rsq     standard       0.784\n3 mae     standard       3.32 \n\n\n📉 Interpretation:\n\nIf test R² is much lower than training R², overfitting may be present.\nIf test RMSE is high, the model’s absolute prediction error is large — another sign of poor generalization."
  },
  {
    "objectID": "posts/2025-04-30_rsquared/index.html#cross-validation-for-predicted-r²",
    "href": "posts/2025-04-30_rsquared/index.html#cross-validation-for-predicted-r²",
    "title": "Explained vs. Predictive Power: R², Adjusted R², and Beyond",
    "section": "5.5 Cross-Validation for Predicted R²",
    "text": "5.5 Cross-Validation for Predicted R²\nTo get a more robust performance estimate, we use 10-fold cross-validation:\n\nset.seed(42)\ncv &lt;- vfold_cv(train, v = 10)\nresample &lt;- fit_resamples(\n  workflow,\n  resamples = cv,\n  metrics = metric_set(rsq, rmse),\n  control = control_resamples(save_pred = TRUE)\n)\ncollect_metrics(resample)\n\n# A tibble: 2 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   4.79     10  0.384  Preprocessor1_Model1\n2 rsq     standard   0.712    10  0.0341 Preprocessor1_Model1\n\n\n✅ Interpretation:\n\nPredicted R² (via CV) tells us how well the model would perform on unseen data across multiple resamples.\nIt typically lies between training R² and test R².\nConsistency between cross-validated and test R² implies a stable model.\n\n\n\n\n\n\n\nTip\n\n\n\nUse cross-validation as a standard evaluation tool, especially when data is limited.\n\n\n💬 Summary of Findings:\n\nOur linear model explains a good portion of the variance, but some predictors might be irrelevant or redundant.\nCross-validation confirms the model is relatively stable but leaves room for refinement — possibly through feature selection or nonlinear modeling.\n\nIn the next step, we can analyze residuals or explore model improvements such as polynomial terms or regularization."
  },
  {
    "objectID": "posts/2025-04-30_rsquared/index.html#residual-diagnostics",
    "href": "posts/2025-04-30_rsquared/index.html#residual-diagnostics",
    "title": "Explained vs. Predictive Power: R², Adjusted R², and Beyond",
    "section": "5.6 Residual Diagnostics",
    "text": "5.6 Residual Diagnostics\nLet’s now check if our linear model satisfies basic regression assumptions. We’ll plot residuals and assess patterns, non-linearity, and potential heteroskedasticity.\n\nlibrary(broom)\nlibrary(ggthemes)\n\naug &lt;- augment(fit$fit$fit$fit)\n\nggplot(aug, aes(.fitted, .resid)) +\n  geom_point(alpha = 0.5, color = \"#2c7fb8\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(\n    title = \"Residuals vs Fitted Values\",\n    x = \"Fitted Values\",\n    y = \"Residuals\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n📌 Interpretation:\n\nWe want residuals to be randomly scattered around zero.\nIf there’s a pattern or funnel shape, that may indicate non-linearity or heteroskedasticity."
  },
  {
    "objectID": "posts/2025-04-30_rsquared/index.html#improving-the-model-transforming-lstat",
    "href": "posts/2025-04-30_rsquared/index.html#improving-the-model-transforming-lstat",
    "title": "Explained vs. Predictive Power: R², Adjusted R², and Beyond",
    "section": "5.7 Improving the Model: Transforming lstat",
    "text": "5.7 Improving the Model: Transforming lstat\nFrom our earlier EDA, we saw a strong nonlinear relationship between lstat (lower status %) and medv. Let’s try log-transforming lstat to capture that curvature.\n\n5.7.1 Updated Recipe with Transformation\n\nrec_log &lt;- recipe(medv ~ ., data = train) %&gt;%\n  step_log(lstat)\n\nworkflow_log &lt;- workflow() %&gt;%\n  add_model(model) %&gt;%\n  add_recipe(rec_log)\n\nfit_log &lt;- fit(workflow_log, data = train)\n\n\n\n5.7.2 Evaluation of Transformed Model\n\npreds_log &lt;- predict(fit_log, test) %&gt;% bind_cols(test)\nmetrics(preds_log, truth = medv, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       4.43 \n2 rsq     standard       0.815\n3 mae     standard       3.16 \n\n\n\nglance(fit_log)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.785         0.778  4.21      110. 2.64e-121    13 -1147. 2324. 2384.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n🧠 Interpretation:\n\nCompare RMSE and R² from the transformed model to the original.\nIf we see improvement, the transformation helped capture underlying nonlinearity.\nAdjusted R² is especially helpful here to assess whether the transformation truly improved fit — not just overfit.\n\n\n\n\n\n\n\nTip\n\n\n\nTransformations, polynomial terms, and splines are all valid strategies to improve linear models without abandoning interpretability.\n\n\nWith residuals checked and a transformation tested, our next step could be to explore regularized models like ridge or lasso regression, or even move beyond linearity with tree-based models."
  },
  {
    "objectID": "posts/2025-04-30_rsquared/index.html#summary",
    "href": "posts/2025-04-30_rsquared/index.html#summary",
    "title": "Explained vs. Predictive Power: R², Adjusted R², and Beyond",
    "section": "7.1 📌 Summary",
    "text": "7.1 📌 Summary\nIn this post, we explored R², Adjusted R², and Predicted R² in depth — not just as mathematical constructs, but as tools for critical thinking in modeling. We walked through theory, practical application in R with tidymodels, residual diagnostics, and even model improvement through transformation.\nLet’s recap: - R² tells us how well our model fits the training data, but can be misleading on its own. - Adjusted R² improves upon R² by accounting for model complexity. - Predicted R², evaluated via cross-validation, provides the most trustworthy estimate of real-world performance.\nHigh R² values can be seductive. But as we saw, they don’t guarantee causality, generalizability, or correctness. Only by combining R² with residual diagnostics, domain knowledge, and out-of-sample validation can we judge a model responsibly."
  },
  {
    "objectID": "posts/2025-04-30_rsquared/index.html#recommendations-for-practitioners",
    "href": "posts/2025-04-30_rsquared/index.html#recommendations-for-practitioners",
    "title": "Explained vs. Predictive Power: R², Adjusted R², and Beyond",
    "section": "7.2 💡 Recommendations for Practitioners",
    "text": "7.2 💡 Recommendations for Practitioners\n\nAlways accompany R² with Adjusted and Predicted R² — never rely on one metric alone.\nPerform residual diagnostics to check linearity, variance assumptions, and outlier influence.\nUse cross-validation (e.g., 10-fold) as a default evaluation strategy, especially when the dataset is not large.\nTransform nonlinear predictors (as we did with lstat) or use flexible models (e.g., splines, GAMs) when needed.\nAvoid including irrelevant predictors — they inflate R² without improving generalization.\nContextualize your R² — in some fields, a lower R² is still useful; in others, it may signal inadequacy.\nComplement numerical metrics with visual tools — scatterplots, predicted vs. actual plots, and residuals reveal insights numbers alone may miss."
  },
  {
    "objectID": "posts/2025-04-30_rsquared/index.html#looking-ahead",
    "href": "posts/2025-04-30_rsquared/index.html#looking-ahead",
    "title": "Explained vs. Predictive Power: R², Adjusted R², and Beyond",
    "section": "7.3 🚀 Looking Ahead",
    "text": "7.3 🚀 Looking Ahead\nIf you want to take your modeling further: - Try ridge or lasso regression to handle multicollinearity. - Explore tree-based models (e.g., random forests) when relationships are complex and nonlinear. - Use tools like yardstick and modeltime to automate robust validation and reporting.\n\nIn the end, modeling isn’t just about maximizing R² — it’s about understanding your data, validating your decisions, and making informed predictions.\n\nThanks for reading!\nFeel free to share, fork, or reuse this analysis. Questions and comments are welcome."
  },
  {
    "objectID": "posts/2025-06-11_sd_vs_se/index.html",
    "href": "posts/2025-06-11_sd_vs_se/index.html",
    "title": "Standard Deviation vs. Standard Error: Meaning, Misuse, and the Math Behind the Confusion",
    "section": "",
    "text": "The left side illustrates standard deviation as the spread of individual data values around the population mean (μ). The right side shows standard error as the variability in sample means (x̄) obtained from repeated sampling. Notice how the SE distribution is narrower—it represents uncertainty in the estimate, not variability in the raw data."
  },
  {
    "objectID": "posts/2025-06-11_sd_vs_se/index.html#introduction-why-this-confusion-still-matters",
    "href": "posts/2025-06-11_sd_vs_se/index.html#introduction-why-this-confusion-still-matters",
    "title": "Standard Deviation vs. Standard Error: Meaning, Misuse, and the Math Behind the Confusion",
    "section": "1 Introduction: Why This Confusion Still Matters",
    "text": "1 Introduction: Why This Confusion Still Matters\nIn the world of data analysis and statistics, standard deviation (SD) and standard error (SE) are two concepts that are often misunderstood or—worse—used interchangeably. This confusion isn’t just academic: misinterpreting these two measures can lead to poor conclusions, misleading visualizations, and incorrect inferences, especially in reports intended for non-technical audiences.\nThink about this: you read a news article stating that “the average income of a sample group is $3,000 with a standard error of $500.” But then another article says “the same average income with a standard deviation of $500.” Should your level of confidence change? Absolutely—because they tell two fundamentally different stories.\nThis article aims to:\n\nDefine and differentiate standard deviation and standard error,\nExplore their mathematical foundations,\nDemonstrate their practical implications with real R code and visuals,\nWarn about common pitfalls and interpretation mistakes.\n\nBy the end of this post, you’ll not only understand the difference but also know exactly when and why each metric matters."
  },
  {
    "objectID": "posts/2025-06-11_sd_vs_se/index.html#definitions-and-mathematical-foundation",
    "href": "posts/2025-06-11_sd_vs_se/index.html#definitions-and-mathematical-foundation",
    "title": "Standard Deviation vs. Standard Error: Meaning, Misuse, and the Math Behind the Confusion",
    "section": "2 Definitions and Mathematical Foundation",
    "text": "2 Definitions and Mathematical Foundation\nUnderstanding the difference between standard deviation and standard error requires going beyond surface-level definitions. While they are mathematically related, they answer fundamentally different questions.\n\n2.1 Standard Deviation (SD)\nStandard deviation is a measure of variability or dispersion within a single dataset. It tells us how far individual observations tend to deviate from the sample (or population) mean.\nMathematically, for a sample of size \\(n\\), the sample standard deviation is given by:\n\\[\ns = \\sqrt{ \\frac{1}{n - 1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2 }\n\\]\nWhere:\n\n\\(x_i\\): Each data point\n\\(\\bar{x}\\): Sample mean\n\\(n\\): Number of observations\n\nStandard deviation is widely used in descriptive statistics to understand how spread out the values in a dataset are. A large SD implies high variability, while a small SD suggests the values are clustered closely around the mean.\n\n📌 Use case: “How much do individual students’ test scores vary from the class average?”\n\n\n\n2.2 Standard Error (SE)\nStandard error, in contrast, is a measure of precision—specifically, the precision of an estimate like the sample mean. It tells us how much the sample mean would vary if we repeatedly drew samples from the population.\nIt is defined as:\n\\[\n\\text{SE} = \\frac{s}{\\sqrt{n}}\n\\]\nAs you can see, SE is directly related to the standard deviation but scaled down by the square root of the sample size. This reflects the idea that more data gives more precise estimates.\n\n📌 Use case: “How much uncertainty is there in the sample mean as an estimate of the population mean?”\n\n\nIn short:\n\n\n\n\n\n\n\n\n\nConcept\nMeasures\nBased on\nAffected by Sample Size\n\n\n\n\nStandard Deviation\nSpread of individual data points\nIndividual observations\n❌ No\n\n\nStandard Error\nUncertainty in the sample mean\nSampling distribution\n✅ Yes\n\n\n\nUnderstanding this distinction is critical for drawing correct conclusions—especially in inferential statistics, confidence intervals, and hypothesis testing."
  },
  {
    "objectID": "posts/2025-06-11_sd_vs_se/index.html#visualizing-the-difference-with-r-simulation-and-plots",
    "href": "posts/2025-06-11_sd_vs_se/index.html#visualizing-the-difference-with-r-simulation-and-plots",
    "title": "Standard Deviation vs. Standard Error: Meaning, Misuse, and the Math Behind the Confusion",
    "section": "3 Visualizing the Difference with R: Simulation and Plots",
    "text": "3 Visualizing the Difference with R: Simulation and Plots\nLet’s demonstrate the difference between standard deviation and standard error using a simple simulation in R.\nImagine we’re sampling from a population that is normally distributed with a true mean of 100 and standard deviation of 15. We’ll simulate multiple random samples, calculate the sample means, and compare their variability to the variability of individual values within each sample.\n\n3.1 Step 1: Single Sample – Visualizing Standard Deviation\n\nset.seed(42)\nsample_data &lt;- rnorm(50, mean = 100, sd = 15)\n\n# Plotting individual data points and their deviation from the mean\nlibrary(ggplot2)\n\nggplot(data.frame(x = sample_data), aes(x = x)) +\n  geom_histogram(binwidth = 5, fill = \"steelblue\", color = \"white\", alpha = 0.7) +\n  geom_vline(aes(xintercept = mean(x)), color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  labs(\n    title = \"Standard Deviation: Spread of Individual Values\",\n    x = \"Value\", y = \"Frequency\"\n  )\n\n\n\n\n\n\n\n\nIn the plot above, the standard deviation reflects how much individual data points vary around the sample mean. Even if we repeated the sampling, the spread within each sample would remain fairly consistent."
  },
  {
    "objectID": "posts/2025-06-11_sd_vs_se/index.html#visualizing-the-difference-with-r-simulation-and-interpretation",
    "href": "posts/2025-06-11_sd_vs_se/index.html#visualizing-the-difference-with-r-simulation-and-interpretation",
    "title": "Standard Deviation vs. Standard Error: Meaning, Misuse, and the Math Behind the Confusion",
    "section": "3 Visualizing the Difference with R: Simulation and Interpretation",
    "text": "3 Visualizing the Difference with R: Simulation and Interpretation\nLet’s use R to visualize and truly understand the difference between standard deviation and standard error.\nWe’ll start by generating a single random sample from a known population and examining the spread of individual values. Then, we’ll simulate multiple samples to show how the sample means vary—and how that variation reflects the standard error.\n\n3.1 Standard Deviation: Spread of Values Within a Sample\n\nset.seed(42)\nsample_data &lt;- rnorm(50, mean = 100, sd = 15)\n\nWe generate 50 values from a normal distribution with a mean of 100 and a standard deviation of 15. This mimics a situation like measuring the heights, weights, or incomes of 50 individuals.\nLet’s visualize how these values are distributed.\n\nlibrary(ggplot2)\n\nggplot(data.frame(x = sample_data), aes(x = x)) +\n  geom_histogram(aes(y = ..density..), binwidth = 5, fill = \"steelblue\", color = \"white\", alpha = 0.6) +\n  geom_density(color = \"black\", linewidth = 1.2, linetype = \"solid\") +\n  geom_vline(aes(xintercept = mean(x)), color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  labs(\n    title = \"Standard Deviation: Spread of Individual Values\",\n    x = \"Value\", y = \"Density\"\n  )\n\n\n\n\n\n\n\n\nWhat This Graph Shows\n\nThe histogram shows the distribution of raw data from our single sample.\nThe black curve is a kernel density estimate, giving us a smooth representation of the distribution.\nThe red dashed line marks the sample mean.\nThe spread around this mean—the “thickness” of the histogram—is what the standard deviation quantifies.\n\nSo, in simple terms: standard deviation tells us how much individual values differ from their mean in one sample. It answers the question:\n\n“Are most values close to the average, or are they all over the place?”\n\n\n\n3.2 Standard Error: Spread of Sample Means Across Repeated Samples\nNow let’s go one level deeper. Instead of looking at one sample, let’s imagine we repeatedly draw many samples from the same population, each of size 50, and record their means.\n\nsample_means &lt;- replicate(1000, mean(rnorm(50, mean = 100, sd = 15)))\n\nLet’s see how those means are distributed:\n\nggplot(data.frame(mean = sample_means), aes(x = mean)) +\n  geom_histogram(aes(y = ..density..), binwidth = 1, fill = \"darkorange\", color = \"white\", alpha = 0.7) +\n  geom_density(color = \"black\", linewidth = 1.2, linetype = \"solid\") +\n  geom_vline(aes(xintercept = mean(mean)), color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  labs(\n    title = \"Standard Error: Variability of Sample Means\",\n    x = \"Sample Mean\", y = \"Density\"\n  )\n\n\n\n\n\n\n\n\nWhat This Graph Shows\n\nEach bar in the histogram represents the frequency of sample means in a small range.\nThe curve again shows the estimated density of the sample means.\nThe red dashed line is the grand mean of all 1,000 sample means—it should be close to 100.\nUnlike the previous graph, here we don’t see individual values but mean values from many samples.\n\nThis distribution is known as the sampling distribution of the sample mean.\nAnd the standard deviation of this distribution is the standard error:\n\nse_estimate &lt;- sd(sample_means)\nse_estimate\n\n[1] 2.113943\n\n\n\n\n3.3 Interpretation: Two Types of Spread, Two Different Questions\nLet’s pause and reflect on what we’ve seen so far.\nAlthough standard deviation and standard error are both measures of “spread,” they describe very different things, answer different questions, and are used in different contexts.\n\n\n\n\n\n\n\n\n\nConcept\nWhat it Measures\nBased on…\nChanges with Sample Size (\\(n\\))\n\n\n\n\nStandard Deviation\nSpread of individual data values\nSingle sample\n❌ No\n\n\nStandard Error\nSpread of sample means across repeated samples\nSampling distribution\n✅ Yes\n\n\n\n\n\n3.3.1 Summary of Interpretation\n\nStandard deviation (SD) tells us:\n&gt; “How much do individual values differ from the average within a sample?”\nStandard error (SE) tells us:\n&gt; “How much would the sample average vary if we repeated the sampling?”\n\nIn other words:\n\nSD measures natural variability among individuals (or observations).\nSE measures the statistical uncertainty of an estimate, usually the sample mean.\n\nThis difference is not just semantic—it has critical consequences for data interpretation:\n\nYou use SD when describing the spread of your sample or population.\nYou use SE when making inferences, estimating confidence intervals, or assessing how trustworthy your sample statistic is.\n\n\n\n3.3.2 The Mathematical Connection\nAs we saw earlier, the standard error is mathematically derived from the standard deviation:\n\\[\n\\text{SE} = \\frac{s}{\\sqrt{n}}\n\\]\nThis formula reveals a fundamental principle in statistics:\n\nThe more data you collect (larger \\(n\\)), the more stable your sample mean becomes.\nHowever, the variability within the sample (standard deviation \\(s\\)) may remain roughly the same—because it depends on the population, not on how many observations you took.\n\n\n🧠 Key insight:\nStandard deviation reflects the reality of your data.\nStandard error reflects your uncertainty about the mean."
  },
  {
    "objectID": "posts/2025-06-11_sd_vs_se/index.html#common-mistakes-and-misinterpretations",
    "href": "posts/2025-06-11_sd_vs_se/index.html#common-mistakes-and-misinterpretations",
    "title": "Standard Deviation vs. Standard Error: Meaning, Misuse, and the Math Behind the Confusion",
    "section": "4 Common Mistakes and Misinterpretations",
    "text": "4 Common Mistakes and Misinterpretations\nDespite their differences, standard deviation and standard error are frequently confused—even in academic papers, business reports, and media articles. Below are some of the most common mistakes and why they matter.\n\n4.1 Mistake 1: Using Standard Error Instead of Standard Deviation in Descriptive Summaries\nA classic mistake is reporting the standard error when trying to describe how spread out individual values are.\n\n❌ “The average score was 80 ± 2 (SE)”\n✅ “The average score was 80 ± 2 (SD)”\n\nIn descriptive statistics—such as reporting the results of a survey, an experiment, or a class performance—you almost always want to use the standard deviation, because it reflects individual variability.\n\n📌 The standard error, by contrast, only makes sense if your goal is to communicate how uncertain your estimate of the mean is, not how diverse the sample is.\n\n\n\n\n4.2 Mistake 2: Adding Error Bars to a Barplot Without Clarifying Whether It’s SD or SE\nBarplots with error bars are everywhere—but often, those bars are unlabeled, or worse, mislabeled.\n\nIf the error bars are standard deviation, they show the range of variation in the data.\nIf they are standard error, they show the precision of the mean estimate.\n\nYet many charts leave this ambiguous or assume the reader will infer it.\n\n✏️ Always label your error bars. In R and ggplot2, you can add labs(caption = \"Error bars represent ±1 SE\") to avoid confusion.\n\n\n\n\n4.3 Mistake 3: Believing That SE Can Describe the Sample’s Spread\nAnother subtle misinterpretation is thinking that a small SE implies the data itself is tightly clustered. But SE has nothing to do with spread among individual values.\n\nA sample can have high variability (large SD), but still have a small SE if the sample size is large.\n\nThis is especially misleading in clinical trials or public health studies, where the sample size might be very large—but individual responses vary wildly.\n\n📉 Low SE ≠ Low diversity. It just means you’re confident about the average.\n\n\n\n\n4.4 Mistake 4: Reporting SE Without Context\nIt’s not uncommon to see a mean value with a standard error reported like this:\n\n“Mean blood pressure: 132 ± 1.5”\n\nThis may seem informative—but without knowing the sample size, this value has limited meaning.\nWhy? Because SE is dependent on \\(n\\). A standard error of 1.5 from 10 observations is very different from the same SE based on 10,000 observations.\n\n✔️ Always include the sample size and preferably also the standard deviation, especially if the goal is transparency and reproducibility.\n\n\n\n\n4.5 Final Rule of Thumb\n\n\n\nIf you want to…\nUse…\n\n\n\n\nDescribe how individuals vary\nStandard Deviation\n\n\nQuantify uncertainty about the sample mean\nStandard Error\n\n\nConstruct a confidence interval\nStandard Error\n\n\nShow variability in raw data\nStandard Deviation\n\n\n\nBy respecting the purpose and proper use of these two measures, you’ll avoid misleading your audience—and build more trust in your analyses."
  },
  {
    "objectID": "posts/2025-06-11_sd_vs_se/index.html#a-real-world-example-monthly-spending-survey-in-usd",
    "href": "posts/2025-06-11_sd_vs_se/index.html#a-real-world-example-monthly-spending-survey-in-usd",
    "title": "Standard Deviation vs. Standard Error: Meaning, Misuse, and the Math Behind the Confusion",
    "section": "5 A Real-World Example: Monthly Spending Survey in USD",
    "text": "5 A Real-World Example: Monthly Spending Survey in USD\nLet’s now apply what we’ve learned in a more realistic, international scenario.\nImagine a survey conducted in a mid-sized city where 40 individuals are asked:\n\n“How much money do you spend per month (in US Dollars)?”\n\nWe simulate responses centered around $2,000, with a standard deviation of $500.\n\nset.seed(123)\nn &lt;- 40\nmonthly_spending &lt;- round(rnorm(n, mean = 2000, sd = 500), 0)\n\nhead(monthly_spending)\n\n[1] 1720 1885 2779 2035 2065 2858\n\n\n\n5.1 Descriptive Statistics\nNow let’s compute the mean, standard deviation, and standard error:\n\nmean_spending &lt;- mean(monthly_spending)\nsd_spending &lt;- sd(monthly_spending)\nse_spending &lt;- sd_spending / sqrt(n)\n\nmean_spending\n\n[1] 2022.6\n\nsd_spending\n\n[1] 448.8549\n\nse_spending\n\n[1] 70.9702\n\n\nLet’s interpret the output:\n\nMean monthly spending: approximately 2023 USD\nStandard deviation: approximately 449 USD\nStandard error: approximately 71 USD\n\n\n\n5.2 What Do These Numbers Tell Us?\n\nThe standard deviation tells us that individual spending varies by about 449 USD from the average. So one person may spend only around 1574 USD, while another spends over 2471 USD.\nThe standard error tells us that the average we see in this sample could fluctuate by about ±71 USD due to sampling variability.\n\n\n📌 While individuals differ significantly in spending habits, the sample mean is relatively stable thanks to a sufficient sample size \\(n =\\) 40\n\n\n\n5.3 Visualizing the Distribution\n\nlibrary(ggplot2)\n\nggplot(data.frame(spending = monthly_spending), aes(x = spending)) +\n  geom_histogram(aes(y = ..density..), binwidth = 250, fill = \"skyblue\", color = \"white\", alpha = 0.7) +\n  geom_density(color = \"darkblue\", linewidth = 1.2) +\n  geom_vline(aes(xintercept = mean_spending), color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  labs(\n    title = \"Distribution of Monthly Spending\",\n    x = \"Monthly Spending (USD)\", y = \"Density\"\n  )\n\n\n\n\n\n\n\n\nThis graph shows:\n\nThe red dashed line is the sample mean 2023 USD\nThe width of the histogram and smooth curve represents the variability in spending.\nThis is captured by the standard deviation, not the standard error.\n\n\n\n5.4 Confidence Interval for the Mean\nLet’s calculate a 95% confidence interval using the standard error:\n\nlower &lt;- mean_spending - 1.96 * se_spending\nupper &lt;- mean_spending + 1.96 * se_spending\n\nc(lower, upper)\n\n[1] 1883.498 2161.702\n\n\nResult:\n\nConfidence interval: approximately 1883 to 2162 USD\n\nThis tells us:\n\n“We are 95% confident that the true average monthly spending of the population lies between 1883 and 2162 USD.”\n\nRemember: this range reflects uncertainty about the mean, not individual variability."
  },
  {
    "objectID": "posts/2025-06-11_sd_vs_se/index.html#conclusion",
    "href": "posts/2025-06-11_sd_vs_se/index.html#conclusion",
    "title": "Standard Deviation vs. Standard Error: Meaning, Misuse, and the Math Behind the Confusion",
    "section": "6 Conclusion",
    "text": "6 Conclusion\nStandard deviation and standard error are often mentioned in the same breath, but they serve very different purposes in data analysis and statistical reasoning.\n\nStandard deviation reflects the natural variability in a dataset. It tells us how different individuals are from one another.\nStandard error quantifies the precision of a sample estimate, such as the mean. It tells us how much we can trust our estimate of the population parameter.\n\nWhile they are mathematically related, confusing one for the other can lead to serious misinterpretations—especially in scientific communication, data journalism, or policymaking.\nHere are some final takeaways:\n\nUse standard deviation when describing the data you have.\nUse standard error when making inferences about the population from your sample.\nAlways label your charts and error bars clearly, and report sample size to give proper context.\nDon’t mistake low standard error for low variability—it only means your estimate is more precise, not that your data is more uniform.\n\n\n🎯 In short:\nStandard deviation tells you about your data.\nStandard error tells you how much you can trust your mean.\n\nUnderstanding this distinction is more than just a statistical nuance—it’s a sign of analytical maturity."
  },
  {
    "objectID": "posts/2025-06-11_sd_vs_se/index.html#references",
    "href": "posts/2025-06-11_sd_vs_se/index.html#references",
    "title": "Standard Deviation vs. Standard Error: Meaning, Misuse, and the Math Behind the Confusion",
    "section": "7 References",
    "text": "7 References\n\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). An Introduction to Statistical Learning with Applications in R. Springer. https://www.statlearning.com\nMoore, D. S., McCabe, G. P., & Craig, B. A. (2017). Introduction to the Practice of Statistics. W.H. Freeman.\nR Core Team. (2024). R: A language and environment for statistical computing. R Foundation for Statistical Computing. https://www.r-project.org\nWickham, H., Çetinkaya-Rundel, M., & Grolemund, G. (2023). R for Data Science (2e). https://r4ds.hadley.nz\nNavarro, D. (2019). Learning Statistics with R: A tutorial for psychology students and other beginners. https://learningstatisticswithr.com"
  },
  {
    "objectID": "posts/2025-06-11_sd_vs_se/index.html#further-reading",
    "href": "posts/2025-06-11_sd_vs_se/index.html#further-reading",
    "title": "Standard Deviation vs. Standard Error: Meaning, Misuse, and the Math Behind the Confusion",
    "section": "8 Further Reading",
    "text": "8 Further Reading\n\n“Confidence Intervals: From Basic Theory to Practical Application in R”\n(Great follow-up to standard error, focusing on interval estimation.)\nhttps://towardsdatascience.com/confidence-intervals-explained-with-r-code-6d6c3f23d3f2\nHarvard Data Science course lecture on Uncertainty\nhttps://cs109.github.io/2022/lectures.html\n“Understanding Error Bars” by Nature Methods\nhttps://www.nature.com/articles/nmeth.2659"
  },
  {
    "objectID": "posts/2025-08-18_missing_values/index.html",
    "href": "posts/2025-08-18_missing_values/index.html",
    "title": "Handling Missing Data in R: A Comprehensive Guide",
    "section": "",
    "text": "Data preprocessing is a cornerstone of any data analysis or machine learning pipeline. Raw data rarely comes in a form ready for direct analysis — it often requires cleaning, transformation, normalization, and careful handling of anomalies. Among these preprocessing tasks, dealing with missing data stands out as one of the most critical and unavoidable challenges.\nMissing values appear in virtually every domain: surveys may have skipped questions, administrative registers might contain incomplete records, and clinical trials can suffer from dropout patients. Ignoring these gaps or handling them naively does not just reduce the amount of usable information; it can also introduce bias, decrease statistical power, and ultimately compromise the validity of conclusions. In other words, missing data is not just an inconvenience — it is a methodological problem that demands rigorous attention.\nIn statistical practice, missingness is often represented as NA (Not Available) in R. However, not all missing values are created equal. Some are missing completely at random, others depend on observed variables, and in some cases, the missingness itself carries meaningful information. Understanding these mechanisms is essential before deciding how to address them. This makes missing data imputation a fundamental part of the broader data preprocessing workflow, alongside tasks such as outlier detection, data normalization, and feature engineering.\nIn this article, we will cover:\n\nThe theoretical foundations of missing data mechanisms (MCAR, MAR, MNAR).\nHow to detect and visualize missing values in R.\nDifferent strategies for handling missingness, from simple imputation to advanced multiple imputation techniques.\nA practical workflow using the NHANES dataset, widely used in health research, to demonstrate methods in R.\nBest practices, pitfalls, and recommendations for applied data science.\n\nWe will use several R packages throughout this tutorial:\n\ntidyverse: Data wrangling and visualization\nnaniar and VIM: Tools for exploring and visualizing missing data\nmice: Multiple imputation by chained equations\nmissForest: Random forest–based imputation for nonlinear data\n\nBy integrating missing data handling into the larger context of preprocessing, this structured approach will not only help you manage incomplete datasets effectively but also ensure that your entire analytical workflow remains robust, transparent, and reliable."
  },
  {
    "objectID": "posts/2025-08-18_missing_values/index.html#introduction",
    "href": "posts/2025-08-18_missing_values/index.html#introduction",
    "title": "Handling Missing Data in R: A Comprehensive Guide",
    "section": "1 Introduction",
    "text": "1 Introduction\nData preprocessing is a cornerstone of any data analysis or machine learning pipeline. Raw data rarely comes in a form ready for direct analysis — it often requires cleaning, transformation, normalization, and careful handling of anomalies. Among these preprocessing tasks, dealing with missing data stands out as one of the most critical and unavoidable challenges.\nMissing values appear in virtually every domain: surveys may have skipped questions, administrative registers might contain incomplete records, and clinical trials can suffer from dropout patients. Ignoring these gaps or handling them naively does not just reduce the amount of usable information; it can also introduce bias, decrease statistical power, and ultimately compromise the validity of conclusions. In other words, missing data is not just an inconvenience — it is a methodological problem that demands rigorous attention.\nIn statistical practice, missingness is often represented as NA (Not Available) in R. However, not all missing values are created equal. Some are missing completely at random, others depend on observed variables, and in some cases, the missingness itself carries meaningful information. Understanding these mechanisms is essential before deciding how to address them. This makes missing data imputation a fundamental part of the broader data preprocessing workflow, alongside tasks such as outlier detection, data normalization, and feature engineering.\nIn this article, we will cover:\n\nThe theoretical foundations of missing data mechanisms (MCAR, MAR, MNAR).\nHow to detect and visualize missing values in R.\nDifferent strategies for handling missingness, from simple imputation to advanced multiple imputation techniques.\nA practical workflow using the NHANES dataset, widely used in health research, to demonstrate methods in R.\nBest practices, pitfalls, and recommendations for applied data science.\n\nWe will use several R packages throughout this tutorial:\n\ntidyverse: Data wrangling and visualization\nnaniar and VIM: Tools for exploring and visualizing missing data\nmice: Multiple imputation by chained equations\nmissForest: Random forest–based imputation for nonlinear data\n\nBy integrating missing data handling into the larger context of preprocessing, this structured approach will not only help you manage incomplete datasets effectively but also ensure that your entire analytical workflow remains robust, transparent, and reliable."
  },
  {
    "objectID": "posts/2025-08-18_missing_values/index.html#introduction-to-the-nhanes-dataset",
    "href": "posts/2025-08-18_missing_values/index.html#introduction-to-the-nhanes-dataset",
    "title": "Handling Missing Data in R: A Comprehensive Guide",
    "section": "2 Introduction to the NHANES Dataset",
    "text": "2 Introduction to the NHANES Dataset\nIn this section, we will work with the NHANES dataset, which comes from the US National Health and Nutrition Examination Survey.\nThe dataset includes demographic, examination, and laboratory data collected from thousands of individuals.\nSince the full dataset is quite large, we will focus only on a subset of variables that are relevant for preprocessing examples.\nHere are the variables we will use:\n\nID: Unique identifier for each participant\nAge: Age of the participant\nGender: Biological sex (male or female)\nBMI: Body Mass Index\nBPSysAve: Average systolic blood pressure\nDiabetes: Whether the participant has been diagnosed with diabetes\n\nBefore diving into preprocessing, let’s take a quick look at the structure of these selected variables:\n\nlibrary(NHANES)\nlibrary(dplyr)\n\ndata(\"NHANES\")\n\n# Select relevant variables\nnhanes_sub &lt;- NHANES |&gt; \n  select(ID, Age, Gender, BMI, BPSysAve, Diabetes)\n\nglimpse(nhanes_sub)\n\nRows: 10,000\nColumns: 6\n$ ID       &lt;int&gt; 51624, 51624, 51624, 51625, 51630, 51638, 51646, 51647, 51647…\n$ Age      &lt;int&gt; 34, 34, 34, 4, 49, 9, 8, 45, 45, 45, 66, 58, 54, 10, 58, 50, …\n$ Gender   &lt;fct&gt; male, male, male, male, female, male, male, female, female, f…\n$ BMI      &lt;dbl&gt; 32.22, 32.22, 32.22, 15.30, 30.57, 16.82, 20.64, 27.24, 27.24…\n$ BPSysAve &lt;int&gt; 113, 113, 113, NA, 112, 86, 107, 118, 118, 118, 111, 104, 134…\n$ Diabetes &lt;fct&gt; No, No, No, No, No, No, No, No, No, No, No, No, No, No, No, N…"
  },
  {
    "objectID": "posts/2025-08-18_missing_values/index.html#why-missingness-matters",
    "href": "posts/2025-08-18_missing_values/index.html#why-missingness-matters",
    "title": "Handling Missing Data in R: A Comprehensive Guide",
    "section": "3 Why Missingness Matters",
    "text": "3 Why Missingness Matters\nMissing data is not just an inconvenience — it can distort the statistical conclusions we draw from a dataset.\nThere are several critical reasons why handling missingness properly is essential:\n\nBiased results: If the missing values are not random, analyses may systematically misrepresent the population.\nReduced sample size: Complete-case analysis (simply dropping missing rows) reduces data availability, weakening statistical power.\nModel incompatibility: Many modeling techniques in R (e.g., lm(), glm()) require complete data, and will automatically drop cases with missing values, sometimes silently.\n\n\n3.1 A Short Case Example: BMI Missingness and Blood Pressure\nSuppose we want to explore how Body Mass Index (BMI) relates to Systolic Blood Pressure (BPSysAve).\nHowever, BMI contains missing values. If we ignore them and only analyze complete cases, we may end up with biased conclusions.\n\n# How many missing in BMI?\nsum(is.na(nhanes_sub$BMI))\n\n[1] 366\n\n# Complete-case dataset (dropping missing BMI)\nnhanes_complete &lt;- nhanes_sub |&gt; \n  filter(!is.na(BMI))\n\n# Compare sample sizes\nnrow(nhanes_sub)     # original sample size\n\n[1] 10000\n\nnrow(nhanes_complete) # after dropping missing BMI\n\n[1] 9634\n\n\nWe see that a substantial portion of the data is dropped when we remove missing BMI values. This reduction not only decreases efficiency but can also bias the estimates if those missing values are not randomly distributed.\n\n# Fit regression with complete cases only\nmodel_complete &lt;- lm(BPSysAve ~ BMI + Age + Gender, data = nhanes_complete)\nsummary(model_complete)\n\n\nCall:\nlm(formula = BPSysAve ~ BMI + Age + Gender, data = nhanes_complete)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-56.281  -8.652  -0.955   7.560 102.790 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 90.016503   0.677618  132.84   &lt;2e-16 ***\nBMI          0.328076   0.023228   14.12   &lt;2e-16 ***\nAge          0.412758   0.008076   51.11   &lt;2e-16 ***\nGendermale   4.346847   0.313476   13.87   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.43 on 8483 degrees of freedom\n  (1147 observations deleted due to missingness)\nMultiple R-squared:  0.2969,    Adjusted R-squared:  0.2966 \nF-statistic:  1194 on 3 and 8483 DF,  p-value: &lt; 2.2e-16\n\n\nInterpretation:\n\nThe model only uses complete cases, ignoring potentially informative missingness.\nIf BMI is more often missing in certain subgroups (e.g., older adults or females), then the relationship estimated here does not represent the whole population.\nIn later sections, we will see how different imputation strategies can mitigate this problem."
  },
  {
    "objectID": "posts/2025-08-18_missing_values/index.html#missing-data-mechanisms",
    "href": "posts/2025-08-18_missing_values/index.html#missing-data-mechanisms",
    "title": "Handling Missing Data in R: A Comprehensive Guide",
    "section": "4 Missing Data Mechanisms",
    "text": "4 Missing Data Mechanisms\nOne of the most crucial aspects of handling missing data is to understand why the data are missing.\nThe mechanism behind missingness determines whether our chosen method will yield unbiased and efficient estimates.\n\n4.1 Types of Missing Data Mechanisms\n\nMCAR (Missing Completely At Random)\nThe probability of a value being missing does not depend on either the observed or the unobserved data.\n→ Example: A lab machine randomly fails for some patients, regardless of their characteristics.\n→ Implication: Complete-case analysis is valid (though less efficient).\nMAR (Missing At Random)\nThe probability of missingness depends only on the observed data, not on the missing values themselves.\n→ Example: People with lower income are less likely to report their weight, but we observe income.\n→ Implication: Multiple imputation or likelihood-based methods can recover unbiased estimates.\nMNAR (Missing Not At Random)\nThe probability of missingness depends on the unobserved value itself.\n→ Example: People with higher BMI are less likely to report their weight.\n→ Implication: Strong assumptions or external information are needed; imputation under MAR will still be biased.\n\n\n\n4.2 What each mechanism implies (with NHANES intuition)\n\nMCAR — e.g., random device failure that occasionally prevents recording BMI.\nImplication: Complete-case analysis (dropping rows) is unbiased but wastes data.\nMAR — e.g., BMI missingness varies by observed Age or Gender.\nImplication: Likelihood-based methods or Multiple Imputation (MI) are valid if those predictors are in the imputation model.\nMNAR — e.g., people with very high BMI systematically do not report it.\nImplication: MAR-based methods still biased; requires sensitivity analysis or explicit MNAR models.\n\n\n\n4.3 Quick NHANES checks that suggest a mechanism\nBelow we do two simple diagnostics on our working subset nhanes_sub\n(defined earlier as: NHANES |&gt; select(ID, Age, Gender, BMI, BPSysAve, Diabetes)).\n\n# Packages we already use\nlibrary(dplyr)\nlibrary(knitr)\n\n# 1) Overall BMI missingness\nnhanes_sub |&gt;\n  summarise(pct_missing_BMI = mean(is.na(BMI)) * 100) |&gt;\n  mutate(pct_missing_BMI = round(pct_missing_BMI, 1)) |&gt;\n  kable(caption = \"Overall BMI missingness (%)\")\n\n\nOverall BMI missingness (%)\n\n\npct_missing_BMI\n\n\n\n\n3.7\n\n\n\n\n# 2) Does BMI missingness vary by observed variables? (MAR hint)\n#    - By Gender\nby_gender &lt;- nhanes_sub |&gt;\n  group_by(Gender) |&gt;\n  summarise(pct_miss_BMI = mean(is.na(BMI)) * 100,\n            n = n(), .groups = \"drop\") |&gt;\n  mutate(pct_miss_BMI = round(pct_miss_BMI, 1))\n\n#    - By Age groups (bins)\nby_age &lt;- nhanes_sub |&gt;\n  mutate(AgeBand = cut(Age, breaks = c(0, 30, 45, 60, Inf),\n                       labels = c(\"&lt;=30\", \"31–45\", \"46–60\", \"60+\"),\n                       right = FALSE)) |&gt;\n  group_by(AgeBand) |&gt;\n  summarise(pct_miss_BMI = mean(is.na(BMI)) * 100,\n            n = n(), .groups = \"drop\") |&gt;\n  mutate(pct_miss_BMI = round(pct_miss_BMI, 1))\n\n# Show summaries nicely\nkable(by_gender, caption = \"BMI missingness by Gender (%)\")\n\n\nBMI missingness by Gender (%)\n\n\nGender\npct_miss_BMI\nn\n\n\n\n\nfemale\n3.6\n5020\n\n\nmale\n3.8\n4980\n\n\n\n\nkable(by_age,    caption = \"BMI missingness by Age band (%)\")\n\n\nBMI missingness by Age band (%)\n\n\nAgeBand\npct_miss_BMI\nn\n\n\n\n\n&lt;=30\n7.6\n4121\n\n\n31–45\n0.5\n2049\n\n\n46–60\n0.6\n1991\n\n\n60+\n1.7\n1839\n\n\n\n\n\nInterpretation:\n\nIf pct_miss_BMI is similar across groups, MCAR is more plausible.\nIf missingness changes with Age or Gender, MAR is more plausible (we must include those predictors in imputation).\nThese are indicators, not proofs; true MNAR needs external info or sensitivity analyses.\n\nWhich methods are valid under which mechanism?\n\n\n\n\n\n\n\n\n\nMechanism\nExample (NHANES context)\nValid methods\nNotes\n\n\n\n\nMCAR\nRandom loss of BMI records\nComplete-case, single imputation, MI\nUnbiased but may waste data\n\n\nMAR\nBMI missingness varies by observed Age, Gender\nMultiple Imputation (MICE), likelihood/EM, missForest\nInclude strong predictors of missingness\n\n\nMNAR\nPeople with very high BMI hide it\nSensitivity analysis, selection/pattern-mixture models\nMAR-based MI alone is biased\n\n\n\n\n\n\n\n\n\nOptional: Little’s MCAR Test\n\n\n\nLittle’s MCAR test is a statistical procedure used to examine whether data are Missing Completely at Random (MCAR).\n⚠️ However, this test comes with important caveats:\n- It can be overly sensitive in large samples, flagging trivial deviations.\n- In small samples, its power is often too low to detect meaningful departures from MCAR.\nBecause of these limitations, it should be treated only as a supporting tool rather than a definitive test when diagnosing missingness mechanisms."
  },
  {
    "objectID": "posts/2025-08-18_missing_values/index.html#missing-data-mechanisms-1",
    "href": "posts/2025-08-18_missing_values/index.html#missing-data-mechanisms-1",
    "title": "Handling Missing Data in R: A Comprehensive Guide",
    "section": "5 Missing Data Mechanisms",
    "text": "5 Missing Data Mechanisms\nOne of the most crucial aspects of handling missing data is to understand why the data are missing.\nThe mechanism behind missingness determines whether our chosen method will yield unbiased and efficient estimates.\n\n5.1 Types of Missing Data Mechanisms\n\nMCAR (Missing Completely At Random)\nThe probability of a value being missing does not depend on either the observed or the unobserved data.\n→ Example: A lab machine randomly fails for some patients, regardless of their characteristics.\n→ Implication: Complete-case analysis is valid (though less efficient).\nMAR (Missing At Random)\nThe probability of missingness depends only on the observed data, not on the missing values themselves.\n→ Example: People with lower income are less likely to report their weight, but we observe income.\n→ Implication: Multiple imputation or likelihood-based methods can recover unbiased estimates.\nMNAR (Missing Not At Random)\nThe probability of missingness depends on the unobserved value itself.\n→ Example: People with higher BMI are less likely to report their weight.\n→ Implication: Strong assumptions or external information are needed; imputation under MAR will still be biased.\n\n\n\n5.2 What each mechanism implies (with NHANES intuition)\n\nMCAR — e.g., random device failure that occasionally prevents recording BMI.\nImplication: Complete-case analysis (dropping rows) is unbiased but wastes data.\nMAR — e.g., BMI missingness varies by observed Age or Gender.\nImplication: Likelihood-based methods or Multiple Imputation (MI) are valid if those predictors are in the imputation model.\nMNAR — e.g., people with very high BMI systematically do not report it.\nImplication: MAR-based methods still biased; requires sensitivity analysis or explicit MNAR models.\n\n\n\n5.3 Quick NHANES checks that suggest a mechanism\nBelow we do two simple diagnostics on our working subset nhanes_sub\n(defined earlier as: NHANES |&gt; select(ID, Age, Gender, BMI, BPSysAve, Diabetes)).\n\n# Packages we already use\nlibrary(dplyr)\nlibrary(knitr)\n\n# 1) Overall BMI missingness\nnhanes_sub |&gt;\n  summarise(pct_missing_BMI = mean(is.na(BMI)) * 100) |&gt;\n  mutate(pct_missing_BMI = round(pct_missing_BMI, 1)) |&gt;\n  kable(caption = \"Overall BMI missingness (%)\")\n\n\nOverall BMI missingness (%)\n\n\npct_missing_BMI\n\n\n\n\n3.7\n\n\n\n\n# 2) Does BMI missingness vary by observed variables? (MAR hint)\n#    - By Gender\nby_gender &lt;- nhanes_sub |&gt;\n  group_by(Gender) |&gt;\n  summarise(pct_miss_BMI = mean(is.na(BMI)) * 100,\n            n = n(), .groups = \"drop\") |&gt;\n  mutate(pct_miss_BMI = round(pct_miss_BMI, 1))\n\n#    - By Age groups (bins)\nby_age &lt;- nhanes_sub |&gt;\n  mutate(AgeBand = cut(Age, breaks = c(0, 30, 45, 60, Inf),\n                       labels = c(\"&lt;=30\", \"31–45\", \"46–60\", \"60+\"),\n                       right = FALSE)) |&gt;\n  group_by(AgeBand) |&gt;\n  summarise(pct_miss_BMI = mean(is.na(BMI)) * 100,\n            n = n(), .groups = \"drop\") |&gt;\n  mutate(pct_miss_BMI = round(pct_miss_BMI, 1))\n\n# Show summaries nicely\nkable(by_gender, caption = \"BMI missingness by Gender (%)\")\n\n\nBMI missingness by Gender (%)\n\n\nGender\npct_miss_BMI\nn\n\n\n\n\nfemale\n3.6\n5020\n\n\nmale\n3.8\n4980\n\n\n\n\nkable(by_age,    caption = \"BMI missingness by Age band (%)\")\n\n\nBMI missingness by Age band (%)\n\n\nAgeBand\npct_miss_BMI\nn\n\n\n\n\n&lt;=30\n7.6\n4121\n\n\n31–45\n0.5\n2049\n\n\n46–60\n0.6\n1991\n\n\n60+\n1.7\n1839\n\n\n\n\n\nInterpretation:\n\nIf pct_miss_BMI is similar across groups, MCAR is more plausible.\nIf missingness changes with Age or Gender, MAR is more plausible (we must include those predictors in imputation).\nThese are indicators, not proofs; true MNAR needs external info or sensitivity analyses.\n\nWhich methods are valid under which mechanism?\n\n\n\n\n\n\n\n\n\nMechanism\nExample (NHANES context)\nValid methods\nNotes\n\n\n\n\nMCAR\nRandom loss of BMI records\nComplete-case, single imputation, MI\nUnbiased but may waste data\n\n\nMAR\nBMI missingness varies by observed Age, Gender\nMultiple Imputation (MICE), likelihood/EM, missForest\nInclude strong predictors of missingness\n\n\nMNAR\nPeople with very high BMI hide it\nSensitivity analysis, selection/pattern-mixture models\nMAR-based MI alone is biased\n\n\n\n\n\n\n\n\n\nOptional: Little’s MCAR Test\n\n\n\nLittle’s MCAR test is a statistical procedure used to examine whether data are Missing Completely at Random (MCAR).\n⚠️ However, this test comes with important caveats:\n- It can be overly sensitive in large samples, flagging trivial deviations.\n- In small samples, its power is often too low to detect meaningful departures from MCAR.\nBecause of these limitations, it should be treated only as a supporting tool rather than a definitive test when diagnosing missingness mechanisms."
  },
  {
    "objectID": "posts/2025-08-18_missing_values/index.html#detecting-missing-data",
    "href": "posts/2025-08-18_missing_values/index.html#detecting-missing-data",
    "title": "Handling Missing Data in R: A Comprehensive Guide",
    "section": "5 Detecting Missing Data",
    "text": "5 Detecting Missing Data\nBefore applying any imputation or modeling technique, it is essential to explore the extent and structure of missingness in the dataset. The nhanes_sub data frame, derived from the NHANES dataset, will be used for illustration.\n\n5.1 Simple Counts and Summaries\nThe first step is to quantify how many values are missing per variable.\n\n# Count missing values for each variable\nnhanes_sub %&gt;%\n  summarise(across(everything(), ~ sum(is.na(.)))) \n\n# A tibble: 1 × 6\n     ID   Age Gender   BMI BPSysAve Diabetes\n  &lt;int&gt; &lt;int&gt;  &lt;int&gt; &lt;int&gt;    &lt;int&gt;    &lt;int&gt;\n1     0     0      0   366     1449      142\n\n\nThe output shows the number of missing values in each column, making it easy to spot problematic variables. Another quick check is to identify how many complete vs. incomplete cases exist:\n\nsum(complete.cases(nhanes_sub))       # number of complete rows\n\n[1] 8482\n\nsum(!complete.cases(nhanes_sub))      # number of incomplete rows\n\n[1] 1518\n\n\nThis gives us an idea of the proportion of observations that would be lost if we opted for listwise deletion.\n\n\n5.2 Visualizing Missingness\nTextual summaries are informative, but missing data often has patterns that are better revealed visually. Several R packages support this task:\n\n5.2.1 naniar\n\nlibrary(naniar)\nlibrary(ggplot2)\n\n# Visualize missing values by variable\ngg_miss_var(nhanes_sub, show_pct = TRUE) +\n  labs(title = \"Missing Values by Variable in NHANES Subset\",\n       x = \"Variables\",\n       y = \"Proportion of Missing Values\")\n\n\n\n\n\n\n\n\n\nEach bar corresponds to a variable.\nThe height of the bar shows how many observations are missing for that variable.\nWith show_pct = TRUE, the proportion of missing values is also displayed, making it easier to compare across variables.\nVariables with tall bars clearly have higher missingness (e.g., BMI or blood pressure variables often stand out in this dataset).\n\n\n\n5.2.2 VIM\n\nlibrary(VIM)\n\naggr(nhanes_sub, numbers = TRUE, prop = FALSE, sortVar = TRUE)\n\n\n\n\n\n\n\n\n\n Variables sorted by number of missings: \n Variable Count\n BPSysAve  1449\n      BMI   366\n Diabetes   142\n       ID     0\n      Age     0\n   Gender     0\n\n\nThis aggregated visualization shows the proportion of missing values per variable and the combinations of missingness across variables.\n\n\n5.2.3 visdat\n\nlibrary(visdat)\n\nvis_dat(nhanes_sub)\n\n\n\n\n\n\n\n\nThis function displays the data type of each variable and overlays missingness, helping to identify whether missing values cluster in certain variable types (e.g., numeric vs. categorical).\n\n\n\n5.3 Interpreting the Patterns\n\nRandom scatter of missing values across rows/columns may indicate MCAR (though formal testing is required).\nSystematic patterns (e.g., older participants more likely to have missing BMI) hint at MAR.\nBlocks of missingness (entire variables missing for subgroups) may suggest MNAR or structural missingness."
  },
  {
    "objectID": "posts/2025-08-18_missing_values/index.html#handling-missing-data-methods",
    "href": "posts/2025-08-18_missing_values/index.html#handling-missing-data-methods",
    "title": "Handling Missing Data in R: A Comprehensive Guide",
    "section": "6 Handling Missing Data — Methods",
    "text": "6 Handling Missing Data — Methods\nIn this section we review the main families of methods, show when each is appropriate, and demonstrate them on nhanes_sub. We will explicitly call out the trade-offs so readers can choose deliberately—not by habit.\n\n\n6.1 Deletion\nListwise deletion (complete-case) removes any row that contains at least one missing value.\nPairwise deletion uses all available pairs to compute correlations/covariances, which can later lead to non–positive-definite covariance matrices and failures in modeling.\n\nPros - Simple; widely implemented by default (often silently). - Unbiased only under MCAR.\nCons - Wastes data; reduces power. - Biased under MAR/MNAR; can change the sample composition.\n\n\n# How many rows would we lose if we required complete cases for these variables?\nn_total &lt;- nrow(nhanes_sub)\nn_cc    &lt;- nhanes_sub |&gt; stats::complete.cases() |&gt; sum()\n\ncbind(\n  total_rows    = n_total,\n  complete_cases= n_cc,\n  lost_rows     = n_total - n_cc,\n  lost_pct      = round((n_total - n_cc) / n_total * 100, 1)\n)\n\n     total_rows complete_cases lost_rows lost_pct\n[1,]      10000           8482      1518     15.2\n\n\nInterpretation: If the lost percentage is non-trivial (e.g., &gt;5–10%), listwise deletion both shrinks power and risks bias unless MCAR truly holds. Pairwise deletion is not recommended for modeling because it can yield inconsistent covariance structures.\n\n\n6.2 Simple Imputation\nIdea. Fill missing values with a single plausible value (one pass). Fast and convenient, but it underestimates uncertainty (standard errors too small) and can distort distributions.\nTypical choices\n\nMean/Median/Mode (baselines; median is more robust to skew)\nk-Nearest Neighbors (kNN) (borrows information from similar rows)\nHot-deck (donor-based; similar spirit to kNN)\n\n\n6.2.1 Median (numeric) + Mode (categorical) baselines\n\nset.seed(2025)\n\n# Create a median-imputed BMI for illustration (only if BMI is missing)\nnh_med &lt;- nhanes_sub |&gt;\n  mutate(\n    BMI_med = ifelse(is.na(BMI), stats::median(BMI, na.rm = TRUE), BMI)\n  )\n\n# Compare how many BMI were imputed\nsum(is.na(nhanes_sub$BMI))           # original missing BMI count\n\n[1] 366\n\nsum(is.na(nh_med$BMI_med))           # should be 0\n\n[1] 0\n\n\nDistribution distortion (variance shrinkage).\n\nlibrary(ggplot2)\n\n# Compare BMI distribution: complete-case vs median-imputed\np_cc  &lt;- nhanes_sub |&gt;\n  filter(!is.na(BMI)) |&gt;\n  ggplot(aes(x = BMI)) +\n  geom_density() +\n  labs(title = \"BMI density — complete cases\")\n\np_med &lt;- nh_med |&gt;\n  ggplot(aes(x = BMI_med)) +\n  geom_density() +\n  labs(title = \"BMI density — median-imputed\")\n\np_cc; p_med\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation: Median imputation spikes the distribution around the median and reduces variance. This can attenuate real relationships that depend on dispersion.\n\n\n6.2.2 kNN (donor-based) imputation\n\n# kNN imputation with VIM::kNN (works on data frames; chooses donors by similarity)\nlibrary(VIM)\n\n# We impute only BMI here; set k=5 as a reasonable starting point.\nnh_knn &lt;- nhanes_sub |&gt;\n  select(Age, Gender, BMI, BPSysAve, Diabetes) |&gt;\n  VIM::kNN(k = 5, imp_var = FALSE)  # imp_var=FALSE avoids extra *_imp columns\n\n# Check imputation effect\nsum(is.na(nhanes_sub$BMI))   # original missing BMI\n\n[1] 366\n\nsum(is.na(nh_knn$BMI))       # after kNN (should be 0)\n\n[1] 0\n\n\nInterpretation: kNN preserves local structure better than mean/median, but it is still single imputation → uncertainty is not propagated. Choice of k and included predictors matters.\n\nRule of thumb. Simple methods are acceptable for quick EDA or as baselines. For principled inference under MAR, prefer Multiple Imputation.\n\n\n\n\n6.3 Advanced Methods\n\n6.3.1 Multiple Imputation with mice\nSo far, we have seen that missing values exist in several variables of our dataset. A common and powerful approach to handle missingness is Multiple Imputation by Chained Equations (MICE). The mice package in R is widely used for this purpose. The idea is simple:\n\nInstead of filling in missing values once, MICE creates multiple complete datasets by imputing values several times.\nEach dataset is then analyzed separately.\nFinally, results are pooled together to account for the variability introduced by missingness.\n\nLet’s try this approach on our subset of the NHANES data:\n\nlibrary(mice)\n\n# Create imputations\nimp &lt;- mice(nhanes_sub, m = 3, seed = 123)\n\n\n iter imp variable\n  1   1  BMI  BPSysAve  Diabetes\n  1   2  BMI  BPSysAve  Diabetes\n  1   3  BMI  BPSysAve  Diabetes\n  2   1  BMI  BPSysAve  Diabetes\n  2   2  BMI  BPSysAve  Diabetes\n  2   3  BMI  BPSysAve  Diabetes\n  3   1  BMI  BPSysAve  Diabetes\n  3   2  BMI  BPSysAve  Diabetes\n  3   3  BMI  BPSysAve  Diabetes\n  4   1  BMI  BPSysAve  Diabetes\n  4   2  BMI  BPSysAve  Diabetes\n  4   3  BMI  BPSysAve  Diabetes\n  5   1  BMI  BPSysAve  Diabetes\n  5   2  BMI  BPSysAve  Diabetes\n  5   3  BMI  BPSysAve  Diabetes\n\n# Look at a summary\nimp\n\nClass: mids\nNumber of multiple imputations:  3 \nImputation methods:\n      ID      Age   Gender      BMI BPSysAve Diabetes \n      \"\"       \"\"       \"\"    \"pmm\"    \"pmm\" \"logreg\" \nPredictorMatrix:\n         ID Age Gender BMI BPSysAve Diabetes\nID        0   1      1   1        1        1\nAge       1   0      1   1        1        1\nGender    1   1      0   1        1        1\nBMI       1   1      1   0        1        1\nBPSysAve  1   1      1   1        0        1\nDiabetes  1   1      1   1        1        0\n\n\nThe output shows:\n\nm = 3: number of imputed datasets created.\nFor each variable with missingness, the method used for imputation.\nHow many iterations were performed in the algorithm.\n\nWe can take a quick look at the imputed values:\n\n# Inspect first few imputations for BMI\nhead(imp$imp$BMI)\n\n        1     2     3\n61  43.00 17.70 12.90\n161 16.70 13.50 18.59\n210 16.28 24.00 23.10\n309 24.00 37.32 26.20\n310 24.00 28.55 26.30\n320 25.10 32.25 29.20\n\n\nThis shows different plausible values for missing BMI observations across the three imputed datasets. Each dataset gives slightly different results, which is expected and important for reflecting uncertainty.\nOnce we have these imputations, we can complete the dataset:\n\n# Extract the first imputed dataset\nnhanes_completed &lt;- complete(imp, 1)\n\nhead(nhanes_completed)\n\n     ID Age Gender   BMI BPSysAve Diabetes\n1 51624  34   male 32.22      113       No\n2 51624  34   male 32.22      113       No\n3 51624  34   male 32.22      113       No\n4 51625   4   male 15.30       92       No\n5 51630  49 female 30.57      112       No\n6 51638   9   male 16.82       86       No\n\n\nNow we have a complete dataset with no missing values. In practice, we would analyze all imputed datasets and then combine results using Rubin’s rules, but the key takeaway here is:\n\nmice() provides multiple versions of the data,\nimputations are based on relationships among variables,\nand the method preserves uncertainty rather than hiding it.\n\n\n\n\n\n\n\nMICE Essentials: Key Arguments\n\n\n\n\nmethod: Specifies the imputation model for each variable.\n\npmm: predictive mean matching (continuous variables)\nlogreg: logistic regression (binary)\npolyreg: multinomial regression (nominal categorical)\npolr: proportional odds model (ordered categorical)\n\nRule of thumb: If a factor has &gt;2 levels, prefer polyreg (nominal) or polr (ordered) instead of logreg. Always check the actual levels of variables such as Gender or Diabetes in your data before setting methods.\npredictorMatrix: Controls which variables are used to predict others.\n\nRows = target variables (to be imputed)\nColumns = predictor variables\n\nm: Number of multiple imputations to generate (commonly 5–20).\n\nMore imputations recommended for high missingness.\n\nmaxit: Number of iterations of the chained equations (often 5–10).\nseed: Random seed for reproducibility.\n\nAlways set when writing tutorials or reports.\n\n\n\n\n\n\n6.3.2 Multiple Imputation with missForest\nThe missForest package provides a non-parametric imputation method based on random forests.\nUnlike mice, which generates multiple imputations, missForest creates a single completed dataset by iteratively predicting missing values using random forest models. It works well with both continuous and categorical variables and can capture nonlinear relationships.\nWe will use the same nhanes_sub dataset as before:\n\nlibrary(dplyr)\nlibrary(missForest)\n\n# Start from the existing subset:\n# nhanes_sub &lt;- NHANES |&gt; select(ID, Age, Gender, BMI, BPSysAve, Diabetes)\n\n# 1) Keep only model-relevant columns (drop pure identifier)\n# 2) Convert character variables to factors (missForest expects factors, not raw character)\n# 3) Coerce to base data.frame to avoid tibble-related method dispatch issues\nmf_input &lt;- nhanes_sub |&gt;\n  select(Age, Gender, BMI, BPSysAve, Diabetes) |&gt;\n  mutate(across(where(is.character), as.factor)) |&gt;\n  as.data.frame()\n\nset.seed(123)\nmf_fit &lt;- missForest(\n  mf_input,\n  ntree   = 200,    # more trees -&gt; stabler imputations\n  maxiter = 5,      # outer iterations (default 10; 5 is fine for demo)\n  verbose = FALSE\n)\n\n# Completed data and OOB error\nmf_imputed &lt;- mf_fit$ximp\nmf_oob     &lt;- mf_fit$OOBerror\n\n# Quick checks\nsum(is.na(mf_input$BMI))\n\n[1] 366\n\nsum(is.na(mf_imputed$BMI))   # should go to 0\n\n[1] 0\n\nmf_oob\n\n     NRMSE        PFC \n0.17890307 0.02667884 \n\n\nThe function returns a list with two key elements:\n\nximp: the completed dataset after imputation.\nOOBerror: the estimated imputation error (normalized root mean squared error for continuous variables and proportion of falsely classified entries for categorical variables).\n\nInterpretation:\n\nThe completed dataset (ximp) replaces all missing values with imputed estimates.\nNRMSE (Normalized Root Mean Squared Error): 0.1789\n\nThis value reflects the imputation error for continuous variables (e.g., Age, BMI, BPSysAve).\nSince it is normalized, values closer to 0 indicate better accuracy. Here, an error of ~0.18 suggests that the imputed values are quite close to the true (non-missing) values.\n\nPFC (Proportion of Falsely Classified): 0.0267\n\nThis metric evaluates categorical variables (e.g., Gender, Diabetes).\nA value of ~0.027 means only about 2.7% of categorical imputations were misclassified, which is a strong performance.\n\n\n✅ Interpretation:\nThe results indicate that missForest produced high-quality imputations: continuous variables are imputed with relatively low error, and categorical variables with very low misclassification. In practical terms, this means the dataset after imputation is reliable and close to the original data distribution.\nPros and Cons of missForest\nAdvantages:\n\nHandles mixed data types (continuous + categorical).\nCaptures nonlinearities and complex interactions.\nNo need to specify an explicit imputation model.\n\nLimitations:\n\nProduces only a single imputed dataset, so uncertainty is not directly quantified (unlike mice).\nComputationally more expensive for very large datasets."
  },
  {
    "objectID": "posts/2025-08-18_missing_values/index.html#single-vs.-multiple-imputation",
    "href": "posts/2025-08-18_missing_values/index.html#single-vs.-multiple-imputation",
    "title": "Handling Missing Data in R: A Comprehensive Guide",
    "section": "7 Single vs. Multiple Imputation",
    "text": "7 Single vs. Multiple Imputation\nOne critical distinction in handling missing data is single imputation vs. multiple imputation (MI).\n\nSingle imputation (mean, median, regression, etc.) fills each missing value once. While simple, it ignores uncertainty, treating imputed values as if they were observed.\nMultiple imputation generates several plausible versions of the dataset (e.g., 5–10). Each dataset is analyzed separately, and results are then combined (pooled). This approach accounts for variability due to missingness and produces more reliable inferences.\n\nLet’s illustrate with our nhanes_sub dataset:\n\n# Complete-case analysis (ignores missing data)\nlm_cc &lt;- lm(BMI ~ Age + Gender + BPSysAve + Diabetes,\n            data = nhanes_sub, na.action = na.omit)\n\n# Single imputation (mean imputation for BMI)\nnhanes_single &lt;- nhanes_sub |&gt; \n  mutate(BMI = ifelse(is.na(BMI), mean(BMI, na.rm = TRUE), BMI))\n\nlm_si &lt;- lm(BMI ~ Age + Gender + BPSysAve + Diabetes,\n            data = nhanes_single)\n\n# Multiple imputation with mice\nimp &lt;- mice(nhanes_sub, m = 5, method = \"pmm\", seed = 123)\n\n\n iter imp variable\n  1   1  BMI  BPSysAve  Diabetes\n  1   2  BMI  BPSysAve  Diabetes\n  1   3  BMI  BPSysAve  Diabetes\n  1   4  BMI  BPSysAve  Diabetes\n  1   5  BMI  BPSysAve  Diabetes\n  2   1  BMI  BPSysAve  Diabetes\n  2   2  BMI  BPSysAve  Diabetes\n  2   3  BMI  BPSysAve  Diabetes\n  2   4  BMI  BPSysAve  Diabetes\n  2   5  BMI  BPSysAve  Diabetes\n  3   1  BMI  BPSysAve  Diabetes\n  3   2  BMI  BPSysAve  Diabetes\n  3   3  BMI  BPSysAve  Diabetes\n  3   4  BMI  BPSysAve  Diabetes\n  3   5  BMI  BPSysAve  Diabetes\n  4   1  BMI  BPSysAve  Diabetes\n  4   2  BMI  BPSysAve  Diabetes\n  4   3  BMI  BPSysAve  Diabetes\n  4   4  BMI  BPSysAve  Diabetes\n  4   5  BMI  BPSysAve  Diabetes\n  5   1  BMI  BPSysAve  Diabetes\n  5   2  BMI  BPSysAve  Diabetes\n  5   3  BMI  BPSysAve  Diabetes\n  5   4  BMI  BPSysAve  Diabetes\n  5   5  BMI  BPSysAve  Diabetes\n\nlm_mi &lt;- with(imp, lm(BMI ~ Age + Gender + BPSysAve + Diabetes))\npooled &lt;- pool(lm_mi)\n\n\nsummary(lm_cc)\n\n\nCall:\nlm(formula = BMI ~ Age + Gender + BPSysAve + Diabetes, data = nhanes_sub, \n    na.action = na.omit)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.771  -4.640  -1.053   3.553  53.003 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 17.488597   0.513225  34.076   &lt;2e-16 ***\nAge          0.047930   0.004273  11.217   &lt;2e-16 ***\nGendermale  -0.278756   0.144799  -1.925   0.0542 .  \nBPSysAve     0.067504   0.004903  13.767   &lt;2e-16 ***\nDiabetesYes  3.789606   0.265240  14.287   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.588 on 8477 degrees of freedom\n  (1518 observations deleted due to missingness)\nMultiple R-squared:  0.1136,    Adjusted R-squared:  0.1132 \nF-statistic: 271.5 on 4 and 8477 DF,  p-value: &lt; 2.2e-16\n\nsummary(lm_si)\n\n\nCall:\nlm(formula = BMI ~ Age + Gender + BPSysAve + Diabetes, data = nhanes_single)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.657  -4.634  -1.029   3.533  53.004 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 17.600406   0.508667  34.601   &lt;2e-16 ***\nAge          0.047159   0.004237  11.129   &lt;2e-16 ***\nGendermale  -0.287221   0.143820  -1.997   0.0458 *  \nBPSysAve     0.066791   0.004858  13.748   &lt;2e-16 ***\nDiabetesYes  3.725941   0.262781  14.179   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.57 on 8541 degrees of freedom\n  (1454 observations deleted due to missingness)\nMultiple R-squared:  0.1118,    Adjusted R-squared:  0.1114 \nF-statistic: 268.8 on 4 and 8541 DF,  p-value: &lt; 2.2e-16\n\nsummary(pooled)\n\n         term    estimate   std.error statistic       df       p.value\n1 (Intercept) 14.66559186 0.487686304 30.071773 1253.210 5.075329e-150\n2         Age  0.10367759 0.003748806 27.656164 3916.798 6.073157e-154\n3  Gendermale -0.30521870 0.134707691 -2.265785 3727.382  2.352165e-02\n4    BPSysAve  0.06754332 0.004761820 14.184349 1160.000  3.121286e-42\n5 DiabetesYes  3.23549109 0.260299313 12.429887 8866.664  3.529334e-35\n\n\nWe applied three different approaches to handle missing BMI values in the nhanes_sub dataset, modeling BMI ~ Age + Gender + BPSysAve + Diabetes. Here is what we found:\n1. Complete-Case Analysis (CCA)\n\nWhat we did: We dropped all observations with missing values (na.omit).\nResult:\n\nCoefficients: Age (0.048), BPSysAve (0.068), DiabetesYes (+3.79), Gender slightly negative.\nStandard errors: Relatively large because ~1500 observations were discarded.\nR²: 0.114 — fairly low.\n\nTakeaway: CCA wastes data and may bias estimates if missingness is not MCAR (Missing Completely at Random).\n\n2. Single Imputation (Mean Substitution for BMI)\n\nWhat we did: Replaced missing BMI values with the mean BMI.\nResult:\n\nCoefficients: Very close to CCA (Age 0.047, BPSysAve 0.067, DiabetesYes +3.73).\nGender effect became just significant (p = 0.045).\nResidual SE decreased slightly (6.57).\n\nTakeaway: Looks “better” because all observations are retained, but this approach ignores imputation uncertainty and artificially stabilizes estimates. Standard errors are underestimated, leading to overconfidence.\n\n3. Multiple Imputation (MI with mice, m = 5, method = “pmm”)\n\nWhat we did: Generated 5 imputed datasets using Predictive Mean Matching (PMM), fit the same model in each, and pooled results.\nResult:\n\nCoefficients: Age effect doubled (0.104), intercept dropped (14.7 vs. ~17.5), Diabetes effect slightly smaller (+3.24), Gender effect remained modest but significant (p = 0.023).\nStandard errors: Properly adjusted upwards — reflecting real uncertainty in imputed BMI values.\nInference: Despite differences in point estimates, the conclusions are more statistically honest.\n\nTakeaway: MI balances efficiency (uses all data) and validity (acknowledges missingness uncertainty).\n\n🔑 Overall Comparison\n\n\n\n\n\n\n\n\n\n\nMethod\nKeeps All Data\nCoefficients Similar?\nSE Adjusted for Uncertainty?\nMain Issue\n\n\n\n\nComplete Case (CCA)\n❌ (~1500 rows lost)\nYes, but less precise\n✅ (but biased if MAR/MNAR)\nData loss, possible bias\n\n\nSingle Imputation (SI)\n✅\nSimilar to CCA\n❌ Underestimated\nOverconfident inference\n\n\nMultiple Imputation (MI)\n✅\nSomewhat different (esp. Age)\n✅ Properly adjusted\nMore computation needed\n\n\n\nInterpretation:\n\nComplete-case drops too much data and risks bias.\nSingle imputation keeps the data but gives too much confidence in results.\nMultiple imputation changes some coefficients (notably Age) and reports more realistic uncertaint\n\n👉 Lesson: If your goal is valid inference, especially in epidemiological or social science settings, multiple imputation is the gold standard."
  },
  {
    "objectID": "posts/2025-08-18_missing_values/index.html#comparison-of-common-imputation-methods",
    "href": "posts/2025-08-18_missing_values/index.html#comparison-of-common-imputation-methods",
    "title": "Handling Missing Data in R: A Comprehensive Guide",
    "section": "8 Comparison of Common Imputation Methods",
    "text": "8 Comparison of Common Imputation Methods\n\n\n\n\n\n\n\n\n\nMethod\nDescription\nAdvantages\nDisadvantages\n\n\n\n\nListwise Deletion\nRemoves all observations containing missing values\nVery simple, quick to implement\nSubstantial data loss, potential bias\n\n\nMean / Median / Mode\nReplaces missing values with a fixed statistic\nEasy to apply, preserves sample size\nReduces variance, distorts relationships\n\n\nLOCF (Last Observation Carried Forward)\nUses the last available value (mainly time series)\nUseful in longitudinal data, preserves continuity\nIgnores trends, underestimates variability\n\n\nLinear Interpolation\nEstimates missing values by connecting known data points\nMaintains trends, intuitive\nFails with sudden changes or nonlinear patterns\n\n\nKNN Imputation\nPredicts missing values using nearest neighbors\nPreserves multivariate structure, flexible\nComputationally expensive, sensitive to k choice\n\n\nMICE (Multiple Imputation by Chained Equations)\nIterative regression-based multiple imputation\nAccounts for uncertainty, widely used in research\nTime-consuming, requires expertise\n\n\nmissForest\nUses Random Forest to impute missing values\nHandles nonlinearities and interactions\nBlack-box method, computationally intensive\n\n\nEM Algorithm\nIterative expectation-maximization for likelihood-based estimation\nStatistically principled, robust in theory\nRequires strong assumptions, advanced knowledge\n\n\n\nNo single imputation method is universally optimal—each comes with trade-offs between simplicity, accuracy, and interpretability. For instance, listwise deletion is tempting for its ease but can heavily bias results if missingness is not random. Simple mean or median imputation keeps the dataset intact but artificially reduces variability and masks true correlations. More advanced techniques such as MICE, missForest, and EM provide statistically sound imputations that preserve uncertainty and relationships, but they demand more computational resources and methodological expertise.\nIn practice:\n\nExploratory analysis often starts with simple methods (e.g., median replacement) to get a sense of the data.\nTime series data may rely on LOCF or interpolation.\nComplex survey or clinical datasets typically benefit from advanced approaches like MICE or missForest, which better respect the multivariate nature of the data.\n\nUltimately, the choice depends on the data structure, missingness mechanism (MCAR, MAR, MNAR), and analytical goals."
  },
  {
    "objectID": "posts/2025-08-18_missing_values/index.html#conclusion",
    "href": "posts/2025-08-18_missing_values/index.html#conclusion",
    "title": "Handling Missing Data in R: A Comprehensive Guide",
    "section": "9 Conclusion",
    "text": "9 Conclusion\nThere is no one-size-fits-all solution for missing data. The right approach depends on your goal (prediction vs. inference), the missingness mechanism (MCAR/MAR/MNAR), your data structure (cross-sectional vs. longitudinal), and practical constraints (time, compute, expertise).\n\n9.1 What our NHANES walkthrough showed\n\nComplete-case analysis is simple but wastes data and can bias results unless MCAR is plausible.\nSingle imputation (mean/median, kNN, missForest run once) keeps all rows but underestimates uncertainty, yielding overconfident inferences.\nMultiple imputation (MICE) typically strikes the best balance for inference under MAR: it preserves multivariate structure and propagates uncertainty (via pooling), producing more honest standard errors and CIs.\nNonparametric imputers like missForest are strong for predictive accuracy on complex, nonlinear structure, but they do not capture imputation uncertainty by themselves.\n\n\n\n9.2 Practical guidance (decision-oriented)\n\nIf your main task is prediction and interpretability is secondary → a good single-imputation engine (e.g., missForest) can be effective, with careful validation.\nIf your main task is inference (effect sizes, CIs, p-values) and MAR is reasonable → prefer MICE; include strong predictors of both the outcome and missingness; check diagnostics.\nIf you suspect MNAR → acknowledge this explicitly and consider sensitivity analyses (pattern-mixture/selection models) rather than assuming MAR.\n\n\n\n9.3 Reporting checklist (make your analysis reproducible & credible)\n\n% missing by variable and by key subgroups (e.g., Age, Gender).\nYour assumed mechanism (MCAR/MAR/MNAR) and why it’s plausible.\nThe method(s) used (e.g., MICE with pmm, m, maxit, predictorMatrix; or missForest with ntree, maxiter).\nDiagnostics (trace/density/strip plots for MICE; OOB error for missForest).\nFor MI: pooled estimates with standard errors/intervals; clarify how pooling was performed.\nLimitations (e.g., potential MNAR, model misspecification, small-sample caveats).\n\n\n\n\n\n\n\nTip\n\n\n\nRule of thumb. Use simple methods for quick EDA; use MICE for publication-grade inference under MAR; use missForest when you primarily need strong predictive performance on mixed/complex data.\n\n\n\n\n9.4 Common pitfalls to avoid\n\nTreating imputed values as if they were observed “truth” (single imputation + significance testing).\nImputing the outcome itself (generally avoid; let it inform predictor imputations instead).\nIgnoring leakage: fit imputers within resampling folds/splits, not on the full data.\nOmitting key covariates that explain missingness (weakens the MAR assumption and the imputer).\n\n\n\n9.5 Where to go next\n\nLeakage-free pipelines with tidymodels::recipes (train/test split done right).\nSensitivity analyses for MNAR.\nRobustness checks (alternative imputation models, different m, predictor sets).\n\nBottom line: Choose methods intentionally, justify assumptions, show diagnostics, and report pooled results when using MI. Good missing-data practice is less about one magic function and more about transparent, principled workflow."
  },
  {
    "objectID": "posts/2025-08-18_missing_values/index.html#references",
    "href": "posts/2025-08-18_missing_values/index.html#references",
    "title": "Handling Missing Data in R: A Comprehensive Guide",
    "section": "10 References",
    "text": "10 References\n\nAllison, P. D. (2001). Missing Data. Sage Publications.\nEnders, C. K. (2010). Applied Missing Data Analysis. The Guilford Press.\nLittle, R. J. A., & Rubin, D. B. (2002). Statistical Analysis with Missing Data (2nd ed.). Wiley.\nvan Buuren, S. (2018). Flexible Imputation of Missing Data (2nd ed.). Chapman & Hall/CRC.\nStekhoven, D. J., & Bühlmann, P. (2012). “MissForest—Nonparametric Missing Value Imputation for Mixed-Type Data.” Bioinformatics, 28(1), 112–118. https://doi.org/10.1093/bioinformatics/btr597\nRubin, D. B. (1987). Multiple Imputation for Nonresponse in Surveys. Wiley.\nSchafer, J. L. (1997). Analysis of Incomplete Multivariate Data. Chapman & Hall/CRC.\nR Documentation: mice package\nR Documentation: missForest package"
  },
  {
    "objectID": "posts/2025-08-18_missing_values/index.html#nhanes-dataset",
    "href": "posts/2025-08-18_missing_values/index.html#nhanes-dataset",
    "title": "Handling Missing Data in R: A Comprehensive Guide",
    "section": "2 NHANES Dataset",
    "text": "2 NHANES Dataset\nIn this section, we will work with the NHANES dataset, which comes from the US National Health and Nutrition Examination Survey.\nThe dataset includes demographic, examination, and laboratory data collected from thousands of individuals.\nSince the full dataset is quite large, we will focus only on a subset of variables that are relevant for preprocessing examples.\nHere are the variables we will use:\n\nID: Unique identifier for each participant\nAge: Age of the participant\nGender: Biological sex (male or female)\nBMI: Body Mass Index\nBPSysAve: Average systolic blood pressure\nDiabetes: Whether the participant has been diagnosed with diabetes\n\nBefore diving into preprocessing, let’s take a quick look at the structure of these selected variables:\n\nlibrary(NHANES)\nlibrary(dplyr)\n\ndata(\"NHANES\")\n\n# Select relevant variables\nnhanes_sub &lt;- NHANES |&gt; \n  select(ID, Age, Gender, BMI, BPSysAve, Diabetes)\n\nglimpse(nhanes_sub)\n\nRows: 10,000\nColumns: 6\n$ ID       &lt;int&gt; 51624, 51624, 51624, 51625, 51630, 51638, 51646, 51647, 51647…\n$ Age      &lt;int&gt; 34, 34, 34, 4, 49, 9, 8, 45, 45, 45, 66, 58, 54, 10, 58, 50, …\n$ Gender   &lt;fct&gt; male, male, male, male, female, male, male, female, female, f…\n$ BMI      &lt;dbl&gt; 32.22, 32.22, 32.22, 15.30, 30.57, 16.82, 20.64, 27.24, 27.24…\n$ BPSysAve &lt;int&gt; 113, 113, 113, NA, 112, 86, 107, 118, 118, 118, 111, 104, 134…\n$ Diabetes &lt;fct&gt; No, No, No, No, No, No, No, No, No, No, No, No, No, No, No, N…"
  },
  {
    "objectID": "posts/2025-12-19_outliers/index.html#introduction",
    "href": "posts/2025-12-19_outliers/index.html#introduction",
    "title": "Outliers in Data Analysis: Detecting Extreme Values Before Modeling in R with İstanbul Airbnb Data",
    "section": "1 Introduction",
    "text": "1 Introduction\nData preprocessing is often presented as a sequence of technical steps. However, each preprocessing decision implicitly embeds a statistical assumption.\nIn a previous article, I discussed how missing observations can bias analysis if they are ignored or handled improperly:\nHandling Missing Data in R: A Comprehensive Guide\nThis article continues that discussion by focusing on outliers. Unlike missing values, outliers are observed data points. The challenge is not their absence, but their extremeness.\nUnderstanding whether an extreme value is informative or misleading is a crucial step before any modeling effort."
  },
  {
    "objectID": "posts/2025-12-19_outliers/index.html#why-outliers-matter",
    "href": "posts/2025-12-19_outliers/index.html#why-outliers-matter",
    "title": "Outliers in Data Analysis: Detecting Extreme Values Before Modeling in R with İstanbul Airbnb Data",
    "section": "2 Why Outliers Matter",
    "text": "2 Why Outliers Matter\nOutliers can affect statistical analysis in several fundamental ways:\n\nThey distort summary statistics such as the mean and standard deviation\nThey can dominate parameter estimates in regression models\nThey influence distance-based methods such as clustering\n\nMore importantly, outliers force analysts to confront a key question:\n\nAre we observing rare but valid behavior, or a deviation from the assumed data-generating process?"
  },
  {
    "objectID": "posts/2025-12-19_outliers/index.html#what-is-an-outlier",
    "href": "posts/2025-12-19_outliers/index.html#what-is-an-outlier",
    "title": "Outliers in Data Analysis: Detecting Extreme Values Before Modeling in R with İstanbul Airbnb Data",
    "section": "3 What Is an Outlier?",
    "text": "3 What Is an Outlier?\nInformally, an outlier is an observation that appears unusually large or small relative to the rest of the data. Formally, an outlier is an observation that is inconsistent with the bulk of the data under a given statistical model. Outliers are therefore not absolute objects. They depend on assumptions about distribution, scale, and structure."
  },
  {
    "objectID": "posts/2025-12-19_outliers/index.html#the-dataset-palmer-penguins",
    "href": "posts/2025-12-19_outliers/index.html#the-dataset-palmer-penguins",
    "title": "Outliers in Data Analysis: Detecting Extreme Values Before Modeling in R",
    "section": "4 The Dataset: Palmer Penguins",
    "text": "4 The Dataset: Palmer Penguins\nTo demonstrate outlier detection methods, we use the palmerpenguins dataset.\n\nlibrary(palmerpenguins)\nlibrary(dplyr)\n\ndata(penguins)\n\nThe dataset contains physical measurements of penguins observed in the Palmer Archipelago. In this article, we focus on the variable body_mass_g, measured in grams.\nThe dataset contains natural biological variability but does not exhibit extreme anomalies. This makes it suitable for illustrating detection logic rather than aggressive data cleaning."
  },
  {
    "objectID": "posts/2025-12-19_outliers/index.html#visual-inspection-the-starting-point",
    "href": "posts/2025-12-19_outliers/index.html#visual-inspection-the-starting-point",
    "title": "Outliers in Data Analysis: Detecting Extreme Values Before Modeling in R",
    "section": "5 Visual Inspection: The Starting Point",
    "text": "5 Visual Inspection: The Starting Point\nBefore applying formal rules, it is essential to examine the distribution visually.\n\nboxplot(\n  penguins$body_mass_g,\n  main = \"Body Mass Distribution\",\n  ylab = \"Body mass (g)\"\n)\n\n\n\n\n\n\n\n\n\nhist(\n  penguins$body_mass_g,\n  breaks = 30,\n  main = \"Histogram of Body Mass\",\n  xlab = \"Body mass (g)\"\n)\n\n\n\n\n\n\n\n\nVisual inspection highlights observations that lie far from the central mass of the data. At this stage, no decision is made. These plots serve as signals, not verdicts."
  },
  {
    "objectID": "posts/2025-12-19_outliers/index.html#the-iqr-method-a-robust-approach",
    "href": "posts/2025-12-19_outliers/index.html#the-iqr-method-a-robust-approach",
    "title": "Outliers in Data Analysis: Detecting Extreme Values Before Modeling in R",
    "section": "6 The IQR Method: A Robust Approach",
    "text": "6 The IQR Method: A Robust Approach\n\n6.1 Quartiles and the Interquartile Range\nQuartiles divide an ordered dataset into four equal parts.\n\nThe first quartile corresponds to the 25th percentile\n\nThe third quartile corresponds to the 75th percentile\n\nThe interquartile range (IQR) is defined as:\n\\[\n\\text{IQR} = Q_3 - Q_1\n\\]\nThe IQR measures the spread of the middle 50 percent of the data and is robust to extreme values.\n\n\n\n6.2 Defining Outliers Using the IQR Rule\nA commonly used rule defines outliers as observations lying outside the interval:\n\\[\n[ Q_1 - 1.5 \\times \\text{IQR}, \\; Q_3 + 1.5 \\times \\text{IQR} ]\n\\]\nThis rule does not rely on distributional assumptions and works well for skewed data.\n\n\n\n6.3 IQR-Based Detection in R\n\nQ1 &lt;- quantile(penguins$body_mass_g, 0.25, na.rm = TRUE)\nQ3 &lt;- quantile(penguins$body_mass_g, 0.75, na.rm = TRUE)\n\nIQR_value &lt;- Q3 - Q1\n\npenguins_iqr &lt;- penguins %&gt;%\n  mutate(\n    outlier_iqr =\n      body_mass_g &lt; Q1 - 1.5 * IQR_value |\n      body_mass_g &gt; Q3 + 1.5 * IQR_value\n  )\n\npenguins_iqr\n\n# A tibble: 344 × 9\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 3 more variables: sex &lt;fct&gt;, year &lt;int&gt;, outlier_iqr &lt;lgl&gt;\n\n\nThis step labels potential outliers instead of removing them."
  },
  {
    "objectID": "posts/2025-12-19_outliers/index.html#the-z-score-method",
    "href": "posts/2025-12-19_outliers/index.html#the-z-score-method",
    "title": "Outliers in Data Analysis: Detecting Extreme Values Before Modeling in R",
    "section": "7 The Z-Score Method",
    "text": "7 The Z-Score Method\n\n7.1 Definition of the Z-Score\nA Z-score measures how far an observation deviates from the mean in units of standard deviation.\nThe Z-score is defined as:\n\\[\nz = \\frac{x - \\mu}{\\sigma}\n\\]\nwhere:\n\n\\(\\mu\\) is the sample mean\n\\(\\sigma\\) is the sample standard deviation\n\nA common heuristic flags observations satisfying:\n\\[\n|z| &gt; 3\n\\]\n\n\n\n7.2 Assumptions and Limitations\nThe Z-score method assumes approximate symmetry and stable variance.\nExtreme observations inflate the mean and standard deviation, reducing their own Z-scores. As a result, Z-score methods are sensitive to the very values they aim to detect.\n\n\n\n7.3 Z-Score Detection in R\n\npenguins_z &lt;- penguins %&gt;%\n  mutate(\n    z_score = as.numeric(scale(body_mass_g)),\n    outlier_z = abs(z_score) &gt; 3\n  )\n\npenguins_z\n\n# A tibble: 344 × 10\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 4 more variables: sex &lt;fct&gt;, year &lt;int&gt;, z_score &lt;dbl&gt;, outlier_z &lt;lgl&gt;\n\n\nThe method is simple and intuitive but should not be used in isolation."
  },
  {
    "objectID": "posts/2025-12-19_outliers/index.html#comparing-iqr-and-z-score-methods",
    "href": "posts/2025-12-19_outliers/index.html#comparing-iqr-and-z-score-methods",
    "title": "Outliers in Data Analysis: Detecting Extreme Values Before Modeling in R",
    "section": "8 Comparing IQR and Z-Score Methods",
    "text": "8 Comparing IQR and Z-Score Methods\nDifferent methods often flag different observations.\n\nIQR is rank-based and robust\nZ-scores rely on the mean and variance\n\nRather than asking which method is correct, the more relevant question is:\n\nWhich assumptions are appropriate for this dataset and modeling goal?"
  },
  {
    "objectID": "posts/2025-12-19_outliers/index.html#should-outliers-be-removed",
    "href": "posts/2025-12-19_outliers/index.html#should-outliers-be-removed",
    "title": "Outliers in Data Analysis: Detecting Extreme Values Before Modeling in R with İstanbul Airbnb Data",
    "section": "7 Should Outliers Be Removed?",
    "text": "7 Should Outliers Be Removed?\nDetecting outliers does not imply that they should be automatically removed. Outlier detection is a diagnostic step, not a cleaning instruction.\nIn the context of Airbnb price data, many extreme values correspond to luxury properties, large homes, or special accommodation types. Blindly removing such observations may erase precisely the information that makes the data interesting.\nInstead, several alternative strategies should be considered.\n\n7.1 Verify and understand the source of extremeness\nThe first question should always be why an observation is extreme.\n\nIs the listing a luxury property?\nDoes it belong to a specific room_type?\nIs it located in a high-demand neighborhood?\nIs it associated with unusual booking constraints (e.g., very high minimum_nights)?\n\nIn many cases, extreme values are valid reflections of heterogeneity, not data errors.\n\n\n7.2 Use transformations or robust methods\nWhen extreme values distort summaries or model estimates, removal is not the only option.\nCommon alternatives include:\n\ntransforming the response variable (e.g., log transformation of prices),\nusing robust estimators that reduce sensitivity to extremes,\nmodeling medians or quantiles instead of means.\n\nThese approaches preserve information while reducing the influence of extreme observations.\n\n\n7.3 Model extremes explicitly when relevant\nIn some applications, outliers are not nuisances but the primary object of interest.\nExamples include:\n\nluxury market analysis,\nrisk assessment,\nrare but high-impact events.\n\nIn such cases, extreme observations should be modeled explicitly rather than suppressed.\n\n\n7.4 A final perspective\nOutliers are not merely statistical inconveniences. They often highlight structural differences, market segmentation, or meaningful departures from typical behavior.\nUnderstanding why an observation is extreme is frequently more informative than deleting it.\nIn practice, thoughtful outlier handling requires a balance between statistical rules, domain knowledge, and modeling objectives."
  },
  {
    "objectID": "posts/2025-12-19_outliers/index.html#final-remarks",
    "href": "posts/2025-12-19_outliers/index.html#final-remarks",
    "title": "Outliers in Data Analysis: Detecting Extreme Values Before Modeling in R with İstanbul Airbnb Data",
    "section": "8 Final Remarks",
    "text": "8 Final Remarks\nOutlier detection is a natural step in the data preprocessing workflow. It typically follows missing data analysis and precedes scaling, transformation, or model fitting.\nIn this article, the focus was not on eliminating extreme values, but on understanding why they occur. Through visual exploration, context-aware analysis using room_type, and formal detection rules such as the IQR method and Z-scores, we demonstrated that extreme values are not noise by default.\nIn many real-world datasets, especially those involving prices or economic behavior, extreme values reflect structural heterogeneity rather than data quality issues. Treating them blindly as errors risks discarding meaningful information.\nOutliers should therefore be approached as questions posed by the data: Why is this observation extreme? Does it represent a different regime, a rare event, or a distinct subgroup?\nAnswering these questions requires a combination of statistical tools, domain knowledge, and clear analytical goals. When handled thoughtfully, outlier analysis enhances both the robustness and the interpretability of downstream models."
  },
  {
    "objectID": "posts/2025-12-19_outliers/index.html#references-and-further-reading",
    "href": "posts/2025-12-19_outliers/index.html#references-and-further-reading",
    "title": "Outliers in Data Analysis: Detecting Extreme Values Before Modeling in R with İstanbul Airbnb Data",
    "section": "9 References and Further Reading",
    "text": "9 References and Further Reading\n\nTukey, J. W. (1977). Exploratory Data Analysis. Addison-Wesley.\n(Foundational reference for boxplots, IQR, and exploratory thinking.)\nHastie, T., Tibshirani, R., Friedman, J. (2009). The Elements of Statistical Learning. Springer.\n(Statistical foundations and the role of robust methods in modeling.)\nJames, G., Witten, D., Hastie, T., Tibshirani, R. (2021). An Introduction to Statistical Learning. Springer.\n(Accessible discussion of preprocessing, transformations, and practical modeling considerations.)\nNIST/SEMATECH e-Handbook of Statistical Methods – Outliers\nhttps://www.itl.nist.gov/div898/handbook/eda/section3/eda35h.htm\n(Authoritative overview of outlier concepts and detection methods.)\nWickham, H., Grolemund, G. (2017). R for Data Science. O’Reilly Media.\n(Practical guidance on data exploration, visualization, and preprocessing workflows in R.)\nInside Airbnb – Get the Data\nhttps://insideairbnb.com/get-the-data/\n(Data source used in this article; city-level Airbnb listings data.)\nWickham, H. (2016). ggplot2: Elegant Graphics for Data Analysis. Springer.\n(Principles of layered graphics and effective visualization used throughout the article.)"
  },
  {
    "objectID": "posts/2025-12-19_outliers/index.html#the-dataset-inside-airbnb-listings-istanbul",
    "href": "posts/2025-12-19_outliers/index.html#the-dataset-inside-airbnb-listings-istanbul",
    "title": "Outliers in Data Analysis: Detecting Extreme Values Before Modeling in R with İstanbul Airbnb Data",
    "section": "4 The Dataset: Inside Airbnb Listings (Istanbul)",
    "text": "4 The Dataset: Inside Airbnb Listings (Istanbul)\nTo demonstrate outlier detection methods, we will use Inside Airbnb listings data. Inside Airbnb is a mission-driven project that publishes datasets scraped from publicly available Airbnb listing pages and provides city-level downloads for research and analysis.\nIn this article, we will work with the detailed listings file:\n\nlistings.csv.gz (detailed listing-level data; typically rich and feature-complete)\n\nYou can download the dataset from the Inside Airbnb “Get the Data” page (choose a city and download Detailed Listings data).\n\n4.1 Why this dataset is ideal for outlier detection\nUnlike many “clean” educational datasets, Airbnb listing data often contains genuinely extreme values, especially in price. These extremes are not necessarily errors—luxury properties exist—but they can heavily distort means, variances, and model estimates. That makes Airbnb listings a realistic and highly instructive dataset for outlier detection.\n\n\n4.2 Variables we will use\nAlthough the Airbnb listings dataset contains many variables, this article focuses on a small, purpose-driven subset.\nOur primary variable of interest is:\n\nprice (converted to price_num): nightly listing price.\nThis variable is typically right-skewed and often contains extreme values, making it ideal for illustrating outlier detection methods.\n\nTo provide context for interpreting extreme prices, we also retain a limited number of supporting variables:\n\nminimum_nights: minimum stay requirement, which can occasionally take unusually large values\nnumber_of_reviews: a proxy for listing activity and popularity, often zero-inflated\nroom_type: categorical variable indicating the type of accommodation\nneighbourhood_cleansed: cleaned neighborhood label, useful for geographic context\n\nThese additional variables are not used to detect outliers directly, but to interpret and explain them once identified.\n\n\n4.3 Loading the data in R\nBelow is an example using Istanbul. If you prefer a different city, replace the URL with the corresponding listings.csv.gz link from Inside Airbnb.\n\nlibrary(dplyr)\nlibrary(readr)\nlibrary(stringr)\n\n# Example URL (Istanbul). You can get the latest link from Inside Airbnb \"Get the Data\".\n# The URL structure typically follows:\n# https://data.insideairbnb.com/turkey/marmara/istanbul/2025-09-29/data/listings.csv.gz\n\nlistings_raw &lt;- read_csv(\n  \"listings.csv.gz\",\n  show_col_types = FALSE\n)\n\nvars_keep &lt;- c(\n  \"id\", \"name\",\n  \"price\", \"minimum_nights\", \"number_of_reviews\",\n  \"room_type\", \"neighbourhood_cleansed\"\n)\n\nlistings_small &lt;- listings_raw %&gt;%\n  select(any_of(vars_keep))\n\n\n\n4.4 Inspecting the selected variables\nBefore performing any transformation, it is important to inspect the data as it comes from the source. This allows us to understand variable types and identify potential issues early.\nBelow, we examine only the variables selected for this article.\n\nglimpse(listings_small)\n\nRows: 30,051\nColumns: 7\n$ id                     &lt;dbl&gt; 1.342043e+18, 1.342082e+18, 1.342211e+18, 1.342…\n$ name                   &lt;chr&gt; \"Отдельная квартира на Фатих(Балат).\", \"Blue st…\n$ price                  &lt;chr&gt; \"$2,290.00\", \"$1,101.00\", \"$3,430.00\", \"$3,178.…\n$ minimum_nights         &lt;dbl&gt; 5, 7, 2, 100, 1, 100, 1, 5, 2, 100, 100, 2, 100…\n$ number_of_reviews      &lt;dbl&gt; 4, 4, 26, 1, 2, 0, 41, 0, 19, 0, 0, 26, 0, 0, 1…\n$ room_type              &lt;chr&gt; \"Entire home/apt\", \"Private room\", \"Entire home…\n$ neighbourhood_cleansed &lt;chr&gt; \"Fatih\", \"Beyoglu\", \"Beyoglu\", \"Sisli\", \"Sisli\"…\n\n\nAt this stage, notice in particular the price variable. Although it represents a numerical concept (nightly price), it is not stored as a numeric variable.\nInstead, price is typically read as a character string, often containing currency symbols and separators. This is common in datasets that originate from web scraping or user-facing platforms.\n\n\n4.5 Why we need to convert price to numeric\nOutlier detection methods such as boxplots, the IQR rule, and Z-scores require numeric input. As long as price is stored as a character variable, it cannot be used in quantitative analysis.\nMore importantly, treating price as numeric is not just a technical requirement. It reflects a modeling decision: we explicitly state that this variable represents a measurable quantity on which arithmetic operations are meaningful.\n\n\n4.6 Converting price to a numeric variable\nTo prepare the data for analysis, we remove non-numeric characters and convert price to a numeric variable, which we call price_num.\n\nlistings_small &lt;- listings_small %&gt;%\n  mutate(price_num = price %&gt;%\n           str_replace_all(\"[^0-9.]\", \"\") %&gt;%\n           as.numeric())\n\nAfter the conversion, we can verify the result by inspecting basic summaries:\n\nsummary(listings_small$price_num)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n     80    1644    2538    5084    4108 4437598    4803 \n\n\nAt this point, price_num is ready for outlier detection and visualization. In the next section, we will use this variable to illustrate how extreme values can be identified using visual tools and formal statistical rules."
  },
  {
    "objectID": "posts/2025-12-19_outliers/index.html#visualizing-price-distributions-and-potential-outliers",
    "href": "posts/2025-12-19_outliers/index.html#visualizing-price-distributions-and-potential-outliers",
    "title": "Outliers in Data Analysis: Detecting Extreme Values Before Modeling in R with İstanbul Airbnb Data",
    "section": "5 Visualizing Price Distributions and Potential Outliers",
    "text": "5 Visualizing Price Distributions and Potential Outliers\nBefore applying any formal outlier detection rule, it is good practice to explore the distribution of the variable visually. Visualization helps us understand the shape, spread, and asymmetry of the data, and often reveals extreme values immediately.\nIn this section, we focus on the numeric price variable price_num.\n\n5.1 A first attempt: why the raw histogram fails\nA natural first step is to plot a histogram of nightly prices on the original scale.\n\nlibrary(ggplot2)\nlibrary(scales)\n\nggplot(listings_small, aes(x = price_num)) +\n  geom_histogram(bins = 40, fill = \"#B0B0B0\", color = \"white\") +\n  scale_x_continuous(labels = label_number(big.mark = \",\")) +\n  labs(\n    title = \"Distribution of Nightly Prices (raw scale)\",\n    x = \"Nightly price\",\n    y = \"Count\"\n  ) +\n  theme_minimal(base_size = 13)\n\n\n\n\n\n\n\n\nInterpretation\nThis plot is technically correct, but analytically unhelpful.\n\nA small number of extremely expensive listings stretches the x-axis.\nThe majority of observations are compressed near zero.\nAs a result, the internal structure of the data becomes almost invisible.\n\nThis is not a plotting mistake. It is a direct consequence of heavy right-skewness, which is common in price data. At this point, it is already clear that naive visualizations on the raw scale are insufficient.\n\n\n5.2 Adding context: prices depend on room_type\nAirbnb listings are not drawn from a single homogeneous market. A shared room and an entire home/apt represent fundamentally different accommodation types, and their prices should not be expected to follow the same distribution.\nIf we ignore this context and search for outliers globally, we risk labeling valid group-level differences as anomalies. For this reason, we first examine how prices behave within each room type.\n\nlistings_small %&gt;%\n  count(room_type, sort = TRUE)\n\n# A tibble: 4 × 2\n  room_type           n\n  &lt;chr&gt;           &lt;int&gt;\n1 Entire home/apt 20243\n2 Private room     9494\n3 Hotel room        157\n4 Shared room       157\n\n\n\n\n5.3 Price distributions by room type (log scale)\nTo make the right tail interpretable without discarding extreme values, we visualize prices on a logarithmic scale and separate distributions by room type.\n\nggplot(listings_small, aes(x = price_num)) +\ngeom_histogram(bins = 35, fill = \"#4C72B0\", color = \"white\", alpha = 0.9) +\nscale_x_log10(\nbreaks = log_breaks(n = 6),\nlabels = label_number(big.mark = \",\")\n) +\nfacet_wrap(~ room_type, scales = \"free_y\") +\nlabs(\ntitle = \"Nightly Price Distributions by Room Type (log scale)\",\nsubtitle = \"Log scale improves readability in heavily right-skewed price data\",\nx = \"Nightly price (log scale)\",\ny = \"Count\"\n) +\ntheme_minimal(base_size = 13)\n\n\n\n\n\n\n\n\nInterpretation\nThis visualization reveals several important patterns:\n\nEach room type has its own characteristic price range.\nThe extreme right tail becomes visible without overwhelming the plot.\nWhat appears as an “outlier” globally may be perfectly typical within a given room type\n\nAt this stage, the notion of an outlier becomes context-dependent rather than absolute.\n\n\n5.4 Boxplots by room type: highlighting potential extremes\nHistograms show overall shape, but boxplots are better suited for highlighting extreme observations. We again use a log scale to preserve readability.\n\nggplot(listings_small, aes(x = room_type, y = price_num)) +\n  geom_boxplot(outlier.alpha = 0.35, fill = \"#DDDDDD\") +\n  scale_y_log10(labels = label_number(big.mark = \",\")) +\n  labs(\n    title = \"Nightly Prices by Room Type (boxplot, log scale)\",\n    subtitle = \"Potential outliers are assessed within each room type\",\n    x = \"Room type\",\n    y = \"Nightly price (log scale)\"\n  ) +\n  theme_minimal(base_size = 13) +\n  theme(axis.text.x = element_text(angle = 20, hjust = 1))\n\n\n\n\n\n\n\n\nInterpretation\nThis plot makes a key point explicit:\n\nOutliers are flagged relative to their own room type, not the entire dataset.\nExtremely high prices within shared rooms are statistically more unusual than similarly high prices within entire homes, given the much narrower price distribution of shared rooms.\nStatistical outliers are candidates for further investigation, not automatic deletions.\n\n\n\n5.5 What visual exploration tells us\nFrom visual inspection alone, we can conclude that:\n\nAirbnb price data are highly right-skewed.\nExtreme values exist and strongly influence scale and summaries.\nContext (here, room_type) is essential for meaningful interpretation.\n\nThese observations motivate the next step: formalizing outlier detection using statistical rules such as the IQR method and Z-scores, applied within room types rather than globally."
  },
  {
    "objectID": "posts/2025-12-19_outliers/index.html#formal-outlier-detection-within-room-type",
    "href": "posts/2025-12-19_outliers/index.html#formal-outlier-detection-within-room-type",
    "title": "Outliers in Data Analysis: Detecting Extreme Values Before Modeling in R with İstanbul Airbnb Data",
    "section": "6 Formal Outlier Detection Within Room Type",
    "text": "6 Formal Outlier Detection Within Room Type\nVisual exploration suggested that nightly prices exhibit strong right-skewness and that extreme values should be interpreted within the context of room_type. In this section, we formalize that intuition using statistical outlier detection rules.\nOur goal is not to mechanically remove observations, but to identify and examine listings whose prices are unusually high relative to their own room type.\n\n6.1 The IQR rule\nThe Interquartile Range (IQR) rule defines outliers based on the spread of the middle 50% of the data. For a given variable, the IQR is defined as:\n\\[\n\\text{IQR} = Q_3 - Q_1\n\\]\nAn observation is flagged as a potential outlier if it lies outside the interval:\n\\[\n[ Q_1 - 1.5 \\times \\text{IQR}, \\; Q_3 + 1.5 \\times \\text{IQR} ]\n\\]\nBecause the IQR relies on quantiles rather than the mean and standard deviation, it is relatively robust to skewed distributions—an important property for price data.\n\n\n6.2 Applying the IQR rule within each room type\nInstead of computing a single global IQR, we apply the rule separately within each room type. This ensures that prices are evaluated relative to comparable listings.\n\noutliers_iqr &lt;- listings_small %&gt;%\n  group_by(room_type) %&gt;%\n  mutate(\n    Q1 = quantile(price_num, 0.25, na.rm = TRUE),\n    Q3 = quantile(price_num, 0.75, na.rm = TRUE),\n    IQR_value = Q3 - Q1,\n    lower_bound = Q1 - 1.5 * IQR_value,\n    upper_bound = Q3 + 1.5 * IQR_value,\n    outlier_iqr = price_num &lt; lower_bound | price_num &gt; upper_bound\n  ) %&gt;%\n  ungroup()\n\nAt this stage, each listing is labeled according to whether its price is considered an outlier within its own room type.\n\n\n6.3 How many outliers do we detect?\nBefore inspecting individual listings, it is informative to summarize how many outliers are flagged in each group.\n\noutliers_iqr %&gt;%\n  count(room_type, outlier_iqr) %&gt;%\n  arrange(room_type, desc(outlier_iqr))\n\n# A tibble: 12 × 3\n   room_type       outlier_iqr     n\n   &lt;chr&gt;           &lt;lgl&gt;       &lt;int&gt;\n 1 Entire home/apt TRUE         1458\n 2 Entire home/apt FALSE       16616\n 3 Entire home/apt NA           2169\n 4 Hotel room      TRUE           17\n 5 Hotel room      FALSE         106\n 6 Hotel room      NA             34\n 7 Private room    TRUE          441\n 8 Private room    FALSE        6470\n 9 Private room    NA           2583\n10 Shared room     TRUE            3\n11 Shared room     FALSE         137\n12 Shared room     NA             17\n\n\nInterpretation\nThis table shows that outliers are not evenly distributed across room types. Some categories naturally exhibit greater price dispersion, which leads to more listings being flagged as potential outliers. This reinforces the importance of group-aware detection.\n\n\n6.4 Inspecting extreme cases flagged by IQR\nStatistical flags become meaningful only when we inspect the actual observations. Below, we list the most expensive listings flagged as outliers within each room type.\n\ntop_price_outliers &lt;- outliers_iqr %&gt;%\n  filter(outlier_iqr) %&gt;%\n  group_by(room_type) %&gt;%\n  arrange(desc(price_num)) %&gt;%\n  slice_head(n = 5) %&gt;%\n  ungroup() %&gt;%\n  select(\n    room_type,\n    price,\n    price_num,\n    minimum_nights,\n    number_of_reviews,\n    neighbourhood_cleansed,\n    name\n  )\n\ntop_price_outliers\n\n# A tibble: 18 × 7\n   room_type       price         price_num minimum_nights number_of_reviews\n   &lt;chr&gt;           &lt;chr&gt;             &lt;dbl&gt;          &lt;dbl&gt;             &lt;dbl&gt;\n 1 Entire home/apt $4,437,598.00   4437598            100                14\n 2 Entire home/apt $2,658,600.00   2658600            100                 3\n 3 Entire home/apt $2,109,690.00   2109690            100                 0\n 4 Entire home/apt $2,000,000.00   2000000            100                 0\n 5 Entire home/apt $1,250,008.00   1250008            100                 0\n 6 Hotel room      $2,439,497.00   2439497              1                 0\n 7 Hotel room      $2,439,497.00   2439497              1                 0\n 8 Hotel room      $2,439,497.00   2439497              1                 0\n 9 Hotel room      $2,439,497.00   2439497              1                 0\n10 Hotel room      $2,433,427.00   2433427              1                 0\n11 Private room    $390,271.00      390271            365                 1\n12 Private room    $390,271.00      390271            365                 0\n13 Private room    $390,271.00      390271            365                 0\n14 Private room    $390,271.00      390271            100                 0\n15 Private room    $390,271.00      390271            100                 0\n16 Shared room     $7,221.00          7221              1                 0\n17 Shared room     $6,086.00          6086              1                 2\n18 Shared room     $5,841.00          5841            100                 0\n# ℹ 2 more variables: neighbourhood_cleansed &lt;chr&gt;, name &lt;chr&gt;\n\n\nInterpretation\nAt this point, the analysis moves from abstract rules to concrete questions:\n\nAre these listings luxury properties?\nDo they require unusually long minimum stays?\nDo they have very few (or no) reviews, suggesting new or inactive listings?\nAre they located in specific neighborhoods?\n\nThe answers to these questions determine whether a flagged observation should be:\n\nkept and modeled explicitly,\ntransformed (e.g., via log scaling),\nor excluded due to data quality concerns.\n\n\n\n6.5 Z-score–based outlier detection: concept and limitations\nIn addition to IQR-based rules, outliers are often discussed using Z-scores. Because this method is widely taught and frequently applied, it is important to understand both how it works and when it can be misleading.\n\n6.5.1 What is a Z-score?\nA Z-score measures how far an observation lies from the mean, expressed in units of standard deviation. For a single observation, the Z-score is defined as:\n\\[\nz = \\frac{x - \\mu}{\\sigma}\n\\]\nwhere:\n\n\\(\\mu\\) is the sample mean\n\\(\\sigma\\) is the sample standard deviation\n\nIntuitively, the Z-score answers the question:\n\n“How many standard deviations away from the mean is this observation?”\n\nA common heuristic labels observations with\n\\[\n|z| &gt; 3\n\\]\nas potential outliers.\n\n\n6.5.2 What does the Z-score assume?\nZ-score–based detection implicitly relies on several assumptions:\n\nthe distribution is approximately symmetric,\nthe mean and standard deviation are meaningful summaries,\nextreme values do not dominate the estimation of \\(\\mu\\) and \\(\\sigma\\).\n\nThese assumptions are often reasonable for approximately normal data, but they are problematic for strongly skewed distributions.\n\n\n6.5.3 Why Z-scores are problematic for price data\nAirbnb prices are typically right-skewed with long upper tails. In such cases:\n\nextreme values inflate the mean,\nextreme values inflate the standard deviation,\nas a result, truly extreme observations may receive moderate Z-scores.\n\nThis leads to a paradox: the very observations we want to detect reduce their own apparent extremeness. For this reason, Z-scores tend to under-detect outliers in heavily skewed economic data.\n\n\n6.5.4 Applying Z-scores within each room type\nDespite these limitations, Z-scores can still be informative when used carefully and comparatively. As with the IQR rule, we compute Z-scores within each room type to preserve contextual meaning.\n\noutliers_z &lt;- listings_small %&gt;%\n  group_by(room_type) %&gt;%\n  mutate(\n    mean_price = mean(price_num, na.rm = TRUE),\n    sd_price   = sd(price_num, na.rm = TRUE),\n    z_price    = (price_num - mean_price) / sd_price,\n    outlier_z  = abs(z_price) &gt; 3\n  ) %&gt;%\n  ungroup()\n\n\n\n6.5.5 How many outliers are flagged by the Z-score rule?\nAfter computing Z-scores within each room type, we can summarize how many listings are flagged as outliers.\n\noutliers_z %&gt;%\n  count(room_type, outlier_z) %&gt;%\n  arrange(room_type, desc(outlier_z))\n\n# A tibble: 12 × 3\n   room_type       outlier_z     n\n   &lt;chr&gt;           &lt;lgl&gt;     &lt;int&gt;\n 1 Entire home/apt TRUE         24\n 2 Entire home/apt FALSE     18050\n 3 Entire home/apt NA         2169\n 4 Hotel room      TRUE          5\n 5 Hotel room      FALSE       118\n 6 Hotel room      NA           34\n 7 Private room    TRUE         26\n 8 Private room    FALSE      6885\n 9 Private room    NA         2583\n10 Shared room     TRUE          1\n11 Shared room     FALSE       139\n12 Shared room     NA           17\n\n\nInterpretation\nIn many Airbnb datasets, this table reveals a striking pattern:\n\nThe number of Z-score–based outliers is much smaller than the number detected by the IQR rule.\nIn some room types, no observations are flagged at all.\n\nThis is a direct consequence of right-skewness: extreme prices inflate both the mean and the standard deviation, making Z-scores appear less extreme than expected.\n\n\n6.5.6 Inspecting listings flagged by Z-scores\nTo understand what Z-scores actually flag as outliers, we inspect the most extreme listings according to their Z-score values.\n\ntop_z_outliers &lt;- outliers_z %&gt;%\n  filter(outlier_z) %&gt;%\n  group_by(room_type) %&gt;%\n  arrange(desc(abs(z_price))) %&gt;%\n  slice_head(n = 5) %&gt;%\n  ungroup() %&gt;%\n  select(\n    room_type,\n    price,\n    price_num,\n    z_price,\n    minimum_nights,\n    number_of_reviews,\n    neighbourhood_cleansed,\n    name\n  )\n\ntop_z_outliers\n\n# A tibble: 16 × 8\n   room_type       price      price_num z_price minimum_nights number_of_reviews\n   &lt;chr&gt;           &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;          &lt;dbl&gt;             &lt;dbl&gt;\n 1 Entire home/apt $4,437,59…   4437598   92.9             100                14\n 2 Entire home/apt $2,658,60…   2658600   55.6             100                 3\n 3 Entire home/apt $2,109,69…   2109690   44.1             100                 0\n 4 Entire home/apt $2,000,00…   2000000   41.8             100                 0\n 5 Entire home/apt $1,250,00…   1250008   26.1             100                 0\n 6 Hotel room      $2,439,49…   2439497    4.84              1                 0\n 7 Hotel room      $2,439,49…   2439497    4.84              1                 0\n 8 Hotel room      $2,439,49…   2439497    4.84              1                 0\n 9 Hotel room      $2,439,49…   2439497    4.84              1                 0\n10 Hotel room      $2,433,42…   2433427    4.83              1                 0\n11 Private room    $390,271.…    390271   31.3             365                 1\n12 Private room    $390,271.…    390271   31.3             365                 0\n13 Private room    $390,271.…    390271   31.3             365                 0\n14 Private room    $390,271.…    390271   31.3             100                 0\n15 Private room    $390,271.…    390271   31.3             100                 0\n16 Shared room     $7,221.00       7221    3.65              1                 0\n# ℹ 2 more variables: neighbourhood_cleansed &lt;chr&gt;, name &lt;chr&gt;\n\n\nInterpretation\nWhen compared to the IQR-based outliers, these listings are often:\n\nless extreme in absolute price,\ncloser to the central mass of the distribution,\ndominated by a small number of room types.\n\nThis confirms that Z-score–based detection tends to miss many extreme but valid prices in heavily skewed data.\n\n\n6.5.7 Comparing IQR and Z-score results\nFinally, we compare how many listings are flagged by each method.\n\ncomparison_summary &lt;- outliers_iqr %&gt;%\n  select(id, room_type, outlier_iqr) %&gt;%\n  left_join(\n    outliers_z %&gt;% select(id, outlier_z),\n    by = \"id\"\n  ) %&gt;%\n  count(outlier_iqr, outlier_z)\n\ncomparison_summary\n\n# A tibble: 4 × 3\n  outlier_iqr outlier_z     n\n  &lt;lgl&gt;       &lt;lgl&gt;     &lt;int&gt;\n1 FALSE       FALSE     23329\n2 TRUE        FALSE      1863\n3 TRUE        TRUE         56\n4 NA          NA         4803\n\n\nInterpretation\nThis comparison highlights a key methodological insight:\n\nMany listings flagged by the IQR rule are not flagged by Z-scores.\nListings flagged by Z-scores are almost always flagged by the IQR rule as well.\nThe overlap is asymmetric.\n\nIn other words, the Z-score rule is more conservative and may under-detect outliers when distributions are strongly skewed. This does not make Z-scores “wrong”, but it does limit their usefulness as a primary detection method for price data.\nIn practice, Z-score–based flags often differ substantially from IQR-based flags. This difference is not an error—it reflects different assumptions.\n\nIQR-based methods rely on ranks and quantiles\nZ-score–based methods rely on moments (mean and variance)\n\nFor heavily skewed price data, IQR-based detection is usually more reliable, while Z-scores should be interpreted as a supplementary diagnostic rather than a primary rule."
  },
  {
    "objectID": "posts/2025-12-26_import_export/index.html#introduction",
    "href": "posts/2025-12-26_import_export/index.html#introduction",
    "title": "Understanding Data Import and Export in R: Working with CSV and Excel Files",
    "section": "1 Introduction",
    "text": "1 Introduction\nWhen learning R, most people focus on functions, models, and visualizations. However, many real-world problems start much earlier — at the data import stage — and end much later — with exporting results.\nIf data is read incorrectly, no statistical method can save the analysis.\nIn this post, we focus on the logic of data import and export in R, using CSV and Excel files. Rather than memorizing functions, we build a mental model for how R interacts with files."
  },
  {
    "objectID": "posts/2025-12-26_import_export/index.html#why-data-import-and-export-matters",
    "href": "posts/2025-12-26_import_export/index.html#why-data-import-and-export-matters",
    "title": "Understanding Data Import and Export in R: Working with CSV and Excel Files",
    "section": "2 Why Data Import and Export Matters",
    "text": "2 Why Data Import and Export Matters\nData analysis is a workflow:\nData source → Import → Analysis → Results → Export → Sharing\nErrors often occur at the import stage:\n\nwrong delimiters,\nincorrect decimal separators,\nincorrect file paths,\nsilently converted data types.\n\nThe result?\nA model that runs perfectly — on the wrong data."
  },
  {
    "objectID": "posts/2025-12-26_import_export/index.html#csv-vs-excel-not-a-competition",
    "href": "posts/2025-12-26_import_export/index.html#csv-vs-excel-not-a-competition",
    "title": "Understanding Data Import and Export in R: Working with CSV and Excel Files",
    "section": "3 CSV vs Excel: Not a Competition",
    "text": "3 CSV vs Excel: Not a Competition\nBefore touching R, we should clarify the difference between file formats.\n\n3.1 CSV Files\n\nPlain text files\nLightweight and fast\nUniversally supported\nOne table per file\nNo formatting, only data\n\nExample:\ntotal_bill,tip,sex\n16.99,1.01,Female\n\n\n3.2 Excel Files\n\nBinary format (.xlsx)\nCan contain multiple sheets\nStore structure and presentation together\nWidely used for reporting and sharing\n\nKey idea:\nCSV is a data transport format.\nExcel is a communication format."
  },
  {
    "objectID": "posts/2025-12-26_import_export/index.html#working-directory-where-r-actually-looks",
    "href": "posts/2025-12-26_import_export/index.html#working-directory-where-r-actually-looks",
    "title": "Understanding Data Import and Export in R: Working with CSV and Excel Files",
    "section": "4 Working Directory: Where R Actually Looks",
    "text": "4 Working Directory: Where R Actually Looks\nOne of the most common beginner mistakes has nothing to do with R syntax.\nR does not search your entire computer for files. It only looks inside its working directory.\n\ngetwd()\n\nThis command shows where R is currently looking.\nIf a file exists on your computer but not in this directory, R behaves as if the file does not exist.\nThis is why errors like:\ncannot open the connection\nusually indicate a path problem, not a coding problem."
  },
  {
    "objectID": "posts/2025-12-26_import_export/index.html#the-example-dataset-tips",
    "href": "posts/2025-12-26_import_export/index.html#the-example-dataset-tips",
    "title": "Understanding Data Import and Export in R: Working with CSV and Excel Files",
    "section": "5 The Example Dataset: tips",
    "text": "5 The Example Dataset: tips\nThroughout this post, we use a single dataset: tips.\n\nRestaurant tipping data\nSmall and easy to understand\nContains numeric and categorical variables\nIdeal for demonstrating import/export logic\n\nData source:\nhttps://raw.githubusercontent.com/mwaskom/seaborn-data/master/tips.csv"
  },
  {
    "objectID": "posts/2025-12-26_import_export/index.html#reading-csv-files-the-core-logic",
    "href": "posts/2025-12-26_import_export/index.html#reading-csv-files-the-core-logic",
    "title": "Understanding Data Import and Export in R: Working with CSV and Excel Files",
    "section": "6 Reading CSV Files: The Core Logic",
    "text": "6 Reading CSV Files: The Core Logic\nWhen R reads a CSV file, it needs answers to four questions:\n\nHow are columns separated?\nIs the first row a header?\nWhat is the decimal separator?\nHow should text be interpreted?\n\nThese answers are provided via function arguments."
  },
  {
    "objectID": "posts/2025-12-26_import_export/index.html#read.table-the-foundation",
    "href": "posts/2025-12-26_import_export/index.html#read.table-the-foundation",
    "title": "Understanding Data Import and Export in R: Working with CSV and Excel Files",
    "section": "7 read.table(): The Foundation",
    "text": "7 read.table(): The Foundation\nAll CSV-reading functions in base R are built on read.table().\n\ntips &lt;- read.table(\n  file = \"tips.csv\",\n  header = TRUE,\n  sep = \",\",\n  dec = \".\",\n  stringsAsFactors = FALSE\n)\n\nUnderstanding this function means understanding CSV import in R."
  },
  {
    "objectID": "posts/2025-12-26_import_export/index.html#read.csv-and-its-assumptions",
    "href": "posts/2025-12-26_import_export/index.html#read.csv-and-its-assumptions",
    "title": "Understanding Data Import and Export in R: Working with CSV and Excel Files",
    "section": "8 read.csv() and Its Assumptions",
    "text": "8 read.csv() and Its Assumptions\nread.csv() is simply a shortcut for a common case:\n\nColumns separated by commas\nDecimal separator is a dot\n\n\ntips &lt;- read.csv(\"tips.csv\")\n\nThis works perfectly — if the assumptions match the file.\nThe dangerous part? R may not throw an error even if the assumptions are wrong.\n\nThe most dangerous errors are silent ones."
  },
  {
    "objectID": "posts/2025-12-26_import_export/index.html#read.csv2-and-regional-differences",
    "href": "posts/2025-12-26_import_export/index.html#read.csv2-and-regional-differences",
    "title": "Understanding Data Import and Export in R: Working with CSV and Excel Files",
    "section": "9 read.csv2() and Regional Differences",
    "text": "9 read.csv2() and Regional Differences\nIn many European datasets:\n\nColumns are separated by semicolons\nDecimals use commas\n\ntotal_bill;tip;sex\n16,99;1,01;Female\nFor this structure, read.csv2() is designed.\n\ntips2 &lt;- read.csv2(\"tips_semicolon.csv\")\n\nImportant nuance:\nEven if decimals use dots, read.csv2() may still work in some cases — but this is not guaranteed.\nCorrect approach:\n\nAlways inspect the file structure before choosing the function."
  },
  {
    "objectID": "posts/2025-12-26_import_export/index.html#writing-csv-files-from-r",
    "href": "posts/2025-12-26_import_export/index.html#writing-csv-files-from-r",
    "title": "Understanding Data Import and Export in R: Working with CSV and Excel Files",
    "section": "10 Writing CSV Files from R",
    "text": "10 Writing CSV Files from R\nData analysis rarely ends in R. Results are shared as files.\n\n10.1 Writing comma-separated CSV\n\nwrite.csv(tips, \"tips_comma.csv\", row.names = FALSE)\n\n\n\n10.2 Writing semicolon-separated CSV\n\nwrite.csv2(tips, \"tips_semicolon.csv\", row.names = FALSE)\n\nChoosing the correct format depends on who will read the file next."
  },
  {
    "objectID": "posts/2025-12-26_import_export/index.html#why-we-still-need-excel",
    "href": "posts/2025-12-26_import_export/index.html#why-we-still-need-excel",
    "title": "Understanding Data Import and Export in R: Working with CSV and Excel Files",
    "section": "11 Why We Still Need Excel",
    "text": "11 Why We Still Need Excel\nCSV is technically superior in many ways. Yet Excel remains dominant in practice.\nWhy?\n\nMultiple tables in one file\nFamiliar interface for non-technical users\nCommon reporting format\n\nExcel is not an analysis tool — but it is a powerful delivery tool."
  },
  {
    "objectID": "posts/2025-12-26_import_export/index.html#working-with-excel-in-r-openxlsx",
    "href": "posts/2025-12-26_import_export/index.html#working-with-excel-in-r-openxlsx",
    "title": "Understanding Data Import and Export in R: Working with CSV and Excel Files",
    "section": "12 Working with Excel in R: openxlsx",
    "text": "12 Working with Excel in R: openxlsx\nThe openxlsx package allows Excel operations without requiring Excel itself.\n\nlibrary(openxlsx)\n\n\n12.1 Writing a simple Excel file\n\nwrite.xlsx(tips, \"tips.xlsx\", sheetName = \"tips\")\n\n\n\n12.2 Reading from Excel\n\ntips_excel &lt;- read.xlsx(\"tips.xlsx\", sheet = 1)"
  },
  {
    "objectID": "posts/2025-12-26_import_export/index.html#multiple-sheets-a-mini-report",
    "href": "posts/2025-12-26_import_export/index.html#multiple-sheets-a-mini-report",
    "title": "Understanding Data Import and Export in R: Working with CSV and Excel Files",
    "section": "13 Multiple Sheets: A Mini Report",
    "text": "13 Multiple Sheets: A Mini Report\nExcel shines when organizing related tables.\n\nsummary_tips &lt;- aggregate(tip ~ day, data = tips, mean)\n\nwb &lt;- createWorkbook()\n\naddWorksheet(wb, \"Raw Data\")\nwriteData(wb, \"Raw Data\", tips)\n\naddWorksheet(wb, \"Summary\")\nwriteData(wb, \"Summary\", summary_tips)\n\nsaveWorkbook(wb, \"tips_report.xlsx\", overwrite = TRUE)\n\nOne file.\n\nMultiple views.\n\nClean structure."
  },
  {
    "objectID": "posts/2025-12-26_import_export/index.html#common-mistakes-to-watch-for",
    "href": "posts/2025-12-26_import_export/index.html#common-mistakes-to-watch-for",
    "title": "Understanding Data Import and Export in R: Working with CSV and Excel Files",
    "section": "14 Common Mistakes to Watch For",
    "text": "14 Common Mistakes to Watch For\nMost errors are not caused by R, but by assumptions:\n\nIncorrect working directory\nWrong delimiter (sep)\nWrong decimal separator (dec)\nReading the wrong Excel sheet\nOverwriting files unintentionally\n\nA healthy habit after every import:\n\nhead(data)\nstr(data)\nsummary(data)"
  },
  {
    "objectID": "posts/2025-12-26_import_export/index.html#final-thoughts",
    "href": "posts/2025-12-26_import_export/index.html#final-thoughts",
    "title": "Understanding Data Import and Export in R: Working with CSV and Excel Files",
    "section": "15 Final Thoughts",
    "text": "15 Final Thoughts\nIf you can:\n\nread data correctly,\nwrite data consciously,\nchoose file formats intentionally,\n\nyou have already crossed one of the most important thresholds in data analysis.\nFor a complementary discussion, you may also find this article useful:\nhttps://medium.com/p/e730f4a84b3b\n\nExtended version on Medium:\nhttps://medium.com/@Fatih.Tuzen/understanding-data-import-and-export-in-r-working-with-csv-and-excel-files-6322e61049b2"
  },
  {
    "objectID": "posts/2026-01-02_normalization/index.html",
    "href": "posts/2026-01-02_normalization/index.html",
    "title": "Data Normalization in R: When, Why, and How to Scale Your Data Correctly",
    "section": "",
    "text": "This article is part of a broader series on data preprocessing in R. In earlier posts, we focused on two problems that quietly ruin analyses long before modeling begins: missing data and outliers. Both topics shared a common theme: preprocessing choices are not cosmetic; they change what the model is allowed to learn. In this installment, we move to the next decision point in the same pipeline: normalization (scaling)—often treated as “just a quick step,” but in practice a decisive modeling choice.\n\nRelated posts in this preprocessing series\n\nHandling Missing Data in R: A Comprehensive Guide\nhttps://medium.com/r-evolution/handling-missing-data-in-r-a-comprehensive-guide-eca195eaead3\nOutliers in Data Analysis: Detecting Extreme Values Before Modeling in R\nhttps://medium.com/r-evolution/outliers-in-data-analysis-detecting-extreme-values-before-modeling-in-r-with-i%CC%87stanbul-airbnb-data-3b37e9ee989e\n\n\nNormalization (or more broadly, scaling) is frequently presented as a minor technical adjustment—something to apply quickly and forget. In practice, scaling is not a technical detail but a modeling decision. When the same dataset is processed using different scaling strategies, the behavior of many models changes substantially. Distances, similarity measures, penalty terms, and optimization paths are all affected. As a result, the nearest neighbors selected by KNN, the clusters formed by K-means, the principal components identified by PCA, and even the coefficients chosen by Ridge or Lasso regression can differ. Scaling does not merely “prepare” the data; it actively shapes how a model interprets importance and structure.\nMore importantly, scaling is not universally beneficial. Applied in the wrong context, it can degrade model performance or—worse—introduce subtle forms of data leakage that contaminate evaluation. A common example is learning scaling parameters (such as means and standard deviations) from the entire dataset before splitting into training and test sets. This procedure allows information from the test distribution to leak into the training process, producing performance estimates that cannot be trusted. In such cases, the issue is not the scaling method itself, but when and how it is applied. Knowing how to call scale() in R is trivial; understanding what to scale, when to scale it, and why is not.\nIn this article, normalization is treated as an integral part of the modeling strategy rather than a routine preprocessing step. We will address, step by step, the following questions:\n\nWhy is normalization necessary?\nShould it always be applied?\nAt what stage should it be performed—before or after the train–test split?\nWhich scaling methods are commonly used, and in which contexts do they make sense?\nShould different data types be treated differently?\nIs scaling appropriate for all variables, including the target variable?\n\nBy combining conceptual discussion with practical R implementations, this guide aims to provide clear and principled answers to each of these questions."
  },
  {
    "objectID": "posts/2026-01-02_normalization/index.html#introduction",
    "href": "posts/2026-01-02_normalization/index.html#introduction",
    "title": "Data Normalization in R: When, Why, and How to Scale Your Data Correctly",
    "section": "1 Introduction",
    "text": "1 Introduction\nThis article is part of a broader series on data preprocessing in R. In earlier posts, we focused on two problems that quietly ruin analyses long before modeling begins: missing data and outliers. Both topics shared a common theme: preprocessing choices are not cosmetic; they change what the model is allowed to learn. In this installment, we move to the next decision point in the same pipeline: normalization (scaling)—often treated as “just a quick step,” but in practice a decisive modeling choice.\n\nRelated posts in this preprocessing series\n\nHandling Missing Data in R: A Comprehensive Guide\nhttps://medium.com/r-evolution/handling-missing-data-in-r-a-comprehensive-guide-eca195eaead3\nOutliers in Data Analysis: Detecting Extreme Values Before Modeling in R\nhttps://medium.com/r-evolution/outliers-in-data-analysis-detecting-extreme-values-before-modeling-in-r-with-i%CC%87stanbul-airbnb-data-3b37e9ee989e\n\n\nNormalization (or more broadly, scaling) is frequently presented as a minor technical adjustment—something to apply quickly and forget. In practice, scaling is not a technical detail but a modeling decision. When the same dataset is processed using different scaling strategies, the behavior of many models changes substantially. Distances, similarity measures, penalty terms, and optimization paths are all affected. As a result, the nearest neighbors selected by KNN, the clusters formed by K-means, the principal components identified by PCA, and even the coefficients chosen by Ridge or Lasso regression can differ. Scaling does not merely “prepare” the data; it actively shapes how a model interprets importance and structure.\nMore importantly, scaling is not universally beneficial. Applied in the wrong context, it can degrade model performance or—worse—introduce subtle forms of data leakage that contaminate evaluation. A common example is learning scaling parameters (such as means and standard deviations) from the entire dataset before splitting into training and test sets. This procedure allows information from the test distribution to leak into the training process, producing performance estimates that cannot be trusted. In such cases, the issue is not the scaling method itself, but when and how it is applied. Knowing how to call scale() in R is trivial; understanding what to scale, when to scale it, and why is not.\nIn this article, normalization is treated as an integral part of the modeling strategy rather than a routine preprocessing step. We will address, step by step, the following questions:\n\nWhy is normalization necessary?\nShould it always be applied?\nAt what stage should it be performed—before or after the train–test split?\nWhich scaling methods are commonly used, and in which contexts do they make sense?\nShould different data types be treated differently?\nIs scaling appropriate for all variables, including the target variable?\n\nBy combining conceptual discussion with practical R implementations, this guide aims to provide clear and principled answers to each of these questions."
  },
  {
    "objectID": "posts/2026-01-02_normalization/index.html#normalization-vs.-standardization-clearing-up-the-terminology",
    "href": "posts/2026-01-02_normalization/index.html#normalization-vs.-standardization-clearing-up-the-terminology",
    "title": "Data Normalization in R: When, Why, and How to Scale Your Data Correctly",
    "section": "2 Normalization vs. Standardization: Clearing Up the Terminology",
    "text": "2 Normalization vs. Standardization: Clearing Up the Terminology\nIn both academic writing and everyday practice, the terms normalization and standardization are frequently used interchangeably. This loose usage is one of the main sources of confusion in data preprocessing. In reality, these terms refer to different scaling strategies, each with distinct assumptions, effects, and use cases. Before discussing when and how scaling should be applied, it is therefore essential to clarify what is actually meant by each approach.\nStandardization, often referred to as z-score scaling, rescales a variable so that it has a mean of zero and a standard deviation of one. Formally, each observation is transformed by subtracting the sample mean and dividing by the sample standard deviation. In the R ecosystem, this logic is implemented in preprocessing tools such as step_normalize() from the recipes package. Standardization preserves the shape of the original distribution while putting variables on a comparable scale. It is particularly useful for models that are sensitive to the relative magnitude of predictors, such as linear models with regularization, support vector machines, and neural networks.\nNormalization, in a stricter sense, often refers to min–max scaling. This approach rescales variables to lie within a fixed interval, most commonly [0,1]. Each value is transformed based on the minimum and maximum observed in the training data. Min–max scaling is easy to interpret and is frequently used in algorithms where bounded inputs are desirable. However, it is also more sensitive to extreme values, since a single outlier can heavily influence the scaling range.\nA third commonly used approach is robust scaling, which relies on the median and the interquartile range (IQR) instead of the mean and standard deviation. By construction, this method is less affected by outliers and heavy-tailed distributions. Robust scaling is especially useful in real-world datasets where extreme values are not errors but genuine observations. At the same time, it is not a universal solution; in some data structures, robust measures may become unstable or uninformative.\nThe reason terminology becomes blurred in practice is simple: many practitioners use the word normalization as a generic label for “any kind of scaling.” As a result, two people may both say they normalized their data while having applied entirely different transformations. Throughout this article, we will avoid this ambiguity by explicitly stating which scaling method is used and why. This distinction is not pedantic—it is essential for understanding how scaling choices influence model behavior."
  },
  {
    "objectID": "posts/2026-01-02_normalization/index.html#why-is-normalization-necessary",
    "href": "posts/2026-01-02_normalization/index.html#why-is-normalization-necessary",
    "title": "Data Normalization in R: When, Why, and How to Scale Your Data Correctly",
    "section": "3 Why Is Normalization Necessary?",
    "text": "3 Why Is Normalization Necessary?\nThe necessity of normalization becomes clear once we recognize that many modeling techniques do not operate on raw variable values directly, but on relationships derived from them—such as distances, similarities, penalties, or variance directions. When predictors are measured on different scales, these derived quantities can be dominated by variables with larger numerical ranges, regardless of their substantive importance. In such cases, the model does not learn from the data structure itself, but from arbitrary measurement units.\nThis issue is most apparent in distance-based methods such as k-nearest neighbors (KNN) and K-means clustering. These algorithms rely explicitly on distance calculations, typically Euclidean distance. If one variable ranges between 0 and 1 while another ranges between 0 and 10,000, the latter will dominate the distance computation almost entirely. As a result, proximity is determined not by overall similarity but by the scale of a single variable. Normalization ensures that each predictor contributes to the distance metric in a balanced and interpretable way, allowing the algorithm to reflect genuine similarity rather than numerical magnitude.\nNormalization is equally critical in models that incorporate regularization, such as Ridge and Lasso regression. In these models, coefficients are penalized to control model complexity. However, the penalty term is directly tied to the scale of the predictors. If variables are not on comparable scales, the regularization mechanism will shrink coefficients unevenly, effectively penalizing some predictors more than others for reasons unrelated to their predictive relevance. Scaling aligns the predictors so that regularization operates as intended: as a constraint on model complexity rather than an artifact of measurement units.\nOther widely used techniques—including support vector machines (SVMs), neural networks, and principal component analysis (PCA)—are also highly sensitive to scaling. In SVMs and neural networks, optimization procedures depend on gradients that are influenced by feature magnitudes, affecting both convergence speed and stability. In PCA, the directions of maximum variance are determined by the scale of the variables; without normalization, components may simply reflect variables with the largest variances rather than the most informative underlying structure. In all these cases, scaling is not an optional refinement but a prerequisite for meaningful model behavior.\nBy contrast, tree-based models such as decision trees, random forests, and gradient boosting machines are generally invariant to monotonic transformations of individual predictors. Since splits are based on ordering rather than distance or magnitude, scaling is often unnecessary for these methods. Nevertheless, this does not imply that normalization is universally irrelevant in tree-based pipelines. Hybrid workflows—where tree-based models are combined with distance-based components, rule-based similarity measures, or downstream models sensitive to scale—may still require careful consideration of scaling choices. The key point is not that normalization should always be applied, but that it should be applied with respect to the assumptions of the modeling approach.\nFrom a broader perspective, normalization plays a central role in modern predictive modeling workflows. As emphasized in the predictive modeling literature, preprocessing steps are not independent of the model; they are part of the modeling strategy itself. Scaling decisions shape how information is represented and, ultimately, how learning takes place. Understanding why normalization is necessary is therefore a prerequisite for deciding when and how it should be applied—a topic we address next."
  },
  {
    "objectID": "posts/2026-01-02_normalization/index.html#should-normalization-always-be-applied",
    "href": "posts/2026-01-02_normalization/index.html#should-normalization-always-be-applied",
    "title": "Data Normalization in R: When, Why, and How to Scale Your Data Correctly",
    "section": "4 Should Normalization Always Be Applied?",
    "text": "4 Should Normalization Always Be Applied?\nA natural question at this point is whether normalization should be applied by default in every modeling task. The short answer is no. Normalization is not a universally beneficial preprocessing step; its usefulness depends on the assumptions and internal mechanics of the chosen model. Applying scaling blindly can be as problematic as ignoring it altogether. What is needed is a decision framework that links model characteristics to preprocessing choices.\nFor a large class of models, normalization is strongly recommended. This group includes distance-based methods such as k-nearest neighbors (KNN) and K-means clustering, as well as techniques like principal component analysis (PCA), support vector machines (SVMs), neural networks, and penalized regression models (Ridge, Lasso, Elastic Net). In all these cases, either distances, inner products, variance directions, or penalty terms play a central role. Without scaling, these mechanisms are dominated by variables with larger numerical ranges, leading to distorted learning behavior. For such models, normalization is not a refinement but a prerequisite for meaningful results.\nBy contrast, normalization is generally unnecessary for tree-based models such as decision trees, random forests, and gradient boosting machines (e.g., XGBoost, GBM). These models rely on recursive binary splits based on variable ordering rather than on distances or magnitudes. Since monotonic transformations do not affect the relative ordering of values, scaling typically has no impact on model performance. As a result, normalization is often omitted in purely tree-based pipelines without any loss of effectiveness.\nBetween these two extremes lies a set of models for which normalization is context-dependent. Ordinary linear regression, for example, does not require scaling for estimation itself, but normalization may still be useful for numerical stability, interpretability of coefficients, or comparability across predictors. Similarly, Naive Bayes models may or may not benefit from scaling depending on the assumed feature distributions and the types of variables involved. In these cases, the decision to normalize should be guided by the modeling objective rather than by a fixed rule.\nThe key takeaway is that normalization should be applied with respect to the model’s assumptions, not as a default preprocessing habit. To make this decision explicit, Table 1 summarizes common modeling approaches and whether normalization is typically required.\n\n4.1 When Is Normalization Needed? A Model-Based Decision Table\n\n\n\n\n\n\n\n\nModel / Method\nIs Normalization Recommended?\nRationale\n\n\n\n\nKNN\nYes\nDistance calculations are scale-sensitive\n\n\nK-means\nYes\nCluster assignment depends on distances\n\n\nPCA\nYes\nVariance directions dominated by scale\n\n\nSVM\nYes\nOptimization and margins depend on feature magnitude\n\n\nNeural Networks\nYes\nGradient-based optimization is scale-sensitive\n\n\nRidge / Lasso / Elastic Net\nYes\nPenalty terms depend on predictor scale\n\n\nLinear Regression (OLS)\nDepends\nNot required for estimation, but useful for stability and interpretation\n\n\nNaive Bayes\nDepends\nDepends on feature types and distributional assumptions\n\n\nDecision Trees\nNo\nSplit rules depend on ordering, not scale\n\n\nRandom Forest / GBM / XGBoost\nNo\nTree-based structure is scale-invariant"
  },
  {
    "objectID": "posts/2026-01-02_normalization/index.html#when-should-normalization-be-applied-before-or-after-the-traintest-split",
    "href": "posts/2026-01-02_normalization/index.html#when-should-normalization-be-applied-before-or-after-the-traintest-split",
    "title": "Data Normalization in R: When, Why, and How to Scale Your Data Correctly",
    "section": "5 When Should Normalization Be Applied? Before or After the Train–Test Split?",
    "text": "5 When Should Normalization Be Applied? Before or After the Train–Test Split?\nThis is the most critical question in the entire preprocessing workflow—and the point at which many otherwise sound analyses quietly go wrong. The issue is not whether normalization should be applied, but when it should be applied. At the center of this question lies a fundamental concept in predictive modeling: data leakage.\nData leakage occurs when information from outside the training set is used, directly or indirectly, during model training. In the context of normalization, leakage typically arises when scaling parameters—such as means and standard deviations (for standardization) or minimum and maximum values (for min–max scaling)—are estimated using the full dataset before splitting into training and test sets. Although this may appear harmless, it allows information from the test set to influence the preprocessing step, leading to overly optimistic performance estimates.\nThe correct principle is straightforward but non-negotiable:\nscaling parameters must be learned exclusively from the training data.\nOnce learned, the same transformation—with fixed parameters—must be applied to the test set and to any future, unseen data. This ensures that the test set truly represents new information and that model evaluation reflects genuine generalization rather than procedural artifacts.\nThis principle is central to modern modeling frameworks. In the tidymodels/recipes philosophy, preprocessing steps are trained on the training data and then applied consistently to all other datasets. Similarly, in the caret framework, preprocessing transformations are estimated from the training set and reused when predicting on new data. In both cases, preprocessing is treated as part of the model training process—not as an independent, preliminary operation.\nTo see why this distinction matters, consider the following conceptual comparison.\n\n5.1 An Illustrative Example: Scaling Before vs. After the Split\nSuppose we have a dataset that we intend to split into training and test sets. We want to standardize a numeric predictor using z-score scaling.\nIncorrect approach (scaling before the split):\n\nCompute the mean and standard deviation using the entire dataset.\nStandardize all observations using these global parameters.\nSplit the scaled data into training and test sets.\nTrain and evaluate the model.\n\nAt first glance, this workflow seems efficient. However, the scaling parameters already incorporate information from the test set. The test data are no longer independent of the training process, even though they were not explicitly used to fit the model.\nCorrect approach (scaling after the split):\n\nSplit the raw data into training and test sets.\nCompute scaling parameters (mean, standard deviation, etc.) using only the training set.\nApply the learned transformation to the training set.\nApply the same transformation to the test set.\nTrain the model on the scaled training data and evaluate it on the scaled test data.\n\nIn practice, these two approaches can lead to noticeably different evaluation results. Models trained using the incorrect workflow often appear to perform better on the test set—not because they generalize better, but because the preprocessing step has already “seen” the test data. This difference is especially pronounced in smaller datasets, in datasets with strong distributional differences between training and test splits, or when extreme values are present.\nThe takeaway is unambiguous:\n\nSplit the data first.\nFit preprocessing steps on the training data.\nApply the same transformations to the training and test sets.\n\nAny deviation from this sequence undermines the validity of model evaluation, regardless of how sophisticated the modeling technique may be."
  },
  {
    "objectID": "posts/2026-01-02_normalization/index.html#common-normalization-methods-and-when-to-use-them",
    "href": "posts/2026-01-02_normalization/index.html#common-normalization-methods-and-when-to-use-them",
    "title": "Data Normalization in R: When, Why, and How to Scale Your Data Correctly",
    "section": "6 Common Normalization Methods and When to Use Them",
    "text": "6 Common Normalization Methods and When to Use Them\nNormalization is not a single technique but a family of transformations, each designed to address a specific modeling concern. Choosing an appropriate method requires understanding what problem the transformation is solving and which assumptions it implicitly makes. In this section, we review the most commonly used scaling approaches, discuss their strengths and limitations, and clarify when each method is appropriate.\n\n6.1 Z-score Standardization\nZ-score standardization rescales a variable so that it has a mean of zero and a standard deviation of one. Each observation \\(x_i\\) is transformed as:\n\\[\nz_i = \\frac{x_i - \\mu}{\\sigma},\n\\]\nwhere \\(\\mu\\) denotes the sample mean and \\(\\sigma\\) the sample standard deviation, both estimated from the training data only.\nAdvantages.\nZ-score standardization places variables on a comparable scale while preserving the shape of their original distributions. It is particularly suitable for models that rely on inner products, gradient-based optimization, or regularization (e.g., penalized linear models, SVMs, neural networks).\nLimitations.\nA widespread misconception is that standardization assumes normally distributed data. This is incorrect. Z-score scaling does not require normality; it only uses the first two moments of the distribution. However, it is sensitive to extreme values: large outliers can inflate \\(\\sigma\\), thereby reducing the relative influence of most observations.\nWhen to use.\nA strong default choice when predictors differ substantially in scale and when outliers are either absent or have already been treated.\n\n\n\n6.2 Min–Max (Range) Scaling\nMin–max scaling rescales variables to a fixed interval, most commonly \\([0, 1]\\). The transformation is:\n\\[\nx_i^{*} = \\frac{x_i - \\min(x)}{\\max(x) - \\min(x)}.\n\\]\nAdvantages.\nIntuitive and ensures all transformed values lie within a predefined range. Often used when bounded inputs are desirable (e.g., some neural network settings).\nLimitations.\nHighly sensitive to extreme values: a single outlier can stretch the range and compress most observations. Also, when applied to test or future data, transformed values may fall outside \\([0,1]\\) if they exceed the training-set min/max. This is expected and must be handled in deployment.\nWhen to use.\nWhen input bounds are meaningful and the training data represent the likely range of future observations.\n\n\n\n6.3 Robust Scaling (Median and IQR)\nRobust scaling replaces mean and standard deviation with the median and the interquartile range (IQR). The transformation is:\n\\[\nx_i^{*} = \\frac{x_i - \\mathrm{median}(x)}{\\mathrm{IQR}(x)},\n\\]\nwhere:\n\\[\n\\mathrm{IQR}(x) = Q_{0.75} - Q_{0.25}.\n\\]\nAdvantages.\nLess affected by extreme values and heavy-tailed distributions; useful when outliers are meaningful rather than errors.\nLimitations.\nNot universally stable. In highly concentrated variables, \\(\\mathrm{IQR}(x)\\) (or related robust measures such as MAD) may be zero or extremely small, making the transformation unstable or undefined. This must be checked explicitly.\nWhen to use.\nWhen outliers are present and structurally inherent, and you want scaling that is less sensitive to extremes.\n\n\n\n6.4 Power Transformations Combined with Scaling (Box–Cox and Yeo–Johnson)\nPower transformations aim to stabilize variance and reduce skewness before scaling.\nThe Box–Cox transformation (for strictly positive data) is:\n\\[\nx_i^{(\\lambda)} =\n\\begin{cases}\n\\frac{x_i^{\\lambda} - 1}{\\lambda}, & \\lambda \\neq 0, \\\\\\\\\n\\log(x_i), & \\lambda = 0.\n\\end{cases}\n\\]\nThe Yeo–Johnson transformation (allows zero and negative values) is:\n\\[\nx_i^{(\\lambda)} =\n\\begin{cases}\n\\frac{(x_i + 1)^{\\lambda} - 1}{\\lambda}, & x_i \\ge 0,\\ \\lambda \\neq 0, \\\\\\\\\n\\log(x_i + 1), & x_i \\ge 0,\\ \\lambda = 0, \\\\\\\\\n-\\frac{(-x_i + 1)^{2 - \\lambda} - 1}{2 - \\lambda}, & x_i &lt; 0,\\ \\lambda \\neq 2, \\\\\\\\\n-\\log(-x_i + 1), & x_i &lt; 0,\\ \\lambda = 2.\n\\end{cases}\n\\]\nWhy combine with scaling?\nPower transformations modify distributional shape but do not put variables on a common scale. After applying Box–Cox or Yeo–Johnson, variables are typically centered and scaled.\nOrder matters.\nA practical default sequence is: power transformation → centering → scaling. Scaling before addressing skewness can weaken the effect of the transformation and complicate interpretation.\nWhen to use.\nWhen strong skewness or heteroscedasticity is present and when model assumptions or optimization benefit from more symmetric distributions.\n\n\n\n6.5 Choosing a Method: No Single Best Answer\nThere is no universally optimal normalization method. Each approach reflects a trade-off between robustness, interpretability, and sensitivity to data characteristics. The appropriate choice depends on the model, the data structure, and the modeling objective.\n\nThe relevant question is not “Which normalization method is best?”\nbut “Which transformation aligns with my data and my model’s assumptions?”"
  },
  {
    "objectID": "posts/2026-01-02_normalization/index.html#do-different-data-types-require-different-scaling-strategies",
    "href": "posts/2026-01-02_normalization/index.html#do-different-data-types-require-different-scaling-strategies",
    "title": "Data Normalization in R: When, Why, and How to Scale Your Data Correctly",
    "section": "7 Do Different Data Types Require Different Scaling Strategies?",
    "text": "7 Do Different Data Types Require Different Scaling Strategies?\nNormalization decisions should never be made independently of data types. Different variable types carry different semantic meanings, and applying the same scaling strategy indiscriminately can lead to misleading representations or unnecessary transformations. A principled preprocessing workflow therefore begins by distinguishing between variable types and understanding how each interacts with scaling.\n\n7.1 Continuous Numeric Variables\nContinuous numeric variables are the primary candidates for normalization. When such variables are measured on different scales—such as income in thousands and proportions between 0 and 1—scaling is often essential for models that rely on distances, gradients, or regularization. Z-score standardization, min–max scaling, or robust scaling are all reasonable options, depending on the presence of outliers and the modeling objective.\nIn practice, most normalization methods are designed with continuous variables in mind, and applying them here rarely raises conceptual concerns. The main decision revolves around which scaling method is most appropriate, not whether scaling should be applied at all.\n\n\n\n7.2 Count and Ordinal Numeric Variables\nSome numeric variables are technically continuous in storage but conceptually represent counts or ordered categories. Examples include the number of visits, rankings, Likert-scale responses, or discrete event counts. Treating such variables as purely continuous can be problematic, especially when their distributions are highly skewed or bounded at zero.\nIn these cases, applying a logarithmic or power transformation before scaling is often more appropriate than direct normalization. Power transformations can reduce skewness and stabilize variance, after which standardization or robust scaling may be applied. The key point is that the meaning of the variable matters: a difference of one unit in a count variable does not necessarily carry the same interpretation across its range.\n\n\n\n7.3 Categorical Variables (Factors or Characters)\nCategorical variables should never be scaled directly. Their values represent qualitative categories rather than numerical magnitudes, and applying normalization to raw category codes is meaningless.\nWhen categorical variables are included in models that require numeric inputs, they must first be transformed using an encoding scheme such as one-hot (dummy) encoding. After encoding, the question of scaling arises again. In many cases, scaling encoded variables is unnecessary. However, in penalized regression models or distance-based methods, normalization of one-hot encoded variables may be beneficial to ensure that categorical and continuous predictors are treated on comparable scales.\nThe important distinction is that scaling applies after encoding, not before, and only when the model’s assumptions justify it.\n\n\n\n7.4 Binary Variables (0/1 Indicators)\nBinary variables occupy a special position. Since they already lie on a fixed and interpretable scale, normalization is usually unnecessary and may even obscure interpretation. For many models, leaving binary indicators unchanged is the most transparent choice.\nThat said, binary variables often enter preprocessing pipelines automatically when a rule such as “scale all numeric predictors” is applied. In such cases, standardization will transform a 0/1 variable into values centered around zero with unit variance. While this does not usually harm model performance, it changes the interpretation of coefficients and can complicate downstream analysis.\nThis highlights an important practical lesson: automated preprocessing pipelines should be used with care. Even when a transformation is mathematically valid, it may not be conceptually desirable for all variable types.\n\n\n\n7.5 Summary: Scaling Depends on Variable Meaning\nThe decision to normalize should always be guided by the semantic role of a variable, not merely by its storage type. Continuous measurements, counts, ordered responses, categorical indicators, and binary flags interact with scaling in fundamentally different ways. Effective preprocessing therefore requires more than applying a generic rule—it requires aligning transformations with the structure and meaning of the data."
  },
  {
    "objectID": "posts/2026-01-02_normalization/index.html#should-all-variables-be-scaled",
    "href": "posts/2026-01-02_normalization/index.html#should-all-variables-be-scaled",
    "title": "Data Normalization in R: When, Why, and How to Scale Your Data Correctly",
    "section": "8 Should All Variables Be Scaled?",
    "text": "8 Should All Variables Be Scaled?\nA common mistake in preprocessing workflows is to treat normalization as a blanket operation applied to every variable in the dataset. In reality, not all variables should be scaled, and doing so indiscriminately can reduce interpretability or even introduce unintended distortions. Scaling decisions must therefore be made at the variable level, guided by both statistical and semantic considerations.\n\n8.1 The Target Variable (y)\nIn most predictive modeling tasks, the target variable should not be normalized. Scaling the response does not improve model estimation and often complicates interpretation, particularly in regression settings where coefficients and predictions are expected to be expressed in the original units.\nThere are, however, notable exceptions. In neural network regression or other optimization-heavy models, scaling the target variable can improve numerical stability and convergence behavior. In such cases, predictions must be transformed back to the original scale before evaluation and interpretation. Outside these specific contexts, leaving the target variable unchanged remains the standard and preferred practice.\n\n\n\n8.2 Predictor Variables\nFor predictor variables, scaling should be applied selectively rather than universally.\n\n8.2.1 Numeric Predictors Only\nNormalization is meaningful only for numeric predictors. Applying scaling to non-numeric variables—either directly or implicitly through arbitrary numeric coding—has no conceptual justification. As discussed earlier, categorical variables must first be encoded, and even then, scaling is optional and model-dependent.\n\n\n8.2.2 Excluding Non-informative Numeric Variables\nNot all numeric variables carry meaningful quantitative information. Identifier variables such as IDs, account numbers, or arbitrary codes may be stored as numeric values but do not represent magnitudes or distances. Scaling such variables is meaningless and potentially harmful, as it introduces artificial structure where none exists. These variables should be excluded from the modeling process altogether, not merely from scaling.\n\n\n8.2.3 Handling Low-Variance Predictors\nVariables with extremely low or zero variance provide little to no information for modeling. Scaling such predictors does not solve the underlying problem; it merely rescales noise. In practice, low-variance and zero-variance predictors should be identified and removed before normalization.\nMany preprocessing frameworks formalize this step. For example, approaches based on the logic of zero-variance or near-zero-variance filtering (often referred to as zv or nzv steps) ensure that only informative predictors enter the scaling stage. This not only improves computational efficiency but also reduces the risk of numerical instability in downstream models.\n\n\n\n\n8.3 A Practical Rule of Thumb\nA disciplined preprocessing workflow follows a clear sequence:\n\nIdentify and remove non-informative variables (IDs, constants, near-constants).\nSelect numeric predictors that represent meaningful quantities.\nApply appropriate scaling only to this subset.\nLeave the target variable unscaled, unless there is a compelling model-specific reason to do otherwise.\n\nScaling is most effective when it is deliberate and selective, not automatic. Treating normalization as a universal operation may simplify code, but it rarely leads to better models."
  },
  {
    "objectID": "posts/2026-01-02_normalization/index.html#application-plan-in-r-data-and-modeling-scenario",
    "href": "posts/2026-01-02_normalization/index.html#application-plan-in-r-data-and-modeling-scenario",
    "title": "Data Normalization in R: When, Why, and How to Scale Your Data Correctly",
    "section": "9 Application Plan in R: Data and Modeling Scenario",
    "text": "9 Application Plan in R: Data and Modeling Scenario\nTo demonstrate the practical implications of normalization decisions, we use the Ames Housing dataset, a well-known benchmark dataset designed for predictive modeling. The dataset contains 2,930 observations and a rich set of predictors describing residential properties in Ames, Iowa. These predictors span multiple data types, including continuous numeric variables, discrete counts, ordinal ratings, and categorical features. This diversity makes the dataset particularly suitable for illustrating how scaling interacts with different variable types.\nThe Ames Housing dataset is distributed within the modeldata package in the tidymodels ecosystem. It was explicitly curated for teaching and methodological demonstrations, ensuring a realistic but well-documented structure. The presence of variables measured on vastly different scales—such as living area, lot size, and quality scores—provides a natural setting for exploring the effects of normalization.\n\n9.1 Modeling Objective\nThe primary goal of this application is not to optimize predictive performance, but to isolate and examine the impact of different normalization strategies. For this reason, the modeling task is intentionally kept simple. We focus on predicting the sale price of a house as a regression problem, using a fixed model specification across all experiments.\nThe model itself serves merely as a vehicle for comparison. By holding the model constant and varying only the preprocessing strategy, we can attribute differences in performance and behavior directly to scaling decisions rather than to model complexity or tuning choices.\n\n\n9.2 Scope and Focus\nThroughout the application section, the emphasis remains firmly on preprocessing:\n\nthe same training–test split is used across all scenarios,\nthe same set of predictors is retained,\nthe same model structure is applied.\n\nOnly the normalization strategy changes. This design allows us to answer a focused question:\n\nHow much do scaling choices matter when everything else is kept equal?\n\nBy structuring the analysis in this way, the results highlight normalization as an integral component of the modeling pipeline rather than a secondary technical detail.\n\n\n\n9.3 Transition to Implementation\nIn the next section, we move from design to execution. We begin by defining a train–test split and establishing a baseline preprocessing workflow. From there, we introduce alternative normalization strategies and compare their effects using consistent evaluation criteria.\n\n\n9.4 Data Access and Availability\nThe Ames Housing dataset used in this application is available through the modeldata package, which is part of the tidymodels ecosystem. No external download is required. Once the package is installed, the dataset can be accessed directly within R.\nThe dataset is provided for educational and methodological purposes and is accompanied by detailed documentation. For reference, the official description is available at:\nhttps://modeldata.tidymodels.org/reference/ames.html\nIn the next section, we load the dataset directly from the package and proceed with the train–test split and preprocessing workflow."
  },
  {
    "objectID": "posts/2026-01-02_normalization/index.html#implementation-in-r-split-baseline-and-the-cost-of-doing-it-wrong",
    "href": "posts/2026-01-02_normalization/index.html#implementation-in-r-split-baseline-and-the-cost-of-doing-it-wrong",
    "title": "Data Normalization in R: When, Why, and How to Scale Your Data Correctly",
    "section": "10 Implementation in R: Split, Baseline, and the Cost of Doing It Wrong",
    "text": "10 Implementation in R: Split, Baseline, and the Cost of Doing It Wrong\nIn this section, we operationalize the key principle introduced earlier:\n\nSplit → fit preprocessing on train → apply to train/test\n\nWe use the Ames Housing dataset from the modeldata package (no external download required) and compare three pipelines using the same model:\n\nBaseline (no scaling)\nIncorrect scaling (data leakage): scaling parameters learned from the full dataset\nCorrect scaling: scaling parameters learned from the training set only\n\nThe goal is not to build the best possible model but to isolate the effect of scaling decisions.\n\n10.1 Setup and Variable Selection\nBefore defining any model, we clarify what we are modeling and why these variables are used.\nModeling goal.\nWe treat Sale_Price as the target variable and build a regression model that predicts house sale prices based on a small set of numeric predictors. The purpose is not to maximize predictive accuracy, but to create a controlled environment where the effect of scaling choices is easy to observe.\nWhy a small subset of predictors?\nThe Ames dataset contains many variables, including categorical and ordinal predictors. For the normalization demonstrations, we intentionally select a compact set of numeric features with clearly different measurement scales. This makes the consequences of scaling (and data leakage) more visible and easier to interpret.\nSelected variables (interpretation).\n\nSale_Price: sale price of the house (response variable).\nGr_Liv_Area: above-ground living area (a size-related continuous measure).\nLot_Area: lot size (typically much larger numeric range than living area).\nYear_Built: construction year (a temporal numeric variable).\nOverall_Cond: overall condition rating (an ordinal-like numeric score).\nLatitude, Longitude: geographic coordinates capturing location effects.\n\n\n\n\n10.2 Load Data and Create a Working Dataset\n\nlibrary(tidymodels)\nlibrary(modeldata)\n\ndata(ames, package = \"modeldata\")\n\nset.seed(2026)\n\names_small &lt;- ames %&gt;%\n  dplyr::select(\n    Sale_Price,\n    Gr_Liv_Area,\n    Lot_Area,\n    Year_Built,\n    Overall_Cond,\n    Latitude,\n    Longitude\n  )\n\n# Missing-value check within the selected columns\names_small %&gt;%\n  summarise(across(everything(), ~ sum(is.na(.)))) %&gt;%\n  tidyr::pivot_longer(everything(), names_to = \"variable\", values_to = \"n_missing\")\n\n# A tibble: 7 × 2\n  variable     n_missing\n  &lt;chr&gt;            &lt;int&gt;\n1 Sale_Price           0\n2 Gr_Liv_Area          0\n3 Lot_Area             0\n4 Year_Built           0\n5 Overall_Cond         0\n6 Latitude             0\n7 Longitude            0\n\n\nThis step constructs a clean working dataset (ames_small) and confirms whether missing values exist in the selected columns. For the comparisons in the next sections, it is important that the pipelines differ only by preprocessing choices (e.g., scaling), not by inconsistent handling of missing data.\n\n\n10.3 Train–Test Split and Evaluation Setup\nBefore discussing scaling, we must establish a clean evaluation setup. The key idea is simple:\n\nSplit first. Then learn any preprocessing parameters from the training set only.\n\nWithout a proper train–test split, we cannot meaningfully talk about generalization, and any comparison involving normalization risks becoming misleading.\n\n\n10.3.1 Create a Stratified Train–Test Split\n\nset.seed(2026)\n\nsplit_obj &lt;- initial_split(ames_small, prop = 0.80, strata = Sale_Price)\n\ntrain_data &lt;- training(split_obj)\ntest_data  &lt;- testing(split_obj)\n\nnrow(train_data)\n\n[1] 2342\n\nnrow(test_data)\n\n[1] 588\n\n\nWhat this does.\n\nprop = 0.80 assigns roughly 80% of the data to training and 20% to testing.\nstrata = Sale_Price performs a stratified split based on the target variable.\nThis reduces the risk that the test set ends up with an atypical concentration of very low or very high prices—something that can easily happen with skewed targets like house prices.\n\nHow to interpret the output.\n\nIf the full dataset contains 2,930 observations, you should see approximately:\n\n-    training: 2,342 rows\n\n-    test: 588 rows\nThis corresponds closely to the intended 80/20 split and indicates that no unintended row loss occurred during preprocessing.\n\n\n10.3.2 Sanity Check: Is the Target Distribution Similar Across Splits?\n\nbind_rows(\n  train_data %&gt;% mutate(split = \"train\"),\n  test_data  %&gt;% mutate(split = \"test\")\n) %&gt;%\n  ggplot(aes(x = Sale_Price, fill = split)) +\n  geom_histogram(bins = 40, alpha = 0.7, color = \"white\") +\n  facet_wrap(~ split, scales = \"free_y\") +\n  scale_fill_manual(\n    values = c(train = \"#1f77b4\", test = \"#ff7f0e\")\n  ) +\n  labs(\n    title = \"Sale_Price distribution after train–test split\",\n    x = \"Sale_Price\",\n    y = \"Count\",\n    fill = \"Data split\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nWhat to look for.\n\nBoth distributions should be right-skewed with a similar central mass.\nThere should be no strong imbalance where most expensive (or cheapest) homes appear in only one split.\n\nIn the plot, the overall shapes are highly similar and the mid-range is well represented in both sets, indicating that stratification preserved the structure of the target variable across splits.\n\n\n10.3.3 Optional Check: Quick Summary Statistics\nThis is a compact numerical confirmation of what the plot shows.\n\ntrain_summary &lt;- train_data %&gt;%\nsummarise(\nsplit = \"train\",\nn = n(),\nmean = mean(Sale_Price),\nmedian = median(Sale_Price),\nsd = sd(Sale_Price),\nmin = min(Sale_Price),\nmax = max(Sale_Price)\n)\n\ntest_summary &lt;- test_data %&gt;%\nsummarise(\nsplit = \"test\",\nn = n(),\nmean = mean(Sale_Price),\nmedian = median(Sale_Price),\nsd = sd(Sale_Price),\nmin = min(Sale_Price),\nmax = max(Sale_Price)\n)\n\nbind_rows(train_summary, test_summary)\n\n# A tibble: 2 × 7\n  split     n    mean median     sd   min    max\n  &lt;chr&gt; &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;  &lt;int&gt;\n1 train  2342 180447. 160000 79157. 12789 755000\n2 test    588 182185. 160500 82784. 35311 625000\n\n\nHow to interpret this.\n\nSmall differences between train and test are expected.\nLarge gaps—especially in the median—may indicate an unbalanced split.\n\nYour summaries show nearly identical means and medians (train: 180,447 / 160,000; test: 182,185 / 160,500) and similar standard deviations, supporting the conclusion that the split is well balanced. Differences in the maximum values are expected due to rare high-priced homes and do not indicate a problematic split.\nThe train–test split is well balanced and suitable for downstream modeling. The test set can be treated as a genuine proxy for unseen data, allowing us to evaluate normalization strategies without confounding effects from an unbalanced split.\n\n\n\n10.4 Model Specification: A Scale-Sensitive Baseline\nBefore comparing different normalization strategies, we must fix the modeling component of the pipeline. This ensures that any performance differences observed later can be attributed to preprocessing choices rather than to changes in the model itself.\nWhy KNN Regression?\nWe deliberately choose k-nearest neighbors (KNN) regression for this demonstration. The reason is methodological, not practical.\nKNN is a distance-based algorithm: predictions are determined by the distances between observations in the feature space. As a result, KNN is highly sensitive to the scale of the predictors. Variables with larger numeric ranges can dominate distance calculations, even if they are not substantively more important.\nThis property makes KNN an ideal diagnostic tool for studying the effects of scaling.\n\n\n10.4.1 Model Specification\nWe define a single KNN model that will be used in all subsequent scenarios.\n\nknn_spec &lt;- nearest_neighbor(\n  neighbors = 15,\n  weight_func = \"rectangular\"\n) %&gt;%\n  set_engine(\"kknn\") %&gt;%\n  set_mode(\"regression\")\n\nCommentary.\n\nThe number of neighbors is fixed at 15 to reduce variance while maintaining locality.\nNo hyperparameter tuning is performed, as optimization is not the goal here.\nThis model specification will remain unchanged across all preprocessing pipelines.\n\n\n\n\n10.5 Scenario A — Baseline: No Scaling\nWe begin with a baseline workflow in which no scaling is applied. This provides a reference point against which all normalized pipelines will be compared.\n\nrec_none &lt;- recipe(Sale_Price ~ ., data = train_data)\n\nwf_none &lt;- workflow() %&gt;%\nadd_recipe(rec_none) %&gt;%\nadd_model(knn_spec)\n\nfit_none &lt;- fit(wf_none, data = train_data)\n\n\n\n\n\n\n\nNote\n\n\n\n\nNote on model engines.\nIn the tidymodels ecosystem, model specifications are defined independently of the underlying computational engines. Although we specify the KNN model via nearest_neighbor(), the actual implementation is provided by the kknn package.\nIf the package is not installed, fitting the model will fail. To proceed, install and load the required engine:\ninstall.packages(\"kknn\")\nlibrary(kknn)\nThis separation between model specification and engine implementation is intentional and allows tidymodels to remain modular and extensible.\n\n\n\n\n10.5.1 Evaluate on the Test Set\n\npred_none &lt;- predict(fit_none, test_data) %&gt;%\nbind_cols(test_data %&gt;% dplyr::select(Sale_Price))\n\nmetrics_none &lt;- yardstick::metrics(\npred_none,\ntruth = Sale_Price,\nestimate = .pred\n)\n\nmetrics_none\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard   35643.   \n2 rsq     standard       0.816\n3 mae     standard   23726.   \n\n\n\n\n10.5.2 Interpretation\nThese values are not “good” or “bad” in isolation; what matters is that they provide a stable reference. At this stage, the model operates on raw predictor scales. For a distance-based method like KNN, this implies:\n\nPredictors with larger numeric ranges (e.g., Lot_Area) can disproportionately influence distance calculations.\nSmaller-range variables (e.g., ordinal-like Overall_Cond) may contribute less than intended.\nThe model’s behavior is therefore partially shaped by measurement units, not only by predictive structure.\n\nThis is exactly why KNN is a useful diagnostic tool in a normalization-focused article: if scaling matters, we should see clear changes relative to this baseline once we introduce normalization.\nNext, we introduce scaling—but incorrectly on purpose. We will apply normalization before the train–test split (i.e., using information from the full dataset). This creates data leakage and can lead to deceptively improved test performance.\nAfter that, we will implement the correct workflow (fit scaling parameters on the training set only) and compare all scenarios side by side.\n\n\n\n10.6 Scenario B — Incorrect Normalization (Data Leakage)\nIn this scenario, we intentionally apply normalization the wrong way: we learn scaling parameters from the full dataset (including what will become the test set). This contaminates the evaluation because preprocessing has already “seen” information from the test distribution.\nThe goal is not to recommend this approach, but to demonstrate how easily leakage can happen—and how it can artificially improve test metrics.\n\n10.6.1 Leakage Pipeline: Normalize Using Full Data\nThe step_normalize() operation applies only to numeric predictors. In our dataset, Overall_Cond is stored as a factor (ordinal-like category), so it must not be normalized directly.\n\nrec_leak &lt;- recipe(Sale_Price ~ ., data = ames_small) %&gt;%\n  step_normalize(all_numeric_predictors())\n\n# WRONG on purpose: prepping on full data (leakage), but now type-safe\nprep_leak &lt;- prep(rec_leak, training = ames_small)\n\ntrain_leak &lt;- bake(prep_leak, new_data = train_data)\ntest_leak  &lt;- bake(prep_leak, new_data = test_data)\n\nwf_leak &lt;- workflow() %&gt;%\n  add_model(knn_spec) %&gt;%\n  add_formula(Sale_Price ~ .)\n\nfit_leak &lt;- fit(wf_leak, data = train_leak)\n\npred_leak &lt;- predict(fit_leak, test_leak) %&gt;%\n  bind_cols(test_leak %&gt;% dplyr::select(Sale_Price))\n\nmetrics_leak &lt;- yardstick::metrics(pred_leak, truth = Sale_Price, estimate = .pred)\nmetrics_leak\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard   37036.   \n2 rsq     standard       0.801\n3 mae     standard   24411.   \n\n\n\n\n10.6.2 Interpretation\nThe performance obtained under this scenario reflects the consequences of incorrect normalization with data leakage.\nCompared to the baseline (no scaling), all three metrics deteriorate. This indicates that learning normalization parameters from the full dataset does not automatically lead to better predictive performance. In this case, the leakage-induced transformation appears to distort the distance structure in a way that is unfavorable for KNN.\nThis result is particularly instructive because it challenges a common misconception:\ndata leakage does not necessarily inflate performance metrics. Its effect depends on the interaction between the preprocessing step, the data distribution, and the model. What leakage does guarantee, however, is that the evaluation is no longer valid.\nEven if the metrics had improved under this scenario, they could not be trusted as estimates of out-of-sample performance. The test data would no longer represent genuinely unseen observations, since information from their distribution had already been incorporated during preprocessing.\nAt this point, two important conclusions can be drawn:\n\nScaling decisions materially affect model behavior, especially for distance-based methods.\nThe timing of scaling—when parameters are learned—is as critical as whether scaling is applied at all.\n\nIn the next scenario, we apply normalization correctly by estimating scaling parameters using the training data only and then applying them unchanged to the test set. This will provide the only defensible estimate of generalization performance among the normalization strategies considered.\n\n\n\n10.7 Scenario C — Correct Normalization (Train-Only Scaling)\nIn this final preprocessing scenario, normalization parameters are learned exclusively from the training data and then applied consistently to both the training and test sets.\nThis workflow adheres to the core principle of leakage-free modeling.\n\n10.7.1 Correct Pipeline: Normalize Using Training Data Only\n\nrec_ok &lt;- recipe(Sale_Price ~ ., data = train_data) %&gt;%\n  step_normalize(all_numeric_predictors())\n\nwf_ok &lt;- workflow() %&gt;%\n  add_recipe(rec_ok) %&gt;%\n  add_model(knn_spec)\n\nfit_ok &lt;- fit(wf_ok, data = train_data)\n\npred_ok &lt;- predict(fit_ok, test_data) %&gt;%\n  bind_cols(test_data %&gt;% dplyr::select(Sale_Price))\n\nmetrics_ok &lt;- yardstick::metrics(pred_ok, truth = Sale_Price, estimate = .pred)\n\nmetrics_ok\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard   35643.   \n2 rsq     standard       0.816\n3 mae     standard   23726.   \n\n\n\n\n10.7.2 Interpretation\nThis scenario represents the correct normalization workflow, where scaling parameters are learned exclusively from the training data and then applied unchanged to the test set. The results are identical to the no-scaling baseline. This finding is highly informative.\nFirst, it confirms that normalization itself does not automatically improve model performance. When applied correctly, scaling does not inject additional information into the modeling process; it merely changes the representation of the data. If the underlying distance structure relevant for prediction is already dominated by certain predictors, scaling may have little to no effect on performance.\nSecond, the contrast with the leakage scenario is crucial. In Scenario B, incorrect normalization degraded performance, while in this scenario, correct normalization restores the metrics to their baseline levels. This symmetry reinforces the core message of this article:\nthe validity of preprocessing matters more than the apparent gains it may produce.\nThird, these results highlight an often-overlooked point: the impact of scaling is model- and data-dependent. For this particular subset of predictors and this KNN configuration, normalization neither helps nor harms when applied correctly. In other settings—different feature sets, different distance metrics, or different models—the effect could be substantial.\nThe key takeaway is therefore not that scaling is unnecessary, but that it must be:\napplied deliberately,\nrestricted to appropriate variables,\nand learned at the correct stage of the modeling workflow.\nWith all three scenarios evaluated, we can now compare them side by side and distill the practical lessons they offer.\n\n\n\n10.8 Results Comparison\nWith all three scenarios evaluated, we now compare them side by side. Since the model and data split were held constant, any differences observed here are entirely attributable to preprocessing choices.\n\n10.8.1 Performance Summary\n\nresults_tbl &lt;- dplyr::bind_rows(\n  metrics_none %&gt;% mutate(scenario = \"A — No Scaling\"),\n  metrics_leak %&gt;% mutate(scenario = \"B — Incorrect Scaling (Leakage)\"),\n  metrics_ok   %&gt;% mutate(scenario = \"C — Correct Scaling (Train-Only)\")\n) %&gt;%\n  dplyr::select(scenario, .metric, .estimate) %&gt;%\n  tidyr::pivot_wider(\n    names_from = .metric,\n    values_from = .estimate\n  )\n\nresults_tbl\n\n# A tibble: 3 × 4\n  scenario                           rmse   rsq    mae\n  &lt;chr&gt;                             &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 A — No Scaling                   35643. 0.816 23726.\n2 B — Incorrect Scaling (Leakage)  37036. 0.801 24411.\n3 C — Correct Scaling (Train-Only) 35643. 0.816 23726.\n\n\nThis table summarizes test-set performance across all scenarios.\n\nScenario A (No Scaling) serves as the baseline.\nScenario B (Incorrect Scaling with Leakage) shows degraded performance.\nScenario C (Correct Scaling) reproduces the baseline results exactly.\n\n\n\n10.8.2 Visual Comparison (RMSE)\nTo make the differences easier to interpret, we visualize RMSE across scenarios.\n\nresults_tbl %&gt;%\nggplot(aes(x = scenario, y = rmse, fill = scenario)) +\ngeom_col(alpha = 0.8) +\nscale_fill_manual(\nvalues = c(\n\"A — No Scaling\" = \"#1f77b4\",\n\"B — Incorrect Scaling (Leakage)\" = \"#d62728\",\n\"C — Correct Scaling (Train-Only)\" = \"#2ca02c\"\n)\n) +\nlabs(\ntitle = \"RMSE comparison across preprocessing scenarios\",\nx = \"Preprocessing scenario\",\ny = \"RMSE\"\n) +\ntheme_minimal() +\ntheme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n10.8.3 Interpretation\nSeveral important conclusions emerge from this comparison.\nFirst, normalization does not inherently improve performance. When applied correctly (Scenario C), scaling neither improves nor degrades performance relative to the no-scaling baseline. This confirms that normalization is a representational transformation, not a source of predictive signal.\nSecond, incorrect normalization can be harmful. Scenario B demonstrates that learning scaling parameters from the full dataset can distort the feature space in ways that negatively affect model behavior. Even more importantly, this scenario yields an invalid evaluation, regardless of whether the metrics appear better or worse.\nThird, these results reinforce a central theme of this article:\nthe correctness of the preprocessing workflow matters more than the choice of preprocessing method itself.\nIn practice, this means that:\n\nscaling should be applied only when it aligns with the model’s assumptions,\npreprocessing parameters must be learned exclusively from training data,\nand any apparent performance gains should be scrutinized for potential leakage.\n\n\n\n\n10.9 Practical Takeaways from the Application\nFrom this controlled experiment, we can distill three practical lessons:\n\nDo not expect normalization to be a silver bullet. Its impact depends on the model, the data, and the feature set.\nNever compromise the train–test boundary. Leakage can invalidate results even when performance does not improve.\nTreat preprocessing as part of the model. Decisions about scaling are modeling decisions, not technical afterthoughts.\n\nThese lessons generalize beyond KNN and apply to any workflow involving scale-sensitive models and data transformations."
  },
  {
    "objectID": "posts/2026-01-02_normalization/index.html#introduction-1",
    "href": "posts/2026-01-02_normalization/index.html#introduction-1",
    "title": "Data Normalization in R: When, Why, and How to Scale Your Data Correctly",
    "section": "2 Introduction",
    "text": "2 Introduction\nNormalization (or more broadly, scaling) is often treated as a minor technical step in data preprocessing—something to be applied quickly and then forgotten. In practice, however, scaling is not a technical detail but a modeling decision. When the same dataset is processed using different scaling strategies, the behavior of many models changes substantially. Distances, similarity measures, penalty terms, and optimization paths are all affected. As a result, the nearest neighbors selected by KNN, the clusters formed by K-means, the principal components identified by PCA, and even the coefficients chosen by Ridge or Lasso regression can differ. Scaling does not merely “prepare” the data; it actively shapes how a model interprets importance and structure.\nMore importantly, scaling is not universally beneficial. Applied in the wrong context, it can degrade model performance or, even worse, introduce subtle forms of data leakage that artificially inflate evaluation results. A common example is learning scaling parameters (such as means and standard deviations) from the entire dataset before splitting into training and test sets. This procedure allows information from the test set to leak into the training process, producing overly optimistic performance estimates. In such cases, the issue is not the scaling method itself, but when and how it is applied. Knowing how to call scale() in R is trivial; understanding what to scale, when to scale it, and why is not.\nIn this article, normalization is treated as an integral part of the modeling strategy rather than a routine preprocessing step. We will address, step by step, the following questions: Why is normalization necessary? Should it always be applied? At what stage should it be performed—before or after the train–test split? Which scaling methods are commonly used, and in which contexts do they make sense? Should different data types be treated differently? Is scaling appropriate for all variables, including the target variable? By combining conceptual discussion with practical R implementations, this guide aims to provide clear and principled answers to each of these questions."
  },
  {
    "objectID": "posts/2026-01-02_normalization/index.html#discussion-and-conclusion",
    "href": "posts/2026-01-02_normalization/index.html#discussion-and-conclusion",
    "title": "Data Normalization in R: When, Why, and How to Scale Your Data Correctly",
    "section": "11 Discussion and Conclusion",
    "text": "11 Discussion and Conclusion\nNormalization is often introduced as a routine preprocessing step, applied almost reflexively before modeling. This article has argued—and demonstrated—that such a view is incomplete. Normalization is not a purely technical adjustment; it is a modeling decision whose consequences depend on the interaction between data, model assumptions, and evaluation design.\nFrom a theoretical perspective, scaling matters because many learning algorithms are sensitive to the relative magnitudes of predictors. Distance-based methods, regularized models, kernel methods, and optimization-driven algorithms implicitly encode assumptions about scale. Ignoring these assumptions can distort model behavior, while respecting them can improve stability and interpretability. At the same time, scaling does not create new information. It reshapes how existing information is represented.\nThe empirical application using the Ames Housing dataset reinforced these points. By holding the model and data split constant and varying only the preprocessing strategy, we isolated the effect of normalization decisions. Three key findings emerged.\nFirst, normalization does not guarantee performance improvements. In the correct workflow, scaling reproduced the baseline results exactly. This confirms that normalization should not be expected to “fix” a model by itself. Its role is conditional and context-dependent.\nSecond, incorrect normalization compromises validity. Learning scaling parameters from the full dataset—thereby introducing data leakage—altered model behavior and degraded performance in this example. More importantly, even if the metrics had improved, the evaluation would have been invalid. Leakage undermines the fundamental purpose of a test set: to approximate unseen data.\nThird, the timing of preprocessing is as important as the method chosen. The difference between valid and invalid evaluation hinged not on whether scaling was applied, but on when its parameters were learned. This distinction is often overlooked in practice, yet it is central to trustworthy modeling.\nTaken together, these results support a broader principle: preprocessing steps should be treated as integral components of the modeling pipeline, not as detached technical preliminaries. Decisions about normalization should be guided by model assumptions, data characteristics, and evaluation design—not by habit or generic checklists.\nIn practical terms, this leads to a simple but robust rule:\n\nSplit the data first. Learn preprocessing parameters from the training set only. Apply the same transformations to all future data.\n\nNormalization, when used deliberately and correctly, is a powerful tool. When applied mechanically or at the wrong stage, it can mislead. Understanding this distinction is essential for building models that are not only accurate, but also scientifically defensible."
  },
  {
    "objectID": "posts/2026-01-02_normalization/index.html#references",
    "href": "posts/2026-01-02_normalization/index.html#references",
    "title": "Data Normalization in R: When, Why, and How to Scale Your Data Correctly",
    "section": "12 References",
    "text": "12 References\n\nHastie, T., Tibshirani, R., & Friedman, J. (2009).\nThe Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer.\nKuhn, M., & Johnson, K. (2013).\nApplied Predictive Modeling. Springer.\nKuhn, M., & Wickham, H. (2023).\nTidymodels: A Collection of Packages for Modeling and Machine Learning Using Tidyverse Principles.\nhttps://www.tidymodels.org/\nTidymodels Recipes Documentation.\nhttps://recipes.tidymodels.org/\nKuhn, M. (Caret package documentation).\nhttps://topepo.github.io/caret/\nModeldata package documentation (Ames Housing dataset).\nhttps://modeldata.tidymodels.org/reference/ames.html"
  },
  {
    "objectID": "posts/2026-01-22_data_leakage/index.html#introduction-why-this-topic-matters",
    "href": "posts/2026-01-22_data_leakage/index.html#introduction-why-this-topic-matters",
    "title": "Data Leakage in R: Why Correct Evaluation Matters Even When Metrics Do Not Change",
    "section": "1 Introduction – Why This Topic Matters",
    "text": "1 Introduction – Why This Topic Matters\nA model that performs exceptionally well on a test set is not necessarily a good model; in many cases, it is a warning sign. High accuracy or low error metrics are meaningful only if we understand how they were obtained. In real-world settings, models rarely encounter data generated under the same conditions as the training phase: data arrive sequentially, delays occur, missingness patterns change, and measurement errors accumulate. Under such conditions, impressive validation metrics can quickly lose their relevance.\nA common scenario in applied data science is deceptively familiar. During development, the model looks flawless: cross-validation results are stable, performance metrics are strong, and diagnostic plots inspire confidence. Once deployed, however, performance deteriorates—sometimes rapidly. Forecasts drift, classification decisions become unreliable, and stakeholders begin to question the entire modeling pipeline. While this failure is often attributed to distributional shift or concept drift, a more fundamental issue is frequently overlooked: the model was exposed, directly or indirectly, to information it would not have access to at prediction time.\nThis phenomenon is known as data leakage. Importantly, data leakage is rarely the result of an obvious coding mistake. More often, it emerges from subtle flaws in experimental design, preprocessing order, or feature construction decisions made well before the model is fitted. As a result, leakage can silently inflate performance metrics, creating models that appear robust on paper but collapse in practice.\n&gt; “A model that performs perfectly on paper but fails miserably in practice is often a victim of data leakage.”\nIn this article, we examine data leakage not as a technical curiosity, but as a structural threat to valid statistical modeling. We begin by clarifying what data leakage is—and what it is not—before demonstrating, using a real dataset and R-based workflows, how seemingly reasonable preprocessing choices can contaminate model evaluation. We then reconstruct the same analysis using a leakage-free pipeline, highlighting the practical and conceptual differences through numerical results and carefully designed visualizations."
  },
  {
    "objectID": "posts/2026-01-22_data_leakage/index.html#what-is-data-leakage",
    "href": "posts/2026-01-22_data_leakage/index.html#what-is-data-leakage",
    "title": "Data Leakage in R: Why Correct Evaluation Matters Even When Metrics Do Not Change",
    "section": "2 What Is Data Leakage?",
    "text": "2 What Is Data Leakage?\nAt its core, data leakage occurs when information that would not be available at prediction time is inadvertently used during model training or evaluation. This information can enter the modeling pipeline in subtle ways—often long before a model is fitted—leading to overly optimistic performance estimates. The critical issue is not that the model “cheats,” but that the experimental setup allows future or target-related information to influence learning.\nFormally, consider a supervised learning problem where we aim to estimate a function:\n\\[\nf : \\mathcal{X} \\rightarrow \\mathcal{Y}\n\\]\nusing a training set \\((X_{\\text{train}}, y_{\\text{train}})\\) and evaluate it on a test set \\((X_{\\text{test}}, y_{\\text{test}})\\). A valid evaluation assumes that \\(X_{\\text{test}}\\) is generated independently of \\(y_{\\text{train}}\\) and that no function of \\(y_{\\text{test}}\\) influences the training process. Data leakage violates this assumption by introducing a dependency—direct or indirect—between training and test information.\n\n2.1 What Data Leakage Is Not\nData leakage is often confused with other, related modeling issues. Clarifying these distinctions is essential.\n\nOverfitting refers to a model learning noise or idiosyncrasies in the training data. While overfitted models generalize poorly, they do not necessarily rely on forbidden information.\nData snooping involves repeated testing and model selection on the same validation set. This inflates performance through selection bias, but the data themselves are not structurally contaminated.\nDistribution shift (or concept drift) occurs when the data-generating process changes over time. This is a real-world phenomenon, not a methodological error.\n\nIn contrast, data leakage is a violation of the temporal or logical boundary between training and prediction. It creates an artificial setting in which the model has access to information it should not logically possess.\n\n\n2.2 Common Forms of Data Leakage\nData leakage can be broadly categorized into three practical forms:\n\nTarget Leakage\nPredictors encode information that is directly derived from, or strongly dependent on, the target variable. For example, constructing a feature using an outcome measured after the event being predicted.\nTrain–Test Contamination\nInformation from the test set influences preprocessing steps such as scaling, imputation, or feature selection. This often happens when transformations are applied to the full dataset before splitting.\nTemporal Leakage\nFuture observations leak into the past, a particularly common issue in time series and forecasting contexts. Rolling averages, lag structures, or normalization computed using future data fall into this category.\n\n\n\n2.3 A Simple Conceptual Example\nSuppose we aim to predict apartment prices using listing characteristics. If missing values in the price variable are imputed using the global mean price computed over the entire dataset, and the train–test split is performed afterward, then information from the test set has already influenced the training process. The model evaluation is no longer an honest simulation of future performance.\nThis type of leakage is especially dangerous because it often produces stable and impressive metrics, giving practitioners a false sense of security. The model appears reliable not because it has learned a robust relationship, but because the evaluation framework itself is compromised.\nIn the next section, we move from definitions to practice. Using a real dataset, we will deliberately construct a seemingly reasonable—but flawed—preprocessing pipeline and observe how data leakage manifests itself through inflated performance metrics."
  },
  {
    "objectID": "posts/2026-01-22_data_leakage/index.html#common-sources-of-data-leakage-in-practice",
    "href": "posts/2026-01-22_data_leakage/index.html#common-sources-of-data-leakage-in-practice",
    "title": "Data Leakage in R: Why Correct Evaluation Matters Even When Metrics Do Not Change",
    "section": "3 Common Sources of Data Leakage in Practice",
    "text": "3 Common Sources of Data Leakage in Practice\nData leakage rarely appears as an obvious error. In practice, it is often the result of reasonable-looking preprocessing decisions applied in the wrong order or under incorrect assumptions. This section outlines the most common sources of leakage encountered in applied statistical modeling and machine learning workflows, with a particular focus on preprocessing stages that precede model fitting.\n\n3.1 Leakage During Data Preprocessing\nOne of the most frequent sources of data leakage occurs during data preprocessing. Operations such as centering, scaling, normalization, and missing-value imputation are often applied mechanically to the entire dataset before any data splitting takes place. While this approach may seem harmless, it implicitly allows information from the test set to influence transformations applied to the training data.\nFor example, consider standardization using the sample mean \\(\\mu\\) and standard deviation \\(\\sigma\\). If these quantities are computed using the full dataset rather than the training subset alone, then statistics derived from the test data directly affect the transformed training observations. As a result, the model is evaluated in an artificially favorable setting that will never occur in real-world prediction.\n\n\n3.2 Leakage Through Feature Engineering\nFeature engineering is another common entry point for leakage, particularly when new variables are constructed using aggregated information. Group-level statistics—such as averages, frequencies, or ranks—can easily encode target-related information if computed without respecting the train–test boundary.\nA typical example involves creating neighborhood-level average prices in a housing dataset. If these averages are calculated using all available observations, including those later assigned to the test set, the resulting features implicitly incorporate information from unseen data. The model appears to generalize well, but only because future information has already been embedded in the predictors.\n\n\n3.3 Leakage from Improper Train–Test Splitting\nIn many workflows, data splitting is treated as a purely mechanical step. However, when and how the split is performed matters greatly. Random splits applied after preprocessing steps allow contamination to propagate silently. This issue is exacerbated in small or moderately sized datasets, where even minor information leakage can have a disproportionate effect on evaluation metrics.\nThe fundamental principle is simple: any operation that learns from the data must be performed exclusively on the training set. The learned transformation can then be applied to the test set—but never re-estimated using it.\n\n\n3.4 Temporal Leakage in Time-Dependent Data\nTime-dependent data introduce an additional and particularly dangerous form of leakage: temporal leakage. This occurs when future observations influence the representation of past data. Common examples include rolling statistics computed using symmetric windows, global normalization across time, or lagged features that unintentionally incorporate future values.\nIn forecasting and time series analysis, such leakage violates the chronological ordering of information. The model effectively gains access to future states of the system, leading to performance estimates that are fundamentally invalid. Unlike random contamination, temporal leakage often produces extremely smooth and stable validation results—precisely because the future is partially known.\n\n\n3.5 Why These Issues Are Hard to Detect\nWhat makes data leakage especially problematic is not its complexity, but its subtlety. Leakage-prone pipelines often run without errors, produce clean outputs, and yield impressive metrics. In many cases, the only warning sign is performance that seems too consistent or too good to be true.\nCrucially, standard validation techniques cannot detect leakage if the underlying data-generating assumptions have already been violated. Once contamination occurs, even rigorous cross-validation merely reinforces a flawed evaluation framework.\nIn the next section, we will make these ideas concrete by constructing a deliberately flawed preprocessing pipeline using a real dataset. By examining the resulting performance metrics and visual diagnostics, we will observe how data leakage manifests itself in practice."
  },
  {
    "objectID": "posts/2026-01-22_data_leakage/index.html#dataset-description-airbnb-listings-data",
    "href": "posts/2026-01-22_data_leakage/index.html#dataset-description-airbnb-listings-data",
    "title": "Data Leakage in R: Why Correct Evaluation Matters Even When Metrics Do Not Change",
    "section": "4 Dataset Description: Airbnb Listings Data",
    "text": "4 Dataset Description: Airbnb Listings Data\nTo demonstrate how data leakage arises in practice, we use a real-world dataset derived from Airbnb listings. The dataset is obtained from the publicly available Inside Airbnb project, which provides detailed, regularly updated information on short-term rental listings for major cities worldwide. In this study, we focus on the Istanbul listings, which offer a rich combination of numerical and categorical variables and exhibit common data quality issues encountered in applied modeling tasks.\nThe Inside Airbnb project aims to support research, policy analysis, and public discussion by making scraped Airbnb data openly accessible. The dataset includes listing-level attributes such as pricing information, accommodation characteristics, host-related variables, and geographic identifiers. Due to its size, heterogeneity, and real-world imperfections, it provides an ideal setting for illustrating preprocessing pitfalls and evaluation errors.\n\n4.1 Data Source\nThe data are publicly available at:\n\nhttps://insideairbnb.com/get-the-data/\n\nFor reproducibility, the analysis in this article uses a snapshot of the Istanbul listings dataset downloaded directly from the source. While the exact number of observations may vary across releases, the structure and modeling challenges remain consistent across versions.\n\n\n4.2 Target Variable and Modeling Objective\nOur primary modeling objective is to predict the listing price based on observable characteristics of the property and its location. The target variable, denoted by \\(y\\), corresponds to the nightly price of a listing in local currency units.\nPrice prediction in short-term rental data is a well-studied problem and serves as a natural example for illustrating data leakage. Importantly, price exhibits:\n\nstrong right skewness,\nsubstantial heterogeneity across neighborhoods,\nsensitivity to aggregation and preprocessing choices.\n\nThese properties make the variable particularly vulnerable to leakage through global transformations and improperly constructed features.\n\n\n4.3 Predictor Variables\nThe predictor set includes a mix of numerical and categorical variables commonly used in pricing models, such as:\n\naccommodation capacity (e.g., number of guests),\nroom type and property type,\nneighborhood identifiers,\navailability-related measures,\nhost characteristics.\n\nSeveral variables contain missing values, and many exhibit heavy-tailed distributions. These features necessitate preprocessing steps such as imputation, scaling, and transformation—precisely the stages where data leakage most often occurs.\n\n\n4.4 Why This Dataset Is Suitable for Studying Data Leakage\nThis dataset is especially well-suited for examining data leakage for three reasons. First, it requires nontrivial preprocessing to be usable for modeling, increasing the risk of incorrect transformation order. Second, it includes categorical groupings (such as neighborhoods) that invite aggregation-based feature engineering, a common source of target leakage. Third, its real-world origin ensures that modeling assumptions—such as stationarity, completeness, and clean measurement—are only approximately satisfied.\nBy working with this dataset, we intentionally place ourselves in a realistic applied setting, where leakage is not an abstract concept but a tangible risk. In the next section, we construct a seemingly reasonable preprocessing pipeline that violates key evaluation principles, allowing us to observe how data leakage inflates model performance in practice."
  },
  {
    "objectID": "posts/2026-01-22_data_leakage/index.html#a-naive-preprocessing-pipeline-and-why-it-is-wrong",
    "href": "posts/2026-01-22_data_leakage/index.html#a-naive-preprocessing-pipeline-and-why-it-is-wrong",
    "title": "Data Leakage in R: Why Correct Evaluation Matters Even When Metrics Do Not Change",
    "section": "5 A Naive Preprocessing Pipeline (And Why It Is Wrong)",
    "text": "5 A Naive Preprocessing Pipeline (And Why It Is Wrong)\nAt first glance, many preprocessing pipelines appear perfectly reasonable. Data are cleaned, missing values are handled, variables are scaled, and only then is the dataset split into training and test sets. This workflow is intuitive, easy to implement, and—most importantly—widely used. Unfortunately, it is also fundamentally flawed.\nIn this section, we deliberately construct such a naive pipeline to illustrate how data leakage can arise without any obvious warning signs.\n\n5.1 Step 1: Loading and Preparing the Data\nWe begin by loading the Airbnb listings data and selecting a subset of variables commonly used for price prediction. For simplicity, we focus on numerical predictors that require minimal encoding.\n\nlibrary(tidyverse)\nlibrary(rsample)\n\n# Load data (example assumes listings.csv from Inside Airbnb)\nairbnb &lt;- read_csv(\"listings.csv\")\n\nairbnb_model &lt;- airbnb %&gt;%\n  select(\n    price,\n    accommodates,\n    bedrooms,\n    bathrooms,\n    minimum_nights,\n    availability_365\n  ) %&gt;%\n  mutate(\n    price = as.numeric(str_remove_all(price, \"[$,]\"))\n  )\n\nAt this stage, the dataset already contains missing values and variables with highly skewed distributions—a realistic and unavoidable situation in applied work.\n\n\n5.2 Step 2: Global Preprocessing (The Critical Mistake)\nA common approach is to perform preprocessing once on the full dataset. Below, we impute missing values using the global mean and standardize all predictors using statistics computed from the entire dataset.\n\nairbnb_preprocessed &lt;- airbnb_model %&gt;%\n  mutate(across(\n    .cols = -price,\n    .fns  = ~ ifelse(is.na(.x), mean(.x, na.rm = TRUE), .x)\n  )) %&gt;%\n  mutate(across(\n    .cols = -price,\n    .fns  = scale\n  ))\n\nFrom a purely technical perspective, this code runs without errors and produces clean, well-behaved predictors. However, the preprocessing steps above implicitly use information from all observations, including those that will later be assigned to the test set.\nAt this point, data leakage has already occurred.\n\n\n5.3 Step 3: Train–Test Split After Preprocessing\nNext, we perform a random split of the preprocessed data into training and test sets.\n\nset.seed(123)\n\nsplit &lt;- initial_split(airbnb_preprocessed, prop = 0.8)\ntrain_data &lt;- training(split)\ntest_data  &lt;- testing(split)\n\nBecause the split is applied after preprocessing, the training data have been standardized and imputed using statistics influenced by the test data. The train–test boundary, while present in code, has already been violated in substance.\n\n\n5.4 Step 4: Model Fitting and Evaluation\nWe now fit a simple linear regression model using the training data and evaluate its predictive performance on the test set. At this stage, the goal is not to build an optimal model, but to assess how the evaluation framework itself can be compromised by data leakage.\n\n# Fit a linear regression model on the training data\nmodel_naive &lt;- lm(price ~ ., data = train_data)\n\n# Generate predictions for the test set\npred_test &lt;- predict(model_naive, newdata = test_data)\n\nTo compute a supervised performance metric, we must restrict the evaluation to test observations for which the target variable is observed. Listings with missing prices cannot contribute to an error metric such as RMSE, as no ground truth is available.\n\n# Create an evaluation dataset with observed targets only\neval_df &lt;- test_data %&gt;%\n  transmute(\n    price = price,\n    pred  = pred_test\n  ) %&gt;%\n  filter(!is.na(price), !is.na(pred))\n\n# Root Mean Squared Error\nrmse_naive &lt;- sqrt(mean((eval_df$price - eval_df$pred)^2))\nrmse_naive\n\n[1] 21213.69\n\n\nThe computed RMSE provides a single-point estimate of out-of-sample error under this evaluation setup. However, the absolute magnitude of this value is difficult to interpret in isolation because it depends on the scale and distribution of the target variable (price). More importantly for this article, the key concern is methodological: preprocessing steps were estimated using the full dataset before splitting, which compromises the train–test separation and can lead to overly optimistic performance estimates.\nIn the next section, we will evaluate this suspicion more systematically by repeating the procedure across multiple random splits and inspecting the distribution of performance metrics."
  },
  {
    "objectID": "posts/2026-01-22_data_leakage/index.html#detecting-data-leakage-repeated-splits-and-performance-distributions",
    "href": "posts/2026-01-22_data_leakage/index.html#detecting-data-leakage-repeated-splits-and-performance-distributions",
    "title": "Data Leakage in R: Why Correct Evaluation Matters Even When Metrics Do Not Change",
    "section": "6 Detecting Data Leakage: Repeated Splits and Performance Distributions",
    "text": "6 Detecting Data Leakage: Repeated Splits and Performance Distributions\nA single train–test split provides only a point estimate of model performance. To assess whether the suspiciously favorable evaluation observed earlier is a coincidence or a structural issue, we repeat the naive preprocessing and evaluation procedure across multiple random splits of the data. This allows us to examine the distribution of performance metrics rather than relying on a single value.\n\n6.1 Repeated Evaluation Under the Naive Pipeline\nWe repeat the following steps multiple times: 1. Randomly split the data into training and test sets. 2. Fit the model on the training data. 3. Compute RMSE on the test data using observed targets only.\nCrucially, the same flawed preprocessing pipeline is retained, meaning that scaling and imputation are still performed on the full dataset prior to splitting.\n\nset.seed(123)\n\nn_repeats &lt;- 30\nrmse_values &lt;- numeric(n_repeats)\n\nfor (i in seq_len(n_repeats)) {\n  \n  split_i &lt;- initial_split(airbnb_preprocessed, prop = 0.8)\n  train_i &lt;- training(split_i)\n  test_i  &lt;- testing(split_i)\n  \n  model_i &lt;- lm(price ~ ., data = train_i)\n  pred_i  &lt;- predict(model_i, newdata = test_i)\n  \n  eval_i &lt;- tibble(\n    price = test_i$price,\n    pred  = pred_i\n  ) %&gt;%\n    filter(!is.na(price), !is.na(pred))\n  \n  rmse_values[i] &lt;- sqrt(mean((eval_i$price - eval_i$pred)^2))\n}\n\nrmse_df &lt;- tibble(\n  iteration = seq_len(n_repeats),\n  rmse      = rmse_values\n)\n\n\n\n6.2 Inspecting the RMSE Distribution\nRather than focusing on individual values, we now inspect the distribution of RMSE across repeated splits.\n\nlibrary(ggplot2)\n\nggplot(rmse_df, aes(x = rmse)) +\n  geom_histogram(bins = 15, fill = \"#4C72B0\", color = \"white\") +\n  geom_vline(xintercept = mean(rmse_df$rmse), \n             linetype = \"dashed\", \n             linewidth = 1) +\n  labs(\n    title = \"RMSE Distribution Under Naive Preprocessing\",\n    subtitle = \"Repeated random train–test splits\",\n    x = \"RMSE\",\n    y = \"Count\"\n  ) +\n  theme_minimal(base_size = 12)\n\n\n\n\n\n\n\n\n\n\n6.3 Interpretation\nThe RMSE values obtained across repeated random splits exhibit substantial variability, spanning a wide range rather than concentrating around a narrow interval. This degree of dispersion reflects the heterogeneity of the data and the sensitivity of the model to different training–test partitions.\nImportantly, this result highlights a key limitation of relying on a single train–test split: performance estimates can vary dramatically depending on how the data are partitioned. At this stage, the variability itself does not constitute evidence of data leakage. Instead, it establishes a baseline level of uncertainty against which alternative preprocessing strategies must be evaluated.\nIn the following section, we will repeat the same experiment using a leakage-free preprocessing pipeline. By comparing the resulting RMSE distributions, we can assess whether improper preprocessing leads to systematically optimistic or distorted performance estimates."
  },
  {
    "objectID": "posts/2026-01-22_data_leakage/index.html#a-leakage-free-preprocessing-pipeline",
    "href": "posts/2026-01-22_data_leakage/index.html#a-leakage-free-preprocessing-pipeline",
    "title": "Data Leakage in R: Why Correct Evaluation Matters Even When Metrics Do Not Change",
    "section": "7 A Leakage-Free Preprocessing Pipeline",
    "text": "7 A Leakage-Free Preprocessing Pipeline\nTo assess whether the previously observed behavior is driven by improper preprocessing, we now reconstruct the entire workflow using a leakage-free pipeline. The key principle is simple but fundamental: any transformation that learns from the data must be estimated using the training set only and then applied to the test set without re-estimation.\n\n7.1 Correct Order of Operations\nThe leakage-free workflow follows this sequence:\n\nSplit the data into training and test sets.\nEstimate preprocessing parameters using the training data only.\nApply the learned transformations to both training and test sets.\nFit the model on the transformed training data.\nEvaluate performance on the transformed test data.\n\nThis ordering mirrors real-world deployment, where future observations arrive without access to global dataset statistics.\n\n\n7.2 Implementing Leakage-Free Preprocessing in R\nWe begin by repeating the evaluation procedure across multiple random splits, as in the previous section. This time, however, preprocessing steps are learned exclusively from the training data.\n\nset.seed(123)\n\nn_repeats &lt;- 30\nrmse_correct &lt;- numeric(n_repeats)\n\nfor (i in seq_len(n_repeats)) {\n  \n  # Split first\n  split_i &lt;- initial_split(airbnb_model, prop = 0.8)\n  train_raw &lt;- training(split_i)\n  test_raw  &lt;- testing(split_i)\n  \n  # Estimate preprocessing parameters on training data only\n  train_processed &lt;- train_raw %&gt;%\n    mutate(across(\n      .cols = -price,\n      .fns  = ~ ifelse(is.na(.x), mean(.x, na.rm = TRUE), .x)\n    ))\n  \n  scaling_params &lt;- train_processed %&gt;%\n    summarise(across(-price, list(mean = mean, sd = sd), na.rm = TRUE))\n  \n  scale_train &lt;- function(x, m, s) {\n    ifelse(s &gt; 0, (x - m) / s, 0)\n  }\n  \n  for (v in names(train_processed)[names(train_processed) != \"price\"]) {\n    m &lt;- scaling_params[[paste0(v, \"_mean\")]]\n    s &lt;- scaling_params[[paste0(v, \"_sd\")]]\n    \n    train_processed[[v]] &lt;- scale_train(train_processed[[v]], m, s)\n  }\n  \n  # Apply the same transformations to the test set\n  test_processed &lt;- test_raw %&gt;%\n    mutate(across(\n      .cols = -price,\n      .fns  = ~ ifelse(is.na(.x), mean(train_raw[[cur_column()]], na.rm = TRUE), .x)\n    ))\n  \n  for (v in names(test_processed)[names(test_processed) != \"price\"]) {\n    m &lt;- scaling_params[[paste0(v, \"_mean\")]]\n    s &lt;- scaling_params[[paste0(v, \"_sd\")]]\n    \n    test_processed[[v]] &lt;- scale_train(test_processed[[v]], m, s)\n  }\n  \n  # Fit model\n  model_i &lt;- lm(price ~ ., data = train_processed)\n  pred_i  &lt;- predict(model_i, newdata = test_processed)\n  \n  # Evaluate where target is observed\n  eval_i &lt;- tibble(\n    price = test_processed$price,\n    pred  = pred_i\n  ) %&gt;%\n    filter(!is.na(price), !is.na(pred))\n  \n  rmse_correct[i] &lt;- sqrt(mean((eval_i$price - eval_i$pred)^2))\n}\n\nrmse_correct_df &lt;- tibble(\n  iteration = seq_len(n_repeats),\n  rmse      = rmse_correct\n)\n\n\n\n7.3 Comparing Performance Distributions\nWe now compare RMSE distributions obtained under the naive and leakage-free preprocessing pipelines.\n\nrmse_compare &lt;- bind_rows(\n  rmse_df %&gt;% mutate(pipeline = \"Naive preprocessing\"),\n  rmse_correct_df %&gt;% mutate(pipeline = \"Leakage-free preprocessing\")\n)\n\nggplot(rmse_compare, aes(x = rmse, fill = pipeline)) +\n  geom_histogram(position = \"identity\", alpha = 0.6, bins = 15) +\n  labs(\n    title = \"RMSE Distributions Under Different Preprocessing Pipelines\",\n    subtitle = \"Naive vs leakage-free evaluation\",\n    x = \"RMSE\",\n    y = \"Count\"\n  ) +\n  theme_minimal(base_size = 12)\n\n\n\n\n\n\n\n\n\n\n7.4 Interpretation\nThe RMSE distributions obtained under the naive and leakage-free preprocessing pipelines are nearly indistinguishable. Across repeated random splits, both approaches yield similar ranges, central tendencies, and tail behavior. Visually, the two histograms largely overlap, causing the leakage-free distribution to be obscured in the combined plot; this overlap itself reflects the near-identical numerical behavior of the two pipelines under the present modeling setup.\nThis result demonstrates an important but often overlooked point: data leakage does not always lead to dramatic or easily detectable performance inflation. In some settings—particularly with simple models and highly variable targets—the numerical impact of leakage may be minimal, even though the evaluation procedure remains theoretically flawed.\nCrucially, the absence of a visible performance gap does not validate the naive pipeline. Instead, it highlights the need to assess preprocessing decisions based on methodological correctness rather than empirical convenience. In other contexts, datasets, or modeling frameworks, the same mistake could lead to substantial and misleading performance gains."
  },
  {
    "objectID": "posts/2026-01-22_data_leakage/index.html#conclusion",
    "href": "posts/2026-01-22_data_leakage/index.html#conclusion",
    "title": "Data Leakage in R: Why Correct Evaluation Matters Even When Metrics Do Not Change",
    "section": "8 Conclusion",
    "text": "8 Conclusion\nThis article set out with a seemingly straightforward question: can data leakage lead to misleadingly strong model performance? The empirical results presented here suggest a more nuanced answer. In the examined setting—using a simple linear model and a highly heterogeneous real-world dataset—improper preprocessing did not result in dramatic or easily detectable performance inflation. Naive and leakage-free pipelines produced nearly identical error distributions.\nHowever, this outcome does not diminish the importance of data leakage. On the contrary, it highlights its most insidious characteristic: data leakage is dangerous precisely because it does not always announce itself through obvious performance gains. Evaluation metrics may remain unchanged, stable, or even reasonable, while the underlying logic of the evaluation has already been violated.\nThe central lesson is therefore not about performance optimization, but about validity. Correct model evaluation is a matter of respecting information boundaries—temporal, logical, and structural—regardless of whether immediate numerical consequences are visible. Relying on empirically convenient shortcuts simply because they “seem to work” risks building pipelines that fail silently when transferred to new data, different models, or operational settings.\nUltimately, data leakage should be treated as a methodological error, not a performance issue. Thinking carefully about preprocessing order, information flow, and evaluation design is not optional; it is a prerequisite for trustworthy statistical modeling."
  },
  {
    "objectID": "posts/2026-01-22_data_leakage/index.html#references",
    "href": "posts/2026-01-22_data_leakage/index.html#references",
    "title": "Data Leakage in R: Why Correct Evaluation Matters Even When Metrics Do Not Change",
    "section": "9 References",
    "text": "9 References\n\nHastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning: Data Mining, Inference, and Prediction (2nd ed.). Springer.\nhttps://doi.org/10.1007/978-0-387-84858-7\nKuhn, M., & Johnson, K. (2013). Applied Predictive Modeling. Springer.\nhttps://doi.org/10.1007/978-1-4614-6849-3\nKuhn, M., & Silge, J. (2022). Tidy Modeling with R. O’Reilly Media.\nhttps://www.tmwr.org/\nScikit-learn documentation. (n.d.). Common pitfalls in machine learning.\nhttps://scikit-learn.org/stable/common_pitfalls.html\nInside Airbnb. (n.d.). Get the data.\nhttps://insideairbnb.com/get-the-data/"
  }
]