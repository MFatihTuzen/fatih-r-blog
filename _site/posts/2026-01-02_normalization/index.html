<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="M. Fatih Tüzen">
<meta name="dcterms.date" content="2026-01-02">

<title>Data Normalization in R: When, Why, and How to Scale Your Data Correctly – A Statistician’s R Notebook</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-707d8167ce6003fca903bfe2be84ab7f.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-ffa9a6279353761231ec249df0a7fdd3.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-24772959a0e306aba8d7698078628ce7.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../site_libs/bootstrap/bootstrap-ffa9a6279353761231ec249df0a7fdd3.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="floating nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    </a>
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">A Statistician’s R Notebook</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/MFatihTuzen"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/dr-m-fatih-t-2b2a4328/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../index.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-blogroll" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">BlogRoll</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-blogroll">    
        <li>
    <a class="dropdown-item" href="https://www.r-bloggers.com/">
 <span class="dropdown-text">R-bloggers</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://community.rstudio.com/">
 <span class="dropdown-text">R-Studio Community</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://rweekly.org/">
 <span class="dropdown-text">R weekly</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://allisonhorst.com/allison-horst">
 <span class="dropdown-text">Allison Horst</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/topics/r">
 <span class="dropdown-text">Github R Topics</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title">Data Normalization in R: When, Why, and How to Scale Your Data Correctly</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
                                <div class="quarto-categories">
                <div class="quarto-category">Data Preprocessing</div>
                <div class="quarto-category">R Programming</div>
                <div class="quarto-category">Data Science</div>
                <div class="quarto-category">Machine Learning</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>M. Fatih Tüzen </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">January 2, 2026</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">1</span> Introduction</a></li>
  <li><a href="#normalization-vs.-standardization-clearing-up-the-terminology" id="toc-normalization-vs.-standardization-clearing-up-the-terminology" class="nav-link" data-scroll-target="#normalization-vs.-standardization-clearing-up-the-terminology"><span class="header-section-number">2</span> Normalization vs.&nbsp;Standardization: Clearing Up the Terminology</a></li>
  <li><a href="#why-is-normalization-necessary" id="toc-why-is-normalization-necessary" class="nav-link" data-scroll-target="#why-is-normalization-necessary"><span class="header-section-number">3</span> Why Is Normalization Necessary?</a></li>
  <li><a href="#should-normalization-always-be-applied" id="toc-should-normalization-always-be-applied" class="nav-link" data-scroll-target="#should-normalization-always-be-applied"><span class="header-section-number">4</span> Should Normalization Always Be Applied?</a>
  <ul>
  <li><a href="#when-is-normalization-needed-a-model-based-decision-table" id="toc-when-is-normalization-needed-a-model-based-decision-table" class="nav-link" data-scroll-target="#when-is-normalization-needed-a-model-based-decision-table"><span class="header-section-number">4.1</span> When Is Normalization Needed? A Model-Based Decision Table</a></li>
  </ul></li>
  <li><a href="#when-should-normalization-be-applied-before-or-after-the-traintest-split" id="toc-when-should-normalization-be-applied-before-or-after-the-traintest-split" class="nav-link" data-scroll-target="#when-should-normalization-be-applied-before-or-after-the-traintest-split"><span class="header-section-number">5</span> When Should Normalization Be Applied? Before or After the Train–Test Split?</a>
  <ul>
  <li><a href="#an-illustrative-example-scaling-before-vs.-after-the-split" id="toc-an-illustrative-example-scaling-before-vs.-after-the-split" class="nav-link" data-scroll-target="#an-illustrative-example-scaling-before-vs.-after-the-split"><span class="header-section-number">5.1</span> An Illustrative Example: Scaling Before vs.&nbsp;After the Split</a></li>
  </ul></li>
  <li><a href="#common-normalization-methods-and-when-to-use-them" id="toc-common-normalization-methods-and-when-to-use-them" class="nav-link" data-scroll-target="#common-normalization-methods-and-when-to-use-them"><span class="header-section-number">6</span> Common Normalization Methods and When to Use Them</a>
  <ul>
  <li><a href="#z-score-standardization" id="toc-z-score-standardization" class="nav-link" data-scroll-target="#z-score-standardization"><span class="header-section-number">6.1</span> Z-score Standardization</a></li>
  <li><a href="#minmax-range-scaling" id="toc-minmax-range-scaling" class="nav-link" data-scroll-target="#minmax-range-scaling"><span class="header-section-number">6.2</span> Min–Max (Range) Scaling</a></li>
  <li><a href="#robust-scaling-median-and-iqr" id="toc-robust-scaling-median-and-iqr" class="nav-link" data-scroll-target="#robust-scaling-median-and-iqr"><span class="header-section-number">6.3</span> Robust Scaling (Median and IQR)</a></li>
  <li><a href="#power-transformations-combined-with-scaling-boxcox-and-yeojohnson" id="toc-power-transformations-combined-with-scaling-boxcox-and-yeojohnson" class="nav-link" data-scroll-target="#power-transformations-combined-with-scaling-boxcox-and-yeojohnson"><span class="header-section-number">6.4</span> Power Transformations Combined with Scaling (Box–Cox and Yeo–Johnson)</a></li>
  <li><a href="#choosing-a-method-no-single-best-answer" id="toc-choosing-a-method-no-single-best-answer" class="nav-link" data-scroll-target="#choosing-a-method-no-single-best-answer"><span class="header-section-number">6.5</span> Choosing a Method: No Single Best Answer</a></li>
  </ul></li>
  <li><a href="#do-different-data-types-require-different-scaling-strategies" id="toc-do-different-data-types-require-different-scaling-strategies" class="nav-link" data-scroll-target="#do-different-data-types-require-different-scaling-strategies"><span class="header-section-number">7</span> Do Different Data Types Require Different Scaling Strategies?</a>
  <ul>
  <li><a href="#continuous-numeric-variables" id="toc-continuous-numeric-variables" class="nav-link" data-scroll-target="#continuous-numeric-variables"><span class="header-section-number">7.1</span> Continuous Numeric Variables</a></li>
  <li><a href="#count-and-ordinal-numeric-variables" id="toc-count-and-ordinal-numeric-variables" class="nav-link" data-scroll-target="#count-and-ordinal-numeric-variables"><span class="header-section-number">7.2</span> Count and Ordinal Numeric Variables</a></li>
  <li><a href="#categorical-variables-factors-or-characters" id="toc-categorical-variables-factors-or-characters" class="nav-link" data-scroll-target="#categorical-variables-factors-or-characters"><span class="header-section-number">7.3</span> Categorical Variables (Factors or Characters)</a></li>
  <li><a href="#binary-variables-01-indicators" id="toc-binary-variables-01-indicators" class="nav-link" data-scroll-target="#binary-variables-01-indicators"><span class="header-section-number">7.4</span> Binary Variables (0/1 Indicators)</a></li>
  <li><a href="#summary-scaling-depends-on-variable-meaning" id="toc-summary-scaling-depends-on-variable-meaning" class="nav-link" data-scroll-target="#summary-scaling-depends-on-variable-meaning"><span class="header-section-number">7.5</span> Summary: Scaling Depends on Variable Meaning</a></li>
  </ul></li>
  <li><a href="#should-all-variables-be-scaled" id="toc-should-all-variables-be-scaled" class="nav-link" data-scroll-target="#should-all-variables-be-scaled"><span class="header-section-number">8</span> Should All Variables Be Scaled?</a>
  <ul>
  <li><a href="#the-target-variable-y" id="toc-the-target-variable-y" class="nav-link" data-scroll-target="#the-target-variable-y"><span class="header-section-number">8.1</span> The Target Variable (y)</a></li>
  <li><a href="#predictor-variables" id="toc-predictor-variables" class="nav-link" data-scroll-target="#predictor-variables"><span class="header-section-number">8.2</span> Predictor Variables</a>
  <ul class="collapse">
  <li><a href="#numeric-predictors-only" id="toc-numeric-predictors-only" class="nav-link" data-scroll-target="#numeric-predictors-only"><span class="header-section-number">8.2.1</span> Numeric Predictors Only</a></li>
  <li><a href="#excluding-non-informative-numeric-variables" id="toc-excluding-non-informative-numeric-variables" class="nav-link" data-scroll-target="#excluding-non-informative-numeric-variables"><span class="header-section-number">8.2.2</span> Excluding Non-informative Numeric Variables</a></li>
  <li><a href="#handling-low-variance-predictors" id="toc-handling-low-variance-predictors" class="nav-link" data-scroll-target="#handling-low-variance-predictors"><span class="header-section-number">8.2.3</span> Handling Low-Variance Predictors</a></li>
  </ul></li>
  <li><a href="#a-practical-rule-of-thumb" id="toc-a-practical-rule-of-thumb" class="nav-link" data-scroll-target="#a-practical-rule-of-thumb"><span class="header-section-number">8.3</span> A Practical Rule of Thumb</a></li>
  </ul></li>
  <li><a href="#application-plan-in-r-data-and-modeling-scenario" id="toc-application-plan-in-r-data-and-modeling-scenario" class="nav-link" data-scroll-target="#application-plan-in-r-data-and-modeling-scenario"><span class="header-section-number">9</span> Application Plan in R: Data and Modeling Scenario</a>
  <ul>
  <li><a href="#modeling-objective" id="toc-modeling-objective" class="nav-link" data-scroll-target="#modeling-objective"><span class="header-section-number">9.1</span> Modeling Objective</a></li>
  <li><a href="#scope-and-focus" id="toc-scope-and-focus" class="nav-link" data-scroll-target="#scope-and-focus"><span class="header-section-number">9.2</span> Scope and Focus</a></li>
  <li><a href="#transition-to-implementation" id="toc-transition-to-implementation" class="nav-link" data-scroll-target="#transition-to-implementation"><span class="header-section-number">9.3</span> Transition to Implementation</a></li>
  <li><a href="#data-access-and-availability" id="toc-data-access-and-availability" class="nav-link" data-scroll-target="#data-access-and-availability"><span class="header-section-number">9.4</span> Data Access and Availability</a></li>
  </ul></li>
  <li><a href="#implementation-in-r-split-baseline-and-the-cost-of-doing-it-wrong" id="toc-implementation-in-r-split-baseline-and-the-cost-of-doing-it-wrong" class="nav-link" data-scroll-target="#implementation-in-r-split-baseline-and-the-cost-of-doing-it-wrong"><span class="header-section-number">10</span> Implementation in R: Split, Baseline, and the Cost of Doing It Wrong</a>
  <ul>
  <li><a href="#setup-and-variable-selection" id="toc-setup-and-variable-selection" class="nav-link" data-scroll-target="#setup-and-variable-selection"><span class="header-section-number">10.1</span> Setup and Variable Selection</a></li>
  <li><a href="#load-data-and-create-a-working-dataset" id="toc-load-data-and-create-a-working-dataset" class="nav-link" data-scroll-target="#load-data-and-create-a-working-dataset"><span class="header-section-number">10.2</span> Load Data and Create a Working Dataset</a></li>
  <li><a href="#traintest-split-and-evaluation-setup" id="toc-traintest-split-and-evaluation-setup" class="nav-link" data-scroll-target="#traintest-split-and-evaluation-setup"><span class="header-section-number">10.3</span> Train–Test Split and Evaluation Setup</a>
  <ul class="collapse">
  <li><a href="#create-a-stratified-traintest-split" id="toc-create-a-stratified-traintest-split" class="nav-link" data-scroll-target="#create-a-stratified-traintest-split"><span class="header-section-number">10.3.1</span> Create a Stratified Train–Test Split</a></li>
  <li><a href="#sanity-check-is-the-target-distribution-similar-across-splits" id="toc-sanity-check-is-the-target-distribution-similar-across-splits" class="nav-link" data-scroll-target="#sanity-check-is-the-target-distribution-similar-across-splits"><span class="header-section-number">10.3.2</span> Sanity Check: Is the Target Distribution Similar Across Splits?</a></li>
  <li><a href="#optional-check-quick-summary-statistics" id="toc-optional-check-quick-summary-statistics" class="nav-link" data-scroll-target="#optional-check-quick-summary-statistics"><span class="header-section-number">10.3.3</span> Optional Check: Quick Summary Statistics</a></li>
  </ul></li>
  <li><a href="#model-specification-a-scale-sensitive-baseline" id="toc-model-specification-a-scale-sensitive-baseline" class="nav-link" data-scroll-target="#model-specification-a-scale-sensitive-baseline"><span class="header-section-number">10.4</span> Model Specification: A Scale-Sensitive Baseline</a>
  <ul class="collapse">
  <li><a href="#model-specification" id="toc-model-specification" class="nav-link" data-scroll-target="#model-specification"><span class="header-section-number">10.4.1</span> Model Specification</a></li>
  </ul></li>
  <li><a href="#scenario-a-baseline-no-scaling" id="toc-scenario-a-baseline-no-scaling" class="nav-link" data-scroll-target="#scenario-a-baseline-no-scaling"><span class="header-section-number">10.5</span> Scenario A — Baseline: No Scaling</a>
  <ul class="collapse">
  <li><a href="#evaluate-on-the-test-set" id="toc-evaluate-on-the-test-set" class="nav-link" data-scroll-target="#evaluate-on-the-test-set"><span class="header-section-number">10.5.1</span> <strong>Evaluate on the Test Set</strong></a></li>
  <li><a href="#interpretation" id="toc-interpretation" class="nav-link" data-scroll-target="#interpretation"><span class="header-section-number">10.5.2</span> Interpretation</a></li>
  </ul></li>
  <li><a href="#scenario-b-incorrect-normalization-data-leakage" id="toc-scenario-b-incorrect-normalization-data-leakage" class="nav-link" data-scroll-target="#scenario-b-incorrect-normalization-data-leakage"><span class="header-section-number">10.6</span> Scenario B — Incorrect Normalization (Data Leakage)</a>
  <ul class="collapse">
  <li><a href="#leakage-pipeline-normalize-using-full-data" id="toc-leakage-pipeline-normalize-using-full-data" class="nav-link" data-scroll-target="#leakage-pipeline-normalize-using-full-data"><span class="header-section-number">10.6.1</span> Leakage Pipeline: Normalize Using Full Data</a></li>
  <li><a href="#interpretation-1" id="toc-interpretation-1" class="nav-link" data-scroll-target="#interpretation-1"><span class="header-section-number">10.6.2</span> Interpretation</a></li>
  </ul></li>
  <li><a href="#scenario-c-correct-normalization-train-only-scaling" id="toc-scenario-c-correct-normalization-train-only-scaling" class="nav-link" data-scroll-target="#scenario-c-correct-normalization-train-only-scaling"><span class="header-section-number">10.7</span> Scenario C — Correct Normalization (Train-Only Scaling)</a>
  <ul class="collapse">
  <li><a href="#correct-pipeline-normalize-using-training-data-only" id="toc-correct-pipeline-normalize-using-training-data-only" class="nav-link" data-scroll-target="#correct-pipeline-normalize-using-training-data-only"><span class="header-section-number">10.7.1</span> Correct Pipeline: Normalize Using Training Data Only</a></li>
  <li><a href="#interpretation-2" id="toc-interpretation-2" class="nav-link" data-scroll-target="#interpretation-2"><span class="header-section-number">10.7.2</span> Interpretation</a></li>
  </ul></li>
  <li><a href="#results-comparison" id="toc-results-comparison" class="nav-link" data-scroll-target="#results-comparison"><span class="header-section-number">10.8</span> Results Comparison</a>
  <ul class="collapse">
  <li><a href="#performance-summary" id="toc-performance-summary" class="nav-link" data-scroll-target="#performance-summary"><span class="header-section-number">10.8.1</span> Performance Summary</a></li>
  <li><a href="#visual-comparison-rmse" id="toc-visual-comparison-rmse" class="nav-link" data-scroll-target="#visual-comparison-rmse"><span class="header-section-number">10.8.2</span> Visual Comparison (RMSE)</a></li>
  <li><a href="#interpretation-3" id="toc-interpretation-3" class="nav-link" data-scroll-target="#interpretation-3"><span class="header-section-number">10.8.3</span> Interpretation</a></li>
  </ul></li>
  <li><a href="#practical-takeaways-from-the-application" id="toc-practical-takeaways-from-the-application" class="nav-link" data-scroll-target="#practical-takeaways-from-the-application"><span class="header-section-number">10.9</span> Practical Takeaways from the Application</a></li>
  </ul></li>
  <li><a href="#discussion-and-conclusion" id="toc-discussion-and-conclusion" class="nav-link" data-scroll-target="#discussion-and-conclusion"><span class="header-section-number">11</span> Discussion and Conclusion</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references"><span class="header-section-number">12</span> References</a></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="index.pdf"><i class="bi bi-file-pdf"></i>PDF</a></li></ul></div></nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">






<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="normalization.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="491"></p>
</figure>
</div>
<section id="introduction" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">1</span> Introduction</h2>
<p>This article is part of a broader series on <strong>data preprocessing in R</strong>. In earlier posts, we focused on two problems that quietly ruin analyses long before modeling begins: <strong>missing data</strong> and <strong>outliers</strong>. Both topics shared a common theme: preprocessing choices are not cosmetic; they change what the model is allowed to learn. In this installment, we move to the next decision point in the same pipeline: <strong>normalization (scaling)</strong>—often treated as “just a quick step,” but in practice a decisive modeling choice.</p>
<blockquote class="blockquote">
<p><strong>Related posts in this preprocessing series</strong></p>
<ul>
<li><p><em>Handling Missing Data in R: A Comprehensive Guide</em><br>
<a href="https://medium.com/r-evolution/handling-missing-data-in-r-a-comprehensive-guide-eca195eaead3" class="uri">https://medium.com/r-evolution/handling-missing-data-in-r-a-comprehensive-guide-eca195eaead3</a></p></li>
<li><p><em>Outliers in Data Analysis: Detecting Extreme Values Before Modeling in R</em><br>
<a href="https://medium.com/r-evolution/outliers-in-data-analysis-detecting-extreme-values-before-modeling-in-r-with-i%CC%87stanbul-airbnb-data-3b37e9ee989e" class="uri">https://medium.com/r-evolution/outliers-in-data-analysis-detecting-extreme-values-before-modeling-in-r-with-i%CC%87stanbul-airbnb-data-3b37e9ee989e</a></p></li>
</ul>
</blockquote>
<p>Normalization (or more broadly, scaling) is frequently presented as a minor technical adjustment—something to apply quickly and forget. In practice, scaling is not a technical detail but a modeling decision. When the same dataset is processed using different scaling strategies, the behavior of many models changes substantially. Distances, similarity measures, penalty terms, and optimization paths are all affected. As a result, the nearest neighbors selected by KNN, the clusters formed by K-means, the principal components identified by PCA, and even the coefficients chosen by Ridge or Lasso regression can differ. Scaling does not merely “prepare” the data; it actively shapes how a model interprets importance and structure.</p>
<p>More importantly, scaling is not universally beneficial. Applied in the wrong context, it can degrade model performance or—worse—introduce subtle forms of <strong>data leakage</strong> that contaminate evaluation. A common example is learning scaling parameters (such as means and standard deviations) from the entire dataset before splitting into training and test sets. This procedure allows information from the test distribution to leak into the training process, producing performance estimates that cannot be trusted. In such cases, the issue is not the scaling method itself, but <strong>when and how</strong> it is applied. Knowing how to call <code>scale()</code> in R is trivial; understanding what to scale, when to scale it, and why is not.</p>
<p>In this article, normalization is treated as an integral part of the modeling strategy rather than a routine preprocessing step. We will address, step by step, the following questions:</p>
<ul>
<li>Why is normalization necessary?</li>
<li>Should it always be applied?</li>
<li>At what stage should it be performed—before or after the train–test split?</li>
<li>Which scaling methods are commonly used, and in which contexts do they make sense?</li>
<li>Should different data types be treated differently?</li>
<li>Is scaling appropriate for all variables, including the target variable?</li>
</ul>
<p>By combining conceptual discussion with practical R implementations, this guide aims to provide clear and principled answers to each of these questions.</p>
</section>
<section id="normalization-vs.-standardization-clearing-up-the-terminology" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="normalization-vs.-standardization-clearing-up-the-terminology"><span class="header-section-number">2</span> Normalization vs.&nbsp;Standardization: Clearing Up the Terminology</h2>
<p>In both academic writing and everyday practice, the terms <em>normalization</em> and <em>standardization</em> are frequently used interchangeably. This loose usage is one of the main sources of confusion in data preprocessing. In reality, these terms refer to <strong>different scaling strategies</strong>, each with distinct assumptions, effects, and use cases. Before discussing when and how scaling should be applied, it is therefore essential to clarify what is actually meant by each approach.</p>
<p><strong>Standardization</strong>, often referred to as <em>z-score scaling</em>, rescales a variable so that it has a mean of zero and a standard deviation of one. Formally, each observation is transformed by subtracting the sample mean and dividing by the sample standard deviation. In the R ecosystem, this logic is implemented in preprocessing tools such as <code>step_normalize()</code> from the <strong>recipes</strong> package. Standardization preserves the shape of the original distribution while putting variables on a comparable scale. It is particularly useful for models that are sensitive to the relative magnitude of predictors, such as linear models with regularization, support vector machines, and neural networks.</p>
<p><strong>Normalization</strong>, in a stricter sense, often refers to <em>min–max scaling</em>. This approach rescales variables to lie within a fixed interval, most commonly [0,1]. Each value is transformed based on the minimum and maximum observed in the training data. Min–max scaling is easy to interpret and is frequently used in algorithms where bounded inputs are desirable. However, it is also more sensitive to extreme values, since a single outlier can heavily influence the scaling range.</p>
<p>A third commonly used approach is <strong>robust scaling</strong>, which relies on the median and the interquartile range (IQR) instead of the mean and standard deviation. By construction, this method is less affected by outliers and heavy-tailed distributions. Robust scaling is especially useful in real-world datasets where extreme values are not errors but genuine observations. At the same time, it is not a universal solution; in some data structures, robust measures may become unstable or uninformative.</p>
<p>The reason terminology becomes blurred in practice is simple: many practitioners use the word <em>normalization</em> as a generic label for “any kind of scaling.” As a result, two people may both say they normalized their data while having applied entirely different transformations. Throughout this article, we will avoid this ambiguity by explicitly stating which scaling method is used and why. This distinction is not pedantic—it is essential for understanding how scaling choices influence model behavior.</p>
</section>
<section id="why-is-normalization-necessary" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="why-is-normalization-necessary"><span class="header-section-number">3</span> Why Is Normalization Necessary?</h2>
<p>The necessity of normalization becomes clear once we recognize that many modeling techniques do not operate on raw variable values directly, but on <strong>relationships derived from them</strong>—such as distances, similarities, penalties, or variance directions. When predictors are measured on different scales, these derived quantities can be dominated by variables with larger numerical ranges, regardless of their substantive importance. In such cases, the model does not learn from the data structure itself, but from arbitrary measurement units.</p>
<p>This issue is most apparent in <strong>distance-based methods</strong> such as k-nearest neighbors (KNN) and K-means clustering. These algorithms rely explicitly on distance calculations, typically Euclidean distance. If one variable ranges between 0 and 1 while another ranges between 0 and 10,000, the latter will dominate the distance computation almost entirely. As a result, proximity is determined not by overall similarity but by the scale of a single variable. Normalization ensures that each predictor contributes to the distance metric in a balanced and interpretable way, allowing the algorithm to reflect genuine similarity rather than numerical magnitude.</p>
<p>Normalization is equally critical in models that incorporate <strong>regularization</strong>, such as Ridge and Lasso regression. In these models, coefficients are penalized to control model complexity. However, the penalty term is directly tied to the scale of the predictors. If variables are not on comparable scales, the regularization mechanism will shrink coefficients unevenly, effectively penalizing some predictors more than others for reasons unrelated to their predictive relevance. Scaling aligns the predictors so that regularization operates as intended: as a constraint on model complexity rather than an artifact of measurement units.</p>
<p>Other widely used techniques—including <strong>support vector machines (SVMs)</strong>, <strong>neural networks</strong>, and <strong>principal component analysis (PCA)</strong>—are also highly sensitive to scaling. In SVMs and neural networks, optimization procedures depend on gradients that are influenced by feature magnitudes, affecting both convergence speed and stability. In PCA, the directions of maximum variance are determined by the scale of the variables; without normalization, components may simply reflect variables with the largest variances rather than the most informative underlying structure. In all these cases, scaling is not an optional refinement but a prerequisite for meaningful model behavior.</p>
<p>By contrast, <strong>tree-based models</strong> such as decision trees, random forests, and gradient boosting machines are generally invariant to monotonic transformations of individual predictors. Since splits are based on ordering rather than distance or magnitude, scaling is often unnecessary for these methods. Nevertheless, this does not imply that normalization is universally irrelevant in tree-based pipelines. Hybrid workflows—where tree-based models are combined with distance-based components, rule-based similarity measures, or downstream models sensitive to scale—may still require careful consideration of scaling choices. The key point is not that normalization should always be applied, but that it should be applied <strong>with respect to the assumptions of the modeling approach</strong>.</p>
<p>From a broader perspective, normalization plays a central role in modern predictive modeling workflows. As emphasized in the predictive modeling literature, preprocessing steps are not independent of the model; they are part of the modeling strategy itself. Scaling decisions shape how information is represented and, ultimately, how learning takes place. Understanding <em>why</em> normalization is necessary is therefore a prerequisite for deciding <em>when</em> and <em>how</em> it should be applied—a topic we address next.</p>
</section>
<section id="should-normalization-always-be-applied" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="should-normalization-always-be-applied"><span class="header-section-number">4</span> Should Normalization Always Be Applied?</h2>
<p>A natural question at this point is whether normalization should be applied by default in every modeling task. The short answer is <strong>no</strong>. Normalization is not a universally beneficial preprocessing step; its usefulness depends on the assumptions and internal mechanics of the chosen model. Applying scaling blindly can be as problematic as ignoring it altogether. What is needed is a <strong>decision framework</strong> that links model characteristics to preprocessing choices.</p>
<p>For a large class of models, normalization is <strong>strongly recommended</strong>. This group includes distance-based methods such as k-nearest neighbors (KNN) and K-means clustering, as well as techniques like principal component analysis (PCA), support vector machines (SVMs), neural networks, and penalized regression models (Ridge, Lasso, Elastic Net). In all these cases, either distances, inner products, variance directions, or penalty terms play a central role. Without scaling, these mechanisms are dominated by variables with larger numerical ranges, leading to distorted learning behavior. For such models, normalization is not a refinement but a prerequisite for meaningful results.</p>
<p>By contrast, normalization is <strong>generally unnecessary</strong> for tree-based models such as decision trees, random forests, and gradient boosting machines (e.g., XGBoost, GBM). These models rely on recursive binary splits based on variable ordering rather than on distances or magnitudes. Since monotonic transformations do not affect the relative ordering of values, scaling typically has no impact on model performance. As a result, normalization is often omitted in purely tree-based pipelines without any loss of effectiveness.</p>
<p>Between these two extremes lies a set of models for which normalization is <strong>context-dependent</strong>. Ordinary linear regression, for example, does not require scaling for estimation itself, but normalization may still be useful for numerical stability, interpretability of coefficients, or comparability across predictors. Similarly, Naive Bayes models may or may not benefit from scaling depending on the assumed feature distributions and the types of variables involved. In these cases, the decision to normalize should be guided by the modeling objective rather than by a fixed rule.</p>
<p>The key takeaway is that normalization should be applied <strong>with respect to the model’s assumptions</strong>, not as a default preprocessing habit. To make this decision explicit, Table 1 summarizes common modeling approaches and whether normalization is typically required.</p>
<section id="when-is-normalization-needed-a-model-based-decision-table" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="when-is-normalization-needed-a-model-based-decision-table"><span class="header-section-number">4.1</span> When Is Normalization Needed? A Model-Based Decision Table</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 49%">
</colgroup>
<thead>
<tr class="header">
<th>Model / Method</th>
<th>Is Normalization Recommended?</th>
<th>Rationale</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>KNN</td>
<td>Yes</td>
<td>Distance calculations are scale-sensitive</td>
</tr>
<tr class="even">
<td>K-means</td>
<td>Yes</td>
<td>Cluster assignment depends on distances</td>
</tr>
<tr class="odd">
<td>PCA</td>
<td>Yes</td>
<td>Variance directions dominated by scale</td>
</tr>
<tr class="even">
<td>SVM</td>
<td>Yes</td>
<td>Optimization and margins depend on feature magnitude</td>
</tr>
<tr class="odd">
<td>Neural Networks</td>
<td>Yes</td>
<td>Gradient-based optimization is scale-sensitive</td>
</tr>
<tr class="even">
<td>Ridge / Lasso / Elastic Net</td>
<td>Yes</td>
<td>Penalty terms depend on predictor scale</td>
</tr>
<tr class="odd">
<td>Linear Regression (OLS)</td>
<td>Depends</td>
<td>Not required for estimation, but useful for stability and interpretation</td>
</tr>
<tr class="even">
<td>Naive Bayes</td>
<td>Depends</td>
<td>Depends on feature types and distributional assumptions</td>
</tr>
<tr class="odd">
<td>Decision Trees</td>
<td>No</td>
<td>Split rules depend on ordering, not scale</td>
</tr>
<tr class="even">
<td>Random Forest / GBM / XGBoost</td>
<td>No</td>
<td>Tree-based structure is scale-invariant</td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="when-should-normalization-be-applied-before-or-after-the-traintest-split" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="when-should-normalization-be-applied-before-or-after-the-traintest-split"><span class="header-section-number">5</span> When Should Normalization Be Applied? Before or After the Train–Test Split?</h2>
<p>This is the most critical question in the entire preprocessing workflow—and the point at which many otherwise sound analyses quietly go wrong. The issue is not <em>whether</em> normalization should be applied, but <strong>when</strong> it should be applied. At the center of this question lies a fundamental concept in predictive modeling: <strong>data leakage</strong>.</p>
<p>Data leakage occurs when information from outside the training set is used, directly or indirectly, during model training. In the context of normalization, leakage typically arises when scaling parameters—such as means and standard deviations (for standardization) or minimum and maximum values (for min–max scaling)—are estimated using the full dataset before splitting into training and test sets. Although this may appear harmless, it allows information from the test set to influence the preprocessing step, leading to overly optimistic performance estimates.</p>
<p>The correct principle is straightforward but non-negotiable:<br>
<strong>scaling parameters must be learned exclusively from the training data</strong>.<br>
Once learned, the <em>same transformation</em>—with fixed parameters—must be applied to the test set and to any future, unseen data. This ensures that the test set truly represents new information and that model evaluation reflects genuine generalization rather than procedural artifacts.</p>
<p>This principle is central to modern modeling frameworks. In the <strong>tidymodels/recipes</strong> philosophy, preprocessing steps are <em>trained</em> on the training data and then <em>applied</em> consistently to all other datasets. Similarly, in the <strong>caret</strong> framework, preprocessing transformations are estimated from the training set and reused when predicting on new data. In both cases, preprocessing is treated as part of the model training process—not as an independent, preliminary operation.</p>
<p>To see why this distinction matters, consider the following conceptual comparison.</p>
<section id="an-illustrative-example-scaling-before-vs.-after-the-split" class="level3" data-number="5.1">
<h3 data-number="5.1" class="anchored" data-anchor-id="an-illustrative-example-scaling-before-vs.-after-the-split"><span class="header-section-number">5.1</span> An Illustrative Example: Scaling Before vs.&nbsp;After the Split</h3>
<p>Suppose we have a dataset that we intend to split into training and test sets. We want to standardize a numeric predictor using z-score scaling.</p>
<p><strong>Incorrect approach (scaling before the split):</strong></p>
<ol type="1">
<li><p>Compute the mean and standard deviation using the <em>entire dataset</em>.</p></li>
<li><p>Standardize all observations using these global parameters.</p></li>
<li><p>Split the scaled data into training and test sets.</p></li>
<li><p>Train and evaluate the model.</p></li>
</ol>
<p>At first glance, this workflow seems efficient. However, the scaling parameters already incorporate information from the test set. The test data are no longer independent of the training process, even though they were not explicitly used to fit the model.</p>
<p><strong>Correct approach (scaling after the split):</strong></p>
<ol type="1">
<li><p>Split the raw data into training and test sets.</p></li>
<li><p>Compute scaling parameters (mean, standard deviation, etc.) <em>using only the training set</em>.</p></li>
<li><p>Apply the learned transformation to the training set.</p></li>
<li><p>Apply the <em>same</em> transformation to the test set.</p></li>
<li><p>Train the model on the scaled training data and evaluate it on the scaled test data.</p></li>
</ol>
<p>In practice, these two approaches can lead to noticeably different evaluation results. Models trained using the incorrect workflow often appear to perform better on the test set—not because they generalize better, but because the preprocessing step has already “seen” the test data. This difference is especially pronounced in smaller datasets, in datasets with strong distributional differences between training and test splits, or when extreme values are present.</p>
<p>The takeaway is unambiguous:</p>
<blockquote class="blockquote">
<p><strong>Split the data first.<br>
Fit preprocessing steps on the training data.<br>
Apply the same transformations to the training and test sets.</strong></p>
</blockquote>
<p>Any deviation from this sequence undermines the validity of model evaluation, regardless of how sophisticated the modeling technique may be.</p>
</section>
</section>
<section id="common-normalization-methods-and-when-to-use-them" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="common-normalization-methods-and-when-to-use-them"><span class="header-section-number">6</span> Common Normalization Methods and When to Use Them</h2>
<p>Normalization is not a single technique but a family of transformations, each designed to address a specific modeling concern. Choosing an appropriate method requires understanding <strong>what problem the transformation is solving</strong> and <strong>which assumptions it implicitly makes</strong>. In this section, we review the most commonly used scaling approaches, discuss their strengths and limitations, and clarify when each method is appropriate.</p>
<section id="z-score-standardization" class="level3" data-number="6.1">
<h3 data-number="6.1" class="anchored" data-anchor-id="z-score-standardization"><span class="header-section-number">6.1</span> Z-score Standardization</h3>
<p>Z-score standardization rescales a variable so that it has a mean of zero and a standard deviation of one. Each observation <span class="math inline">\(x_i\)</span> is transformed as:</p>
<p><span class="math display">\[
z_i = \frac{x_i - \mu}{\sigma},
\]</span></p>
<p>where <span class="math inline">\(\mu\)</span> denotes the sample mean and <span class="math inline">\(\sigma\)</span> the sample standard deviation, both estimated <strong>from the training data only</strong>.</p>
<p><strong>Advantages.</strong><br>
Z-score standardization places variables on a comparable scale while preserving the shape of their original distributions. It is particularly suitable for models that rely on inner products, gradient-based optimization, or regularization (e.g., penalized linear models, SVMs, neural networks).</p>
<p><strong>Limitations.</strong><br>
A widespread misconception is that standardization assumes normally distributed data. This is incorrect. Z-score scaling does <strong>not</strong> require normality; it only uses the first two moments of the distribution. However, it is sensitive to extreme values: large outliers can inflate <span class="math inline">\(\sigma\)</span>, thereby reducing the relative influence of most observations.</p>
<p><strong>When to use.</strong><br>
A strong default choice when predictors differ substantially in scale and when outliers are either absent or have already been treated.</p>
<hr>
</section>
<section id="minmax-range-scaling" class="level3" data-number="6.2">
<h3 data-number="6.2" class="anchored" data-anchor-id="minmax-range-scaling"><span class="header-section-number">6.2</span> Min–Max (Range) Scaling</h3>
<p>Min–max scaling rescales variables to a fixed interval, most commonly <span class="math inline">\([0, 1]\)</span>. The transformation is:</p>
<p><span class="math display">\[
x_i^{*} = \frac{x_i - \min(x)}{\max(x) - \min(x)}.
\]</span></p>
<p><strong>Advantages.</strong><br>
Intuitive and ensures all transformed values lie within a predefined range. Often used when bounded inputs are desirable (e.g., some neural network settings).</p>
<p><strong>Limitations.</strong><br>
Highly sensitive to extreme values: a single outlier can stretch the range and compress most observations. Also, when applied to test or future data, transformed values may fall outside <span class="math inline">\([0,1]\)</span> if they exceed the training-set min/max. This is expected and must be handled in deployment.</p>
<p><strong>When to use.</strong><br>
When input bounds are meaningful and the training data represent the likely range of future observations.</p>
<hr>
</section>
<section id="robust-scaling-median-and-iqr" class="level3" data-number="6.3">
<h3 data-number="6.3" class="anchored" data-anchor-id="robust-scaling-median-and-iqr"><span class="header-section-number">6.3</span> Robust Scaling (Median and IQR)</h3>
<p>Robust scaling replaces mean and standard deviation with the median and the interquartile range (IQR). The transformation is:</p>
<p><span class="math display">\[
x_i^{*} = \frac{x_i - \mathrm{median}(x)}{\mathrm{IQR}(x)},
\]</span></p>
<p>where:</p>
<p><span class="math display">\[
\mathrm{IQR}(x) = Q_{0.75} - Q_{0.25}.
\]</span></p>
<p><strong>Advantages.</strong><br>
Less affected by extreme values and heavy-tailed distributions; useful when outliers are meaningful rather than errors.</p>
<p><strong>Limitations.</strong><br>
Not universally stable. In highly concentrated variables, <span class="math inline">\(\mathrm{IQR}(x)\)</span> (or related robust measures such as MAD) may be zero or extremely small, making the transformation unstable or undefined. This must be checked explicitly.</p>
<p><strong>When to use.</strong><br>
When outliers are present and structurally inherent, and you want scaling that is less sensitive to extremes.</p>
<hr>
</section>
<section id="power-transformations-combined-with-scaling-boxcox-and-yeojohnson" class="level3" data-number="6.4">
<h3 data-number="6.4" class="anchored" data-anchor-id="power-transformations-combined-with-scaling-boxcox-and-yeojohnson"><span class="header-section-number">6.4</span> Power Transformations Combined with Scaling (Box–Cox and Yeo–Johnson)</h3>
<p>Power transformations aim to stabilize variance and reduce skewness before scaling.</p>
<p>The <strong>Box–Cox transformation</strong> (for strictly positive data) is:</p>
<p><span class="math display">\[
x_i^{(\lambda)} =
\begin{cases}
\frac{x_i^{\lambda} - 1}{\lambda}, &amp; \lambda \neq 0, \\\\
\log(x_i), &amp; \lambda = 0.
\end{cases}
\]</span></p>
<p>The <strong>Yeo–Johnson transformation</strong> (allows zero and negative values) is:</p>
<p><span class="math display">\[
x_i^{(\lambda)} =
\begin{cases}
\frac{(x_i + 1)^{\lambda} - 1}{\lambda}, &amp; x_i \ge 0,\ \lambda \neq 0, \\\\
\log(x_i + 1), &amp; x_i \ge 0,\ \lambda = 0, \\\\
-\frac{(-x_i + 1)^{2 - \lambda} - 1}{2 - \lambda}, &amp; x_i &lt; 0,\ \lambda \neq 2, \\\\
-\log(-x_i + 1), &amp; x_i &lt; 0,\ \lambda = 2.
\end{cases}
\]</span></p>
<p><strong>Why combine with scaling?</strong><br>
Power transformations modify distributional shape but do not put variables on a common scale. After applying Box–Cox or Yeo–Johnson, variables are typically centered and scaled.</p>
<p><strong>Order matters.</strong><br>
A practical default sequence is: <strong>power transformation → centering → scaling</strong>. Scaling before addressing skewness can weaken the effect of the transformation and complicate interpretation.</p>
<p><strong>When to use.</strong><br>
When strong skewness or heteroscedasticity is present and when model assumptions or optimization benefit from more symmetric distributions.</p>
<hr>
</section>
<section id="choosing-a-method-no-single-best-answer" class="level3" data-number="6.5">
<h3 data-number="6.5" class="anchored" data-anchor-id="choosing-a-method-no-single-best-answer"><span class="header-section-number">6.5</span> Choosing a Method: No Single Best Answer</h3>
<p>There is no universally optimal normalization method. Each approach reflects a trade-off between robustness, interpretability, and sensitivity to data characteristics. The appropriate choice depends on the model, the data structure, and the modeling objective.</p>
<blockquote class="blockquote">
<p>The relevant question is not <em>“Which normalization method is best?”</em><br>
but <em>“Which transformation aligns with my data and my model’s assumptions?”</em></p>
</blockquote>
</section>
</section>
<section id="do-different-data-types-require-different-scaling-strategies" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="do-different-data-types-require-different-scaling-strategies"><span class="header-section-number">7</span> Do Different Data Types Require Different Scaling Strategies?</h2>
<p>Normalization decisions should never be made independently of data types. Different variable types carry different semantic meanings, and applying the same scaling strategy indiscriminately can lead to misleading representations or unnecessary transformations. A principled preprocessing workflow therefore begins by distinguishing between variable types and understanding how each interacts with scaling.</p>
<section id="continuous-numeric-variables" class="level3" data-number="7.1">
<h3 data-number="7.1" class="anchored" data-anchor-id="continuous-numeric-variables"><span class="header-section-number">7.1</span> Continuous Numeric Variables</h3>
<p>Continuous numeric variables are the primary candidates for normalization. When such variables are measured on different scales—such as income in thousands and proportions between 0 and 1—scaling is often essential for models that rely on distances, gradients, or regularization. Z-score standardization, min–max scaling, or robust scaling are all reasonable options, depending on the presence of outliers and the modeling objective.</p>
<p>In practice, most normalization methods are designed with continuous variables in mind, and applying them here rarely raises conceptual concerns. The main decision revolves around <em>which</em> scaling method is most appropriate, not <em>whether</em> scaling should be applied at all.</p>
<hr>
</section>
<section id="count-and-ordinal-numeric-variables" class="level3" data-number="7.2">
<h3 data-number="7.2" class="anchored" data-anchor-id="count-and-ordinal-numeric-variables"><span class="header-section-number">7.2</span> Count and Ordinal Numeric Variables</h3>
<p>Some numeric variables are technically continuous in storage but conceptually represent counts or ordered categories. Examples include the number of visits, rankings, Likert-scale responses, or discrete event counts. Treating such variables as purely continuous can be problematic, especially when their distributions are highly skewed or bounded at zero.</p>
<p>In these cases, applying a logarithmic or power transformation before scaling is often more appropriate than direct normalization. Power transformations can reduce skewness and stabilize variance, after which standardization or robust scaling may be applied. The key point is that <strong>the meaning of the variable matters</strong>: a difference of one unit in a count variable does not necessarily carry the same interpretation across its range.</p>
<hr>
</section>
<section id="categorical-variables-factors-or-characters" class="level3" data-number="7.3">
<h3 data-number="7.3" class="anchored" data-anchor-id="categorical-variables-factors-or-characters"><span class="header-section-number">7.3</span> Categorical Variables (Factors or Characters)</h3>
<p>Categorical variables should <strong>never</strong> be scaled directly. Their values represent qualitative categories rather than numerical magnitudes, and applying normalization to raw category codes is meaningless.</p>
<p>When categorical variables are included in models that require numeric inputs, they must first be transformed using an encoding scheme such as one-hot (dummy) encoding. After encoding, the question of scaling arises again. In many cases, scaling encoded variables is unnecessary. However, in penalized regression models or distance-based methods, normalization of one-hot encoded variables may be beneficial to ensure that categorical and continuous predictors are treated on comparable scales.</p>
<p>The important distinction is that scaling applies <strong>after encoding</strong>, not before, and only when the model’s assumptions justify it.</p>
<hr>
</section>
<section id="binary-variables-01-indicators" class="level3" data-number="7.4">
<h3 data-number="7.4" class="anchored" data-anchor-id="binary-variables-01-indicators"><span class="header-section-number">7.4</span> Binary Variables (0/1 Indicators)</h3>
<p>Binary variables occupy a special position. Since they already lie on a fixed and interpretable scale, normalization is usually unnecessary and may even obscure interpretation. For many models, leaving binary indicators unchanged is the most transparent choice.</p>
<p>That said, binary variables often enter preprocessing pipelines automatically when a rule such as “scale all numeric predictors” is applied. In such cases, standardization will transform a 0/1 variable into values centered around zero with unit variance. While this does not usually harm model performance, it changes the interpretation of coefficients and can complicate downstream analysis.</p>
<p>This highlights an important practical lesson: automated preprocessing pipelines should be used with care. Even when a transformation is mathematically valid, it may not be conceptually desirable for all variable types.</p>
<hr>
</section>
<section id="summary-scaling-depends-on-variable-meaning" class="level3" data-number="7.5">
<h3 data-number="7.5" class="anchored" data-anchor-id="summary-scaling-depends-on-variable-meaning"><span class="header-section-number">7.5</span> Summary: Scaling Depends on Variable Meaning</h3>
<p>The decision to normalize should always be guided by the <em>semantic role</em> of a variable, not merely by its storage type. Continuous measurements, counts, ordered responses, categorical indicators, and binary flags interact with scaling in fundamentally different ways. Effective preprocessing therefore requires more than applying a generic rule—it requires aligning transformations with the structure and meaning of the data.</p>
</section>
</section>
<section id="should-all-variables-be-scaled" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="should-all-variables-be-scaled"><span class="header-section-number">8</span> Should All Variables Be Scaled?</h2>
<p>A common mistake in preprocessing workflows is to treat normalization as a blanket operation applied to every variable in the dataset. In reality, <strong>not all variables should be scaled</strong>, and doing so indiscriminately can reduce interpretability or even introduce unintended distortions. Scaling decisions must therefore be made at the variable level, guided by both statistical and semantic considerations.</p>
<section id="the-target-variable-y" class="level3" data-number="8.1">
<h3 data-number="8.1" class="anchored" data-anchor-id="the-target-variable-y"><span class="header-section-number">8.1</span> The Target Variable (y)</h3>
<p>In most predictive modeling tasks, the target variable should <strong>not</strong> be normalized. Scaling the response does not improve model estimation and often complicates interpretation, particularly in regression settings where coefficients and predictions are expected to be expressed in the original units.</p>
<p>There are, however, notable exceptions. In neural network regression or other optimization-heavy models, scaling the target variable can improve numerical stability and convergence behavior. In such cases, predictions must be transformed back to the original scale before evaluation and interpretation. Outside these specific contexts, leaving the target variable unchanged remains the standard and preferred practice.</p>
<hr>
</section>
<section id="predictor-variables" class="level3" data-number="8.2">
<h3 data-number="8.2" class="anchored" data-anchor-id="predictor-variables"><span class="header-section-number">8.2</span> Predictor Variables</h3>
<p>For predictor variables, scaling should be applied selectively rather than universally.</p>
<section id="numeric-predictors-only" class="level4" data-number="8.2.1">
<h4 data-number="8.2.1" class="anchored" data-anchor-id="numeric-predictors-only"><span class="header-section-number">8.2.1</span> Numeric Predictors Only</h4>
<p>Normalization is meaningful only for numeric predictors. Applying scaling to non-numeric variables—either directly or implicitly through arbitrary numeric coding—has no conceptual justification. As discussed earlier, categorical variables must first be encoded, and even then, scaling is optional and model-dependent.</p>
</section>
<section id="excluding-non-informative-numeric-variables" class="level4" data-number="8.2.2">
<h4 data-number="8.2.2" class="anchored" data-anchor-id="excluding-non-informative-numeric-variables"><span class="header-section-number">8.2.2</span> Excluding Non-informative Numeric Variables</h4>
<p>Not all numeric variables carry meaningful quantitative information. Identifier variables such as IDs, account numbers, or arbitrary codes may be stored as numeric values but do not represent magnitudes or distances. Scaling such variables is meaningless and potentially harmful, as it introduces artificial structure where none exists. These variables should be excluded from the modeling process altogether, not merely from scaling.</p>
</section>
<section id="handling-low-variance-predictors" class="level4" data-number="8.2.3">
<h4 data-number="8.2.3" class="anchored" data-anchor-id="handling-low-variance-predictors"><span class="header-section-number">8.2.3</span> Handling Low-Variance Predictors</h4>
<p>Variables with extremely low or zero variance provide little to no information for modeling. Scaling such predictors does not solve the underlying problem; it merely rescales noise. In practice, low-variance and zero-variance predictors should be identified and removed <strong>before</strong> normalization.</p>
<p>Many preprocessing frameworks formalize this step. For example, approaches based on the logic of zero-variance or near-zero-variance filtering (often referred to as <code>zv</code> or <code>nzv</code> steps) ensure that only informative predictors enter the scaling stage. This not only improves computational efficiency but also reduces the risk of numerical instability in downstream models.</p>
<hr>
</section>
</section>
<section id="a-practical-rule-of-thumb" class="level3" data-number="8.3">
<h3 data-number="8.3" class="anchored" data-anchor-id="a-practical-rule-of-thumb"><span class="header-section-number">8.3</span> A Practical Rule of Thumb</h3>
<p>A disciplined preprocessing workflow follows a clear sequence:</p>
<ol type="1">
<li>Identify and remove non-informative variables (IDs, constants, near-constants).</li>
<li>Select numeric predictors that represent meaningful quantities.</li>
<li>Apply appropriate scaling only to this subset.</li>
<li>Leave the target variable unscaled, unless there is a compelling model-specific reason to do otherwise.</li>
</ol>
<p>Scaling is most effective when it is <strong>deliberate and selective</strong>, not automatic. Treating normalization as a universal operation may simplify code, but it rarely leads to better models.</p>
</section>
</section>
<section id="application-plan-in-r-data-and-modeling-scenario" class="level2" data-number="9">
<h2 data-number="9" class="anchored" data-anchor-id="application-plan-in-r-data-and-modeling-scenario"><span class="header-section-number">9</span> Application Plan in R: Data and Modeling Scenario</h2>
<p>To demonstrate the practical implications of normalization decisions, we use the <strong>Ames Housing</strong> dataset, a well-known benchmark dataset designed for predictive modeling. The dataset contains <strong>2,930 observations</strong> and a rich set of predictors describing residential properties in Ames, Iowa. These predictors span multiple data types, including continuous numeric variables, discrete counts, ordinal ratings, and categorical features. This diversity makes the dataset particularly suitable for illustrating how scaling interacts with different variable types.</p>
<p>The Ames Housing dataset is distributed within the <strong>modeldata</strong> package in the tidymodels ecosystem. It was explicitly curated for teaching and methodological demonstrations, ensuring a realistic but well-documented structure. The presence of variables measured on vastly different scales—such as living area, lot size, and quality scores—provides a natural setting for exploring the effects of normalization.</p>
<section id="modeling-objective" class="level3" data-number="9.1">
<h3 data-number="9.1" class="anchored" data-anchor-id="modeling-objective"><span class="header-section-number">9.1</span> Modeling Objective</h3>
<p>The primary goal of this application is <strong>not</strong> to optimize predictive performance, but to isolate and examine the impact of different normalization strategies. For this reason, the modeling task is intentionally kept simple. We focus on predicting the <strong>sale price of a house</strong> as a regression problem, using a fixed model specification across all experiments.</p>
<p>The model itself serves merely as a vehicle for comparison. By holding the model constant and varying only the preprocessing strategy, we can attribute differences in performance and behavior directly to scaling decisions rather than to model complexity or tuning choices.</p>
</section>
<section id="scope-and-focus" class="level3" data-number="9.2">
<h3 data-number="9.2" class="anchored" data-anchor-id="scope-and-focus"><span class="header-section-number">9.2</span> Scope and Focus</h3>
<p>Throughout the application section, the emphasis remains firmly on preprocessing:</p>
<ul>
<li>the same training–test split is used across all scenarios,</li>
<li>the same set of predictors is retained,</li>
<li>the same model structure is applied.</li>
</ul>
<p>Only the normalization strategy changes. This design allows us to answer a focused question:</p>
<blockquote class="blockquote">
<p><em>How much do scaling choices matter when everything else is kept equal?</em></p>
</blockquote>
<p>By structuring the analysis in this way, the results highlight normalization as an integral component of the modeling pipeline rather than a secondary technical detail.</p>
<hr>
</section>
<section id="transition-to-implementation" class="level3" data-number="9.3">
<h3 data-number="9.3" class="anchored" data-anchor-id="transition-to-implementation"><span class="header-section-number">9.3</span> Transition to Implementation</h3>
<p>In the next section, we move from design to execution. We begin by defining a train–test split and establishing a baseline preprocessing workflow. From there, we introduce alternative normalization strategies and compare their effects using consistent evaluation criteria.</p>
</section>
<section id="data-access-and-availability" class="level3" data-number="9.4">
<h3 data-number="9.4" class="anchored" data-anchor-id="data-access-and-availability"><span class="header-section-number">9.4</span> Data Access and Availability</h3>
<p>The Ames Housing dataset used in this application is available through the <strong>modeldata</strong> package, which is part of the tidymodels ecosystem. No external download is required. Once the package is installed, the dataset can be accessed directly within R.</p>
<p>The dataset is provided for educational and methodological purposes and is accompanied by detailed documentation. For reference, the official description is available at:</p>
<p><a href="https://modeldata.tidymodels.org/reference/ames.html" class="uri">https://modeldata.tidymodels.org/reference/ames.html</a></p>
<p>In the next section, we load the dataset directly from the package and proceed with the train–test split and preprocessing workflow.</p>
</section>
</section>
<section id="implementation-in-r-split-baseline-and-the-cost-of-doing-it-wrong" class="level2" data-number="10">
<h2 data-number="10" class="anchored" data-anchor-id="implementation-in-r-split-baseline-and-the-cost-of-doing-it-wrong"><span class="header-section-number">10</span> Implementation in R: Split, Baseline, and the Cost of Doing It Wrong</h2>
<p>In this section, we operationalize the key principle introduced earlier:</p>
<blockquote class="blockquote">
<p><strong>Split → fit preprocessing on train → apply to train/test</strong></p>
</blockquote>
<p>We use the Ames Housing dataset from the <code>modeldata</code> package (no external download required) and compare three pipelines using the <strong>same model</strong>:</p>
<ol type="1">
<li><strong>Baseline (no scaling)</strong></li>
<li><strong>Incorrect scaling (data leakage)</strong>: scaling parameters learned from the full dataset</li>
<li><strong>Correct scaling</strong>: scaling parameters learned from the training set only</li>
</ol>
<p>The goal is not to build the best possible model but to <strong>isolate the effect of scaling decisions</strong>.</p>
<section id="setup-and-variable-selection" class="level3" data-number="10.1">
<h3 data-number="10.1" class="anchored" data-anchor-id="setup-and-variable-selection"><span class="header-section-number">10.1</span> Setup and Variable Selection</h3>
<p>Before defining any model, we clarify what we are modeling and why these variables are used.</p>
<p><strong>Modeling goal.</strong><br>
We treat <code>Sale_Price</code> as the target variable and build a regression model that predicts house sale prices based on a small set of numeric predictors. The purpose is not to maximize predictive accuracy, but to create a controlled environment where the effect of scaling choices is easy to observe.</p>
<p><strong>Why a small subset of predictors?</strong><br>
The Ames dataset contains many variables, including categorical and ordinal predictors. For the normalization demonstrations, we intentionally select a compact set of <strong>numeric</strong> features with clearly different measurement scales. This makes the consequences of scaling (and data leakage) more visible and easier to interpret.</p>
<p><strong>Selected variables (interpretation).</strong></p>
<ul>
<li><p><code>Sale_Price</code>: sale price of the house (response variable).</p></li>
<li><p><code>Gr_Liv_Area</code>: above-ground living area (a size-related continuous measure).</p></li>
<li><p><code>Lot_Area</code>: lot size (typically much larger numeric range than living area).</p></li>
<li><p><code>Year_Built</code>: construction year (a temporal numeric variable).</p></li>
<li><p><code>Overall_Cond</code>: overall condition rating (an ordinal-like numeric score).</p></li>
<li><p><code>Latitude</code>, <code>Longitude</code>: geographic coordinates capturing location effects.</p></li>
</ul>
<hr>
</section>
<section id="load-data-and-create-a-working-dataset" class="level3" data-number="10.2">
<h3 data-number="10.2" class="anchored" data-anchor-id="load-data-and-create-a-working-dataset"><span class="header-section-number">10.2</span> Load Data and Create a Working Dataset</h3>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidymodels)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(modeldata)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(ames, <span class="at">package =</span> <span class="st">"modeldata"</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2026</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>ames_small <span class="ot">&lt;-</span> ames <span class="sc">%&gt;%</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">select</span>(</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    Sale_Price,</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    Gr_Liv_Area,</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    Lot_Area,</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    Year_Built,</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    Overall_Cond,</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    Latitude,</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    Longitude</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Missing-value check within the selected columns</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>ames_small <span class="sc">%&gt;%</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(<span class="fu">across</span>(<span class="fu">everything</span>(), <span class="sc">~</span> <span class="fu">sum</span>(<span class="fu">is.na</span>(.)))) <span class="sc">%&gt;%</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>  tidyr<span class="sc">::</span><span class="fu">pivot_longer</span>(<span class="fu">everything</span>(), <span class="at">names_to =</span> <span class="st">"variable"</span>, <span class="at">values_to =</span> <span class="st">"n_missing"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 7 × 2
  variable     n_missing
  &lt;chr&gt;            &lt;int&gt;
1 Sale_Price           0
2 Gr_Liv_Area          0
3 Lot_Area             0
4 Year_Built           0
5 Overall_Cond         0
6 Latitude             0
7 Longitude            0</code></pre>
</div>
</div>
<p>This step constructs a clean working dataset (<code>ames_small</code>) and confirms whether missing values exist in the selected columns. For the comparisons in the next sections, it is important that the pipelines differ only by preprocessing choices (e.g., scaling), not by inconsistent handling of missing data.</p>
</section>
<section id="traintest-split-and-evaluation-setup" class="level3" data-number="10.3">
<h3 data-number="10.3" class="anchored" data-anchor-id="traintest-split-and-evaluation-setup"><span class="header-section-number">10.3</span> Train–Test Split and Evaluation Setup</h3>
<p>Before discussing scaling, we must establish a clean evaluation setup. The key idea is simple:</p>
<blockquote class="blockquote">
<p><strong>Split first. Then learn any preprocessing parameters from the training set only.</strong></p>
</blockquote>
<p>Without a proper train–test split, we cannot meaningfully talk about generalization, and any comparison involving normalization risks becoming misleading.</p>
<hr>
<section id="create-a-stratified-traintest-split" class="level4" data-number="10.3.1">
<h4 data-number="10.3.1" class="anchored" data-anchor-id="create-a-stratified-traintest-split"><span class="header-section-number">10.3.1</span> Create a Stratified Train–Test Split</h4>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2026</span>)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>split_obj <span class="ot">&lt;-</span> <span class="fu">initial_split</span>(ames_small, <span class="at">prop =</span> <span class="fl">0.80</span>, <span class="at">strata =</span> Sale_Price)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>train_data <span class="ot">&lt;-</span> <span class="fu">training</span>(split_obj)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>test_data  <span class="ot">&lt;-</span> <span class="fu">testing</span>(split_obj)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="fu">nrow</span>(train_data)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 2342</code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">nrow</span>(test_data)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 588</code></pre>
</div>
</div>
<p><strong>What this does.</strong></p>
<ul>
<li><p><code>prop = 0.80</code> assigns roughly 80% of the data to training and 20% to testing.</p></li>
<li><p><code>strata = Sale_Price</code> performs a <em>stratified</em> split based on the target variable.<br>
This reduces the risk that the test set ends up with an atypical concentration of very low or very high prices—something that can easily happen with skewed targets like house prices.</p></li>
</ul>
<p><strong>How to interpret the output.</strong></p>
<ul>
<li>If the full dataset contains 2,930 observations, you should see approximately:</li>
</ul>
<pre><code>-    training: 2,342 rows

-    test: 588 rows</code></pre>
<p>This corresponds closely to the intended 80/20 split and indicates that no unintended row loss occurred during preprocessing.</p>
</section>
<section id="sanity-check-is-the-target-distribution-similar-across-splits" class="level4" data-number="10.3.2">
<h4 data-number="10.3.2" class="anchored" data-anchor-id="sanity-check-is-the-target-distribution-similar-across-splits"><span class="header-section-number">10.3.2</span> Sanity Check: Is the Target Distribution Similar Across Splits?</h4>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">bind_rows</span>(</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>  train_data <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">split =</span> <span class="st">"train"</span>),</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>  test_data  <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">split =</span> <span class="st">"test"</span>)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> Sale_Price, <span class="at">fill =</span> split)) <span class="sc">+</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="at">bins =</span> <span class="dv">40</span>, <span class="at">alpha =</span> <span class="fl">0.7</span>, <span class="at">color =</span> <span class="st">"white"</span>) <span class="sc">+</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span> split, <span class="at">scales =</span> <span class="st">"free_y"</span>) <span class="sc">+</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_fill_manual</span>(</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">values =</span> <span class="fu">c</span>(<span class="at">train =</span> <span class="st">"#1f77b4"</span>, <span class="at">test =</span> <span class="st">"#ff7f0e"</span>)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">"Sale_Price distribution after train–test split"</span>,</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">"Sale_Price"</span>,</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">"Count"</span>,</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>    <span class="at">fill =</span> <span class="st">"Data split"</span></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p><strong>What to look for.</strong></p>
<ul>
<li><p>Both distributions should be right-skewed with a similar central mass.</p></li>
<li><p>There should be no strong imbalance where most expensive (or cheapest) homes appear in only one split.</p></li>
</ul>
<p>In the plot, the overall shapes are highly similar and the mid-range is well represented in both sets, indicating that stratification preserved the structure of the target variable across splits.</p>
</section>
<section id="optional-check-quick-summary-statistics" class="level4" data-number="10.3.3">
<h4 data-number="10.3.3" class="anchored" data-anchor-id="optional-check-quick-summary-statistics"><span class="header-section-number">10.3.3</span> Optional Check: Quick Summary Statistics</h4>
<p>This is a compact numerical confirmation of what the plot shows.</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>train_summary <span class="ot">&lt;-</span> train_data <span class="sc">%&gt;%</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summarise</span>(</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="at">split =</span> <span class="st">"train"</span>,</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="at">n =</span> <span class="fu">n</span>(),</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="at">mean =</span> <span class="fu">mean</span>(Sale_Price),</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="at">median =</span> <span class="fu">median</span>(Sale_Price),</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="at">sd =</span> <span class="fu">sd</span>(Sale_Price),</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="at">min =</span> <span class="fu">min</span>(Sale_Price),</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="at">max =</span> <span class="fu">max</span>(Sale_Price)</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>test_summary <span class="ot">&lt;-</span> test_data <span class="sc">%&gt;%</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a><span class="fu">summarise</span>(</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a><span class="at">split =</span> <span class="st">"test"</span>,</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a><span class="at">n =</span> <span class="fu">n</span>(),</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a><span class="at">mean =</span> <span class="fu">mean</span>(Sale_Price),</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a><span class="at">median =</span> <span class="fu">median</span>(Sale_Price),</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a><span class="at">sd =</span> <span class="fu">sd</span>(Sale_Price),</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a><span class="at">min =</span> <span class="fu">min</span>(Sale_Price),</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a><span class="at">max =</span> <span class="fu">max</span>(Sale_Price)</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a><span class="fu">bind_rows</span>(train_summary, test_summary)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 2 × 7
  split     n    mean median     sd   min    max
  &lt;chr&gt; &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;  &lt;int&gt;
1 train  2342 180447. 160000 79157. 12789 755000
2 test    588 182185. 160500 82784. 35311 625000</code></pre>
</div>
</div>
<p><strong>How to interpret this.</strong></p>
<ul>
<li><p>Small differences between train and test are expected.</p></li>
<li><p>Large gaps—especially in the median—may indicate an unbalanced split.</p></li>
</ul>
<p>Your summaries show nearly identical means and medians (train: 180,447 / 160,000; test: 182,185 / 160,500) and similar standard deviations, supporting the conclusion that the split is well balanced. Differences in the maximum values are expected due to rare high-priced homes and do not indicate a problematic split.</p>
<p>The train–test split is well balanced and suitable for downstream modeling. The test set can be treated as a genuine proxy for unseen data, allowing us to evaluate normalization strategies without confounding effects from an unbalanced split.</p>
</section>
</section>
<section id="model-specification-a-scale-sensitive-baseline" class="level3" data-number="10.4">
<h3 data-number="10.4" class="anchored" data-anchor-id="model-specification-a-scale-sensitive-baseline"><span class="header-section-number">10.4</span> Model Specification: A Scale-Sensitive Baseline</h3>
<p>Before comparing different normalization strategies, we must fix the modeling component of the pipeline. This ensures that any performance differences observed later can be attributed to preprocessing choices rather than to changes in the model itself.</p>
<p><strong>Why KNN Regression?</strong></p>
<p>We deliberately choose <strong>k-nearest neighbors (KNN) regression</strong> for this demonstration. The reason is methodological, not practical.</p>
<p>KNN is a <strong>distance-based algorithm</strong>: predictions are determined by the distances between observations in the feature space. As a result, KNN is highly sensitive to the scale of the predictors. Variables with larger numeric ranges can dominate distance calculations, even if they are not substantively more important.</p>
<p>This property makes KNN an ideal diagnostic tool for studying the effects of scaling.</p>
<hr>
<section id="model-specification" class="level4" data-number="10.4.1">
<h4 data-number="10.4.1" class="anchored" data-anchor-id="model-specification"><span class="header-section-number">10.4.1</span> Model Specification</h4>
<p>We define a single KNN model that will be used in all subsequent scenarios.</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>knn_spec <span class="ot">&lt;-</span> <span class="fu">nearest_neighbor</span>(</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">neighbors =</span> <span class="dv">15</span>,</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">weight_func =</span> <span class="st">"rectangular"</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">"kknn"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_mode</span>(<span class="st">"regression"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p><strong>Commentary.</strong></p>
<ul>
<li><p>The number of neighbors is fixed at 15 to reduce variance while maintaining locality.</p></li>
<li><p>No hyperparameter tuning is performed, as optimization is not the goal here.</p></li>
<li><p>This model specification will remain unchanged across all preprocessing pipelines.</p></li>
</ul>
</section>
</section>
<section id="scenario-a-baseline-no-scaling" class="level3" data-number="10.5">
<h3 data-number="10.5" class="anchored" data-anchor-id="scenario-a-baseline-no-scaling"><span class="header-section-number">10.5</span> Scenario A — Baseline: No Scaling</h3>
<p>We begin with a baseline workflow in which <strong>no scaling is applied</strong>. This provides a reference point against which all normalized pipelines will be compared.</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>rec_none <span class="ot">&lt;-</span> <span class="fu">recipe</span>(Sale_Price <span class="sc">~</span> ., <span class="at">data =</span> train_data)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>wf_none <span class="ot">&lt;-</span> <span class="fu">workflow</span>() <span class="sc">%&gt;%</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="fu">add_recipe</span>(rec_none) <span class="sc">%&gt;%</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="fu">add_model</span>(knn_spec)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>fit_none <span class="ot">&lt;-</span> <span class="fu">fit</span>(wf_none, <span class="at">data =</span> train_data)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<blockquote class="blockquote">
<p><strong>Note on model engines.</strong><br>
In the tidymodels ecosystem, model specifications are defined independently of the underlying computational engines. Although we specify the KNN model via <code>nearest_neighbor()</code>, the actual implementation is provided by the <code>kknn</code> package.</p>
<p>If the package is not installed, fitting the model will fail. To proceed, install and load the required engine:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">"kknn"</span>)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(kknn)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This separation between model specification and engine implementation is intentional and allows tidymodels to remain modular and extensible.</p>
</blockquote>
</div>
</div>
<section id="evaluate-on-the-test-set" class="level4" data-number="10.5.1">
<h4 data-number="10.5.1" class="anchored" data-anchor-id="evaluate-on-the-test-set"><span class="header-section-number">10.5.1</span> <strong>Evaluate on the Test Set</strong></h4>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>pred_none <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit_none, test_data) <span class="sc">%&gt;%</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="fu">bind_cols</span>(test_data <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">select</span>(Sale_Price))</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>metrics_none <span class="ot">&lt;-</span> yardstick<span class="sc">::</span><span class="fu">metrics</span>(</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>pred_none,</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="at">truth =</span> Sale_Price,</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="at">estimate =</span> .pred</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>metrics_none</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 3 × 3
  .metric .estimator .estimate
  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
1 rmse    standard   35643.   
2 rsq     standard       0.816
3 mae     standard   23726.   </code></pre>
</div>
</div>
</section>
<section id="interpretation" class="level4" data-number="10.5.2">
<h4 data-number="10.5.2" class="anchored" data-anchor-id="interpretation"><span class="header-section-number">10.5.2</span> Interpretation</h4>
<p>These values are not “good” or “bad” in isolation; what matters is that they provide a <strong>stable reference</strong>. At this stage, the model operates on raw predictor scales. For a distance-based method like KNN, this implies:</p>
<ul>
<li><p>Predictors with larger numeric ranges (e.g., <code>Lot_Area</code>) can disproportionately influence distance calculations.</p></li>
<li><p>Smaller-range variables (e.g., ordinal-like <code>Overall_Cond</code>) may contribute less than intended.</p></li>
<li><p>The model’s behavior is therefore partially shaped by measurement units, not only by predictive structure.</p></li>
</ul>
<p>This is exactly why KNN is a useful diagnostic tool in a normalization-focused article: if scaling matters, we should see clear changes relative to this baseline once we introduce normalization.</p>
<p>Next, we introduce scaling—but <strong>incorrectly</strong> on purpose. We will apply normalization <em>before</em> the train–test split (i.e., using information from the full dataset). This creates <strong>data leakage</strong> and can lead to deceptively improved test performance.</p>
<p>After that, we will implement the correct workflow (fit scaling parameters on the training set only) and compare all scenarios side by side.</p>
</section>
</section>
<section id="scenario-b-incorrect-normalization-data-leakage" class="level3" data-number="10.6">
<h3 data-number="10.6" class="anchored" data-anchor-id="scenario-b-incorrect-normalization-data-leakage"><span class="header-section-number">10.6</span> Scenario B — Incorrect Normalization (Data Leakage)</h3>
<p>In this scenario, we intentionally apply normalization <strong>the wrong way</strong>: we learn scaling parameters from the full dataset (including what will become the test set). This contaminates the evaluation because preprocessing has already “seen” information from the test distribution.</p>
<p>The goal is not to recommend this approach, but to demonstrate how easily leakage can happen—and how it can artificially improve test metrics.</p>
<section id="leakage-pipeline-normalize-using-full-data" class="level4" data-number="10.6.1">
<h4 data-number="10.6.1" class="anchored" data-anchor-id="leakage-pipeline-normalize-using-full-data"><span class="header-section-number">10.6.1</span> Leakage Pipeline: Normalize Using Full Data</h4>
<p>The <code>step_normalize()</code> operation applies only to numeric predictors. In our dataset, <code>Overall_Cond</code> is stored as a factor (ordinal-like category), so it must not be normalized directly.</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>rec_leak <span class="ot">&lt;-</span> <span class="fu">recipe</span>(Sale_Price <span class="sc">~</span> ., <span class="at">data =</span> ames_small) <span class="sc">%&gt;%</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_normalize</span>(<span class="fu">all_numeric_predictors</span>())</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="co"># WRONG on purpose: prepping on full data (leakage), but now type-safe</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>prep_leak <span class="ot">&lt;-</span> <span class="fu">prep</span>(rec_leak, <span class="at">training =</span> ames_small)</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>train_leak <span class="ot">&lt;-</span> <span class="fu">bake</span>(prep_leak, <span class="at">new_data =</span> train_data)</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>test_leak  <span class="ot">&lt;-</span> <span class="fu">bake</span>(prep_leak, <span class="at">new_data =</span> test_data)</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>wf_leak <span class="ot">&lt;-</span> <span class="fu">workflow</span>() <span class="sc">%&gt;%</span></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_model</span>(knn_spec) <span class="sc">%&gt;%</span></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_formula</span>(Sale_Price <span class="sc">~</span> .)</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>fit_leak <span class="ot">&lt;-</span> <span class="fu">fit</span>(wf_leak, <span class="at">data =</span> train_leak)</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>pred_leak <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit_leak, test_leak) <span class="sc">%&gt;%</span></span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">bind_cols</span>(test_leak <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">select</span>(Sale_Price))</span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>metrics_leak <span class="ot">&lt;-</span> yardstick<span class="sc">::</span><span class="fu">metrics</span>(pred_leak, <span class="at">truth =</span> Sale_Price, <span class="at">estimate =</span> .pred)</span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>metrics_leak</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 3 × 3
  .metric .estimator .estimate
  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
1 rmse    standard   37036.   
2 rsq     standard       0.801
3 mae     standard   24411.   </code></pre>
</div>
</div>
</section>
<section id="interpretation-1" class="level4" data-number="10.6.2">
<h4 data-number="10.6.2" class="anchored" data-anchor-id="interpretation-1"><span class="header-section-number">10.6.2</span> Interpretation</h4>
<p>The performance obtained under this scenario reflects the consequences of <strong>incorrect normalization with data leakage</strong>.</p>
<p>Compared to the baseline (no scaling), all three metrics deteriorate. This indicates that learning normalization parameters from the full dataset does <strong>not</strong> automatically lead to better predictive performance. In this case, the leakage-induced transformation appears to distort the distance structure in a way that is unfavorable for KNN.</p>
<p>This result is particularly instructive because it challenges a common misconception:<br>
<strong>data leakage does not necessarily inflate performance metrics</strong>. Its effect depends on the interaction between the preprocessing step, the data distribution, and the model. What leakage <em>does</em> guarantee, however, is that the evaluation is no longer valid.</p>
<p>Even if the metrics had improved under this scenario, they could not be trusted as estimates of out-of-sample performance. The test data would no longer represent genuinely unseen observations, since information from their distribution had already been incorporated during preprocessing.</p>
<p>At this point, two important conclusions can be drawn:</p>
<ol type="1">
<li><p>Scaling decisions materially affect model behavior, especially for distance-based methods.</p></li>
<li><p>The timing of scaling—<em>when</em> parameters are learned—is as critical as <em>whether</em> scaling is applied at all.</p></li>
</ol>
<p>In the next scenario, we apply normalization correctly by estimating scaling parameters using the training data only and then applying them unchanged to the test set. This will provide the only defensible estimate of generalization performance among the normalization strategies considered.</p>
</section>
</section>
<section id="scenario-c-correct-normalization-train-only-scaling" class="level3" data-number="10.7">
<h3 data-number="10.7" class="anchored" data-anchor-id="scenario-c-correct-normalization-train-only-scaling"><span class="header-section-number">10.7</span> Scenario C — Correct Normalization (Train-Only Scaling)</h3>
<p>In this final preprocessing scenario, normalization parameters are learned <strong>exclusively from the training data</strong> and then applied consistently to both the training and test sets.</p>
<p>This workflow adheres to the core principle of leakage-free modeling.</p>
<section id="correct-pipeline-normalize-using-training-data-only" class="level4" data-number="10.7.1">
<h4 data-number="10.7.1" class="anchored" data-anchor-id="correct-pipeline-normalize-using-training-data-only"><span class="header-section-number">10.7.1</span> Correct Pipeline: Normalize Using Training Data Only</h4>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>rec_ok <span class="ot">&lt;-</span> <span class="fu">recipe</span>(Sale_Price <span class="sc">~</span> ., <span class="at">data =</span> train_data) <span class="sc">%&gt;%</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_normalize</span>(<span class="fu">all_numeric_predictors</span>())</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>wf_ok <span class="ot">&lt;-</span> <span class="fu">workflow</span>() <span class="sc">%&gt;%</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_recipe</span>(rec_ok) <span class="sc">%&gt;%</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_model</span>(knn_spec)</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>fit_ok <span class="ot">&lt;-</span> <span class="fu">fit</span>(wf_ok, <span class="at">data =</span> train_data)</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>pred_ok <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit_ok, test_data) <span class="sc">%&gt;%</span></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">bind_cols</span>(test_data <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">select</span>(Sale_Price))</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>metrics_ok <span class="ot">&lt;-</span> yardstick<span class="sc">::</span><span class="fu">metrics</span>(pred_ok, <span class="at">truth =</span> Sale_Price, <span class="at">estimate =</span> .pred)</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>metrics_ok</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 3 × 3
  .metric .estimator .estimate
  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
1 rmse    standard   35643.   
2 rsq     standard       0.816
3 mae     standard   23726.   </code></pre>
</div>
</div>
</section>
<section id="interpretation-2" class="level4" data-number="10.7.2">
<h4 data-number="10.7.2" class="anchored" data-anchor-id="interpretation-2"><span class="header-section-number">10.7.2</span> Interpretation</h4>
<p>This scenario represents the <strong>correct normalization workflow</strong>, where scaling parameters are learned exclusively from the training data and then applied unchanged to the test set. The results are <strong>identical to the no-scaling baseline</strong>. This finding is highly informative.</p>
<p>First, it confirms that normalization itself does not automatically improve model performance. When applied correctly, scaling does not inject additional information into the modeling process; it merely changes the representation of the data. If the underlying distance structure relevant for prediction is already dominated by certain predictors, scaling may have little to no effect on performance.</p>
<p>Second, the contrast with the leakage scenario is crucial. In Scenario B, incorrect normalization degraded performance, while in this scenario, correct normalization restores the metrics to their baseline levels. This symmetry reinforces the core message of this article:<br>
<strong>the validity of preprocessing matters more than the apparent gains it may produce.</strong></p>
<p>Third, these results highlight an often-overlooked point: the impact of scaling is model- and data-dependent. For this particular subset of predictors and this KNN configuration, normalization neither helps nor harms when applied correctly. In other settings—different feature sets, different distance metrics, or different models—the effect could be substantial.</p>
<p>The key takeaway is therefore not that scaling is unnecessary, but that it must be:</p>
<p>applied deliberately,</p>
<p>restricted to appropriate variables,</p>
<p>and learned at the correct stage of the modeling workflow.</p>
<p>With all three scenarios evaluated, we can now compare them side by side and distill the practical lessons they offer.</p>
</section>
</section>
<section id="results-comparison" class="level3" data-number="10.8">
<h3 data-number="10.8" class="anchored" data-anchor-id="results-comparison"><span class="header-section-number">10.8</span> Results Comparison</h3>
<p>With all three scenarios evaluated, we now compare them side by side. Since the model and data split were held constant, any differences observed here are entirely attributable to preprocessing choices.</p>
<section id="performance-summary" class="level4" data-number="10.8.1">
<h4 data-number="10.8.1" class="anchored" data-anchor-id="performance-summary"><span class="header-section-number">10.8.1</span> Performance Summary</h4>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>results_tbl <span class="ot">&lt;-</span> dplyr<span class="sc">::</span><span class="fu">bind_rows</span>(</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>  metrics_none <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">scenario =</span> <span class="st">"A — No Scaling"</span>),</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>  metrics_leak <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">scenario =</span> <span class="st">"B — Incorrect Scaling (Leakage)"</span>),</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>  metrics_ok   <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">scenario =</span> <span class="st">"C — Correct Scaling (Train-Only)"</span>)</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">select</span>(scenario, .metric, .estimate) <span class="sc">%&gt;%</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>  tidyr<span class="sc">::</span><span class="fu">pivot_wider</span>(</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">names_from =</span> .metric,</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">values_from =</span> .estimate</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>results_tbl</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 3 × 4
  scenario                           rmse   rsq    mae
  &lt;chr&gt;                             &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;
1 A — No Scaling                   35643. 0.816 23726.
2 B — Incorrect Scaling (Leakage)  37036. 0.801 24411.
3 C — Correct Scaling (Train-Only) 35643. 0.816 23726.</code></pre>
</div>
</div>
<p>This table summarizes test-set performance across all scenarios.</p>
<ul>
<li><p><strong>Scenario A (No Scaling)</strong> serves as the baseline.</p></li>
<li><p><strong>Scenario B (Incorrect Scaling with Leakage)</strong> shows degraded performance.</p></li>
<li><p><strong>Scenario C (Correct Scaling)</strong> reproduces the baseline results exactly.</p></li>
</ul>
</section>
<section id="visual-comparison-rmse" class="level4" data-number="10.8.2">
<h4 data-number="10.8.2" class="anchored" data-anchor-id="visual-comparison-rmse"><span class="header-section-number">10.8.2</span> Visual Comparison (RMSE)</h4>
<p>To make the differences easier to interpret, we visualize RMSE across scenarios.</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>results_tbl <span class="sc">%&gt;%</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> scenario, <span class="at">y =</span> rmse, <span class="at">fill =</span> scenario)) <span class="sc">+</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="fu">geom_col</span>(<span class="at">alpha =</span> <span class="fl">0.8</span>) <span class="sc">+</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="fu">scale_fill_manual</span>(</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="at">values =</span> <span class="fu">c</span>(</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a><span class="st">"A — No Scaling"</span> <span class="ot">=</span> <span class="st">"#1f77b4"</span>,</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a><span class="st">"B — Incorrect Scaling (Leakage)"</span> <span class="ot">=</span> <span class="st">"#d62728"</span>,</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a><span class="st">"C — Correct Scaling (Train-Only)"</span> <span class="ot">=</span> <span class="st">"#2ca02c"</span></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>) <span class="sc">+</span></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a><span class="fu">labs</span>(</span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a><span class="at">title =</span> <span class="st">"RMSE comparison across preprocessing scenarios"</span>,</span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a><span class="at">x =</span> <span class="st">"Preprocessing scenario"</span>,</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a><span class="at">y =</span> <span class="st">"RMSE"</span></span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>) <span class="sc">+</span></span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a><span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a><span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"none"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/unnamed-chunk-11-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="interpretation-3" class="level4" data-number="10.8.3">
<h4 data-number="10.8.3" class="anchored" data-anchor-id="interpretation-3"><span class="header-section-number">10.8.3</span> Interpretation</h4>
<p>Several important conclusions emerge from this comparison.</p>
<p>First, <strong>normalization does not inherently improve performance</strong>. When applied correctly (Scenario C), scaling neither improves nor degrades performance relative to the no-scaling baseline. This confirms that normalization is a representational transformation, not a source of predictive signal.</p>
<p>Second, <strong>incorrect normalization can be harmful</strong>. Scenario B demonstrates that learning scaling parameters from the full dataset can distort the feature space in ways that negatively affect model behavior. Even more importantly, this scenario yields an invalid evaluation, regardless of whether the metrics appear better or worse.</p>
<p>Third, these results reinforce a central theme of this article:<br>
<strong>the correctness of the preprocessing workflow matters more than the choice of preprocessing method itself</strong>.</p>
<p>In practice, this means that:</p>
<ul>
<li><p>scaling should be applied only when it aligns with the model’s assumptions,</p></li>
<li><p>preprocessing parameters must be learned exclusively from training data,</p></li>
<li><p>and any apparent performance gains should be scrutinized for potential leakage.</p></li>
</ul>
</section>
</section>
<section id="practical-takeaways-from-the-application" class="level3" data-number="10.9">
<h3 data-number="10.9" class="anchored" data-anchor-id="practical-takeaways-from-the-application"><span class="header-section-number">10.9</span> Practical Takeaways from the Application</h3>
<p>From this controlled experiment, we can distill three practical lessons:</p>
<ol type="1">
<li><p><strong>Do not expect normalization to be a silver bullet.</strong> Its impact depends on the model, the data, and the feature set.</p></li>
<li><p><strong>Never compromise the train–test boundary.</strong> Leakage can invalidate results even when performance does not improve.</p></li>
<li><p><strong>Treat preprocessing as part of the model.</strong> Decisions about scaling are modeling decisions, not technical afterthoughts.</p></li>
</ol>
<p>These lessons generalize beyond KNN and apply to any workflow involving scale-sensitive models and data transformations.</p>
</section>
</section>
<section id="discussion-and-conclusion" class="level2" data-number="11">
<h2 data-number="11" class="anchored" data-anchor-id="discussion-and-conclusion"><span class="header-section-number">11</span> Discussion and Conclusion</h2>
<p>Normalization is often introduced as a routine preprocessing step, applied almost reflexively before modeling. This article has argued—and demonstrated—that such a view is incomplete. Normalization is not a purely technical adjustment; it is a <strong>modeling decision</strong> whose consequences depend on the interaction between data, model assumptions, and evaluation design.</p>
<p>From a theoretical perspective, scaling matters because many learning algorithms are sensitive to the relative magnitudes of predictors. Distance-based methods, regularized models, kernel methods, and optimization-driven algorithms implicitly encode assumptions about scale. Ignoring these assumptions can distort model behavior, while respecting them can improve stability and interpretability. At the same time, scaling does not create new information. It reshapes how existing information is represented.</p>
<p>The empirical application using the Ames Housing dataset reinforced these points. By holding the model and data split constant and varying only the preprocessing strategy, we isolated the effect of normalization decisions. Three key findings emerged.</p>
<p>First, <strong>normalization does not guarantee performance improvements</strong>. In the correct workflow, scaling reproduced the baseline results exactly. This confirms that normalization should not be expected to “fix” a model by itself. Its role is conditional and context-dependent.</p>
<p>Second, <strong>incorrect normalization compromises validity</strong>. Learning scaling parameters from the full dataset—thereby introducing data leakage—altered model behavior and degraded performance in this example. More importantly, even if the metrics had improved, the evaluation would have been invalid. Leakage undermines the fundamental purpose of a test set: to approximate unseen data.</p>
<p>Third, <strong>the timing of preprocessing is as important as the method chosen</strong>. The difference between valid and invalid evaluation hinged not on whether scaling was applied, but on <em>when</em> its parameters were learned. This distinction is often overlooked in practice, yet it is central to trustworthy modeling.</p>
<p>Taken together, these results support a broader principle: preprocessing steps should be treated as integral components of the modeling pipeline, not as detached technical preliminaries. Decisions about normalization should be guided by model assumptions, data characteristics, and evaluation design—not by habit or generic checklists.</p>
<p>In practical terms, this leads to a simple but robust rule:</p>
<blockquote class="blockquote">
<p><strong>Split the data first. Learn preprocessing parameters from the training set only. Apply the same transformations to all future data.</strong></p>
</blockquote>
<p>Normalization, when used deliberately and correctly, is a powerful tool. When applied mechanically or at the wrong stage, it can mislead. Understanding this distinction is essential for building models that are not only accurate, but also scientifically defensible.</p>
<hr>
</section>
<section id="references" class="level2" data-number="12">
<h2 data-number="12" class="anchored" data-anchor-id="references"><span class="header-section-number">12</span> References</h2>
<ul>
<li><p>Hastie, T., Tibshirani, R., &amp; Friedman, J. (2009).<br>
<em>The Elements of Statistical Learning: Data Mining, Inference, and Prediction</em>. Springer.</p></li>
<li><p>Kuhn, M., &amp; Johnson, K. (2013).<br>
<em>Applied Predictive Modeling</em>. Springer.</p></li>
<li><p>Kuhn, M., &amp; Wickham, H. (2023).<br>
<em>Tidymodels: A Collection of Packages for Modeling and Machine Learning Using Tidyverse Principles</em>.<br>
<a href="https://www.tidymodels.org/" class="uri">https://www.tidymodels.org/</a></p></li>
<li><p>Tidymodels Recipes Documentation.<br>
<a href="https://recipes.tidymodels.org/" class="uri">https://recipes.tidymodels.org/</a></p></li>
<li><p>Kuhn, M. (Caret package documentation).<br>
<a href="https://topepo.github.io/caret/" class="uri">https://topepo.github.io/caret/</a></p></li>
<li><p>Modeldata package documentation (Ames Housing dataset).<br>
<a href="https://modeldata.tidymodels.org/reference/ames.html" class="uri">https://modeldata.tidymodels.org/reference/ames.html</a></p></li>
</ul>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/mfatihtuzen\.netlify\.app");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<script src="https://utteranc.es/client.js" repo="MFatihTuzen/blogComments" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb23" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Data Normalization in R: When, Why, and How to Scale Your Data Correctly"</span></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "M. Fatih Tüzen"</span></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2026-01-02"</span></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a><span class="co">  - Data Preprocessing</span></span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a><span class="co">  - R Programming</span></span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a><span class="co">  - Data Science</span></span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a><span class="co">  - Machine Learning</span></span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a><span class="co">    toc: true</span></span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a><span class="co">    toc-depth: 4</span></span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a><span class="co">    number-sections: true</span></span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a><span class="co">    code-tools: true</span></span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a><span class="co">    code-overflow: scroll</span></span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a><span class="co">    code-block-background: true</span></span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a><span class="co">  pdf:</span></span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a><span class="co">    toc: true</span></span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a><span class="an">execute:</span></span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a><span class="co">  echo: true</span></span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a><span class="co">  warning: false</span></span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a><span class="co">  message: false</span></span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb23-25"><a href="#cb23-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-26"><a href="#cb23-26" aria-hidden="true" tabindex="-1"></a><span class="al">![](normalization.png)</span>{fig-align="center" width="491"}</span>
<span id="cb23-27"><a href="#cb23-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-28"><a href="#cb23-28" aria-hidden="true" tabindex="-1"></a><span class="fu">## Introduction</span></span>
<span id="cb23-29"><a href="#cb23-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-30"><a href="#cb23-30" aria-hidden="true" tabindex="-1"></a>This article is part of a broader series on **data preprocessing in R**. In earlier posts, we focused on two problems that quietly ruin analyses long before modeling begins: **missing data** and **outliers**. Both topics shared a common theme: preprocessing choices are not cosmetic; they change what the model is allowed to learn. In this installment, we move to the next decision point in the same pipeline: **normalization (scaling)**—often treated as “just a quick step,” but in practice a decisive modeling choice.</span>
<span id="cb23-31"><a href="#cb23-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-32"><a href="#cb23-32" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **Related posts in this preprocessing series**</span></span>
<span id="cb23-33"><a href="#cb23-33" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb23-34"><a href="#cb23-34" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; -   *Handling Missing Data in R: A Comprehensive Guide*\</span></span>
<span id="cb23-35"><a href="#cb23-35" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;     </span><span class="ot">&lt;https://medium.com/r-evolution/handling-missing-data-in-r-a-comprehensive-guide-eca195eaead3&gt;</span></span>
<span id="cb23-36"><a href="#cb23-36" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb23-37"><a href="#cb23-37" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; -   *Outliers in Data Analysis: Detecting Extreme Values Before Modeling in R*\</span></span>
<span id="cb23-38"><a href="#cb23-38" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;     </span><span class="ot">&lt;https://medium.com/r-evolution/outliers-in-data-analysis-detecting-extreme-values-before-modeling-in-r-with-i%CC%87stanbul-airbnb-data-3b37e9ee989e&gt;</span></span>
<span id="cb23-39"><a href="#cb23-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-40"><a href="#cb23-40" aria-hidden="true" tabindex="-1"></a>Normalization (or more broadly, scaling) is frequently presented as a minor technical adjustment—something to apply quickly and forget. In practice, scaling is not a technical detail but a modeling decision. When the same dataset is processed using different scaling strategies, the behavior of many models changes substantially. Distances, similarity measures, penalty terms, and optimization paths are all affected. As a result, the nearest neighbors selected by KNN, the clusters formed by K-means, the principal components identified by PCA, and even the coefficients chosen by Ridge or Lasso regression can differ. Scaling does not merely “prepare” the data; it actively shapes how a model interprets importance and structure.</span>
<span id="cb23-41"><a href="#cb23-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-42"><a href="#cb23-42" aria-hidden="true" tabindex="-1"></a>More importantly, scaling is not universally beneficial. Applied in the wrong context, it can degrade model performance or—worse—introduce subtle forms of **data leakage** that contaminate evaluation. A common example is learning scaling parameters (such as means and standard deviations) from the entire dataset before splitting into training and test sets. This procedure allows information from the test distribution to leak into the training process, producing performance estimates that cannot be trusted. In such cases, the issue is not the scaling method itself, but **when and how** it is applied. Knowing how to call <span class="in">`scale()`</span> in R is trivial; understanding what to scale, when to scale it, and why is not.</span>
<span id="cb23-43"><a href="#cb23-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-44"><a href="#cb23-44" aria-hidden="true" tabindex="-1"></a>In this article, normalization is treated as an integral part of the modeling strategy rather than a routine preprocessing step. We will address, step by step, the following questions:</span>
<span id="cb23-45"><a href="#cb23-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-46"><a href="#cb23-46" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Why is normalization necessary?</span>
<span id="cb23-47"><a href="#cb23-47" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Should it always be applied?</span>
<span id="cb23-48"><a href="#cb23-48" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>At what stage should it be performed—before or after the train–test split?</span>
<span id="cb23-49"><a href="#cb23-49" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Which scaling methods are commonly used, and in which contexts do they make sense?</span>
<span id="cb23-50"><a href="#cb23-50" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Should different data types be treated differently?</span>
<span id="cb23-51"><a href="#cb23-51" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Is scaling appropriate for all variables, including the target variable?</span>
<span id="cb23-52"><a href="#cb23-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-53"><a href="#cb23-53" aria-hidden="true" tabindex="-1"></a>By combining conceptual discussion with practical R implementations, this guide aims to provide clear and principled answers to each of these questions.</span>
<span id="cb23-54"><a href="#cb23-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-55"><a href="#cb23-55" aria-hidden="true" tabindex="-1"></a><span class="fu">## Normalization vs. Standardization: Clearing Up the Terminology</span></span>
<span id="cb23-56"><a href="#cb23-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-57"><a href="#cb23-57" aria-hidden="true" tabindex="-1"></a>In both academic writing and everyday practice, the terms *normalization* and *standardization* are frequently used interchangeably. This loose usage is one of the main sources of confusion in data preprocessing. In reality, these terms refer to **different scaling strategies**, each with distinct assumptions, effects, and use cases. Before discussing when and how scaling should be applied, it is therefore essential to clarify what is actually meant by each approach.</span>
<span id="cb23-58"><a href="#cb23-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-59"><a href="#cb23-59" aria-hidden="true" tabindex="-1"></a>**Standardization**, often referred to as *z-score scaling*, rescales a variable so that it has a mean of zero and a standard deviation of one. Formally, each observation is transformed by subtracting the sample mean and dividing by the sample standard deviation. In the R ecosystem, this logic is implemented in preprocessing tools such as `step_normalize()` from the **recipes** package. Standardization preserves the shape of the original distribution while putting variables on a comparable scale. It is particularly useful for models that are sensitive to the relative magnitude of predictors, such as linear models with regularization, support vector machines, and neural networks.</span>
<span id="cb23-60"><a href="#cb23-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-61"><a href="#cb23-61" aria-hidden="true" tabindex="-1"></a>**Normalization**, in a stricter sense, often refers to *min–max scaling*. This approach rescales variables to lie within a fixed interval, most commonly <span class="sc">\[</span>0,1<span class="sc">\]</span>. Each value is transformed based on the minimum and maximum observed in the training data. Min–max scaling is easy to interpret and is frequently used in algorithms where bounded inputs are desirable. However, it is also more sensitive to extreme values, since a single outlier can heavily influence the scaling range.</span>
<span id="cb23-62"><a href="#cb23-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-63"><a href="#cb23-63" aria-hidden="true" tabindex="-1"></a>A third commonly used approach is **robust scaling**, which relies on the median and the interquartile range (IQR) instead of the mean and standard deviation. By construction, this method is less affected by outliers and heavy-tailed distributions. Robust scaling is especially useful in real-world datasets where extreme values are not errors but genuine observations. At the same time, it is not a universal solution; in some data structures, robust measures may become unstable or uninformative.</span>
<span id="cb23-64"><a href="#cb23-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-65"><a href="#cb23-65" aria-hidden="true" tabindex="-1"></a>The reason terminology becomes blurred in practice is simple: many practitioners use the word *normalization* as a generic label for “any kind of scaling.” As a result, two people may both say they normalized their data while having applied entirely different transformations. Throughout this article, we will avoid this ambiguity by explicitly stating which scaling method is used and why. This distinction is not pedantic—it is essential for understanding how scaling choices influence model behavior.</span>
<span id="cb23-66"><a href="#cb23-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-67"><a href="#cb23-67" aria-hidden="true" tabindex="-1"></a><span class="fu">## Why Is Normalization Necessary?</span></span>
<span id="cb23-68"><a href="#cb23-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-69"><a href="#cb23-69" aria-hidden="true" tabindex="-1"></a>The necessity of normalization becomes clear once we recognize that many modeling techniques do not operate on raw variable values directly, but on **relationships derived from them**—such as distances, similarities, penalties, or variance directions. When predictors are measured on different scales, these derived quantities can be dominated by variables with larger numerical ranges, regardless of their substantive importance. In such cases, the model does not learn from the data structure itself, but from arbitrary measurement units.</span>
<span id="cb23-70"><a href="#cb23-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-71"><a href="#cb23-71" aria-hidden="true" tabindex="-1"></a>This issue is most apparent in **distance-based methods** such as k-nearest neighbors (KNN) and K-means clustering. These algorithms rely explicitly on distance calculations, typically Euclidean distance. If one variable ranges between 0 and 1 while another ranges between 0 and 10,000, the latter will dominate the distance computation almost entirely. As a result, proximity is determined not by overall similarity but by the scale of a single variable. Normalization ensures that each predictor contributes to the distance metric in a balanced and interpretable way, allowing the algorithm to reflect genuine similarity rather than numerical magnitude.</span>
<span id="cb23-72"><a href="#cb23-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-73"><a href="#cb23-73" aria-hidden="true" tabindex="-1"></a>Normalization is equally critical in models that incorporate **regularization**, such as Ridge and Lasso regression. In these models, coefficients are penalized to control model complexity. However, the penalty term is directly tied to the scale of the predictors. If variables are not on comparable scales, the regularization mechanism will shrink coefficients unevenly, effectively penalizing some predictors more than others for reasons unrelated to their predictive relevance. Scaling aligns the predictors so that regularization operates as intended: as a constraint on model complexity rather than an artifact of measurement units.</span>
<span id="cb23-74"><a href="#cb23-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-75"><a href="#cb23-75" aria-hidden="true" tabindex="-1"></a>Other widely used techniques—including **support vector machines (SVMs)**, **neural networks**, and **principal component analysis (PCA)**—are also highly sensitive to scaling. In SVMs and neural networks, optimization procedures depend on gradients that are influenced by feature magnitudes, affecting both convergence speed and stability. In PCA, the directions of maximum variance are determined by the scale of the variables; without normalization, components may simply reflect variables with the largest variances rather than the most informative underlying structure. In all these cases, scaling is not an optional refinement but a prerequisite for meaningful model behavior.</span>
<span id="cb23-76"><a href="#cb23-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-77"><a href="#cb23-77" aria-hidden="true" tabindex="-1"></a>By contrast, **tree-based models** such as decision trees, random forests, and gradient boosting machines are generally invariant to monotonic transformations of individual predictors. Since splits are based on ordering rather than distance or magnitude, scaling is often unnecessary for these methods. Nevertheless, this does not imply that normalization is universally irrelevant in tree-based pipelines. Hybrid workflows—where tree-based models are combined with distance-based components, rule-based similarity measures, or downstream models sensitive to scale—may still require careful consideration of scaling choices. The key point is not that normalization should always be applied, but that it should be applied **with respect to the assumptions of the modeling approach**.</span>
<span id="cb23-78"><a href="#cb23-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-79"><a href="#cb23-79" aria-hidden="true" tabindex="-1"></a>From a broader perspective, normalization plays a central role in modern predictive modeling workflows. As emphasized in the predictive modeling literature, preprocessing steps are not independent of the model; they are part of the modeling strategy itself. Scaling decisions shape how information is represented and, ultimately, how learning takes place. Understanding *why* normalization is necessary is therefore a prerequisite for deciding *when* and *how* it should be applied—a topic we address next.</span>
<span id="cb23-80"><a href="#cb23-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-81"><a href="#cb23-81" aria-hidden="true" tabindex="-1"></a><span class="fu">## Should Normalization Always Be Applied?</span></span>
<span id="cb23-82"><a href="#cb23-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-83"><a href="#cb23-83" aria-hidden="true" tabindex="-1"></a>A natural question at this point is whether normalization should be applied by default in every modeling task. The short answer is **no**. Normalization is not a universally beneficial preprocessing step; its usefulness depends on the assumptions and internal mechanics of the chosen model. Applying scaling blindly can be as problematic as ignoring it altogether. What is needed is a **decision framework** that links model characteristics to preprocessing choices.</span>
<span id="cb23-84"><a href="#cb23-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-85"><a href="#cb23-85" aria-hidden="true" tabindex="-1"></a>For a large class of models, normalization is **strongly recommended**. This group includes distance-based methods such as k-nearest neighbors (KNN) and K-means clustering, as well as techniques like principal component analysis (PCA), support vector machines (SVMs), neural networks, and penalized regression models (Ridge, Lasso, Elastic Net). In all these cases, either distances, inner products, variance directions, or penalty terms play a central role. Without scaling, these mechanisms are dominated by variables with larger numerical ranges, leading to distorted learning behavior. For such models, normalization is not a refinement but a prerequisite for meaningful results.</span>
<span id="cb23-86"><a href="#cb23-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-87"><a href="#cb23-87" aria-hidden="true" tabindex="-1"></a>By contrast, normalization is **generally unnecessary** for tree-based models such as decision trees, random forests, and gradient boosting machines (e.g., XGBoost, GBM). These models rely on recursive binary splits based on variable ordering rather than on distances or magnitudes. Since monotonic transformations do not affect the relative ordering of values, scaling typically has no impact on model performance. As a result, normalization is often omitted in purely tree-based pipelines without any loss of effectiveness.</span>
<span id="cb23-88"><a href="#cb23-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-89"><a href="#cb23-89" aria-hidden="true" tabindex="-1"></a>Between these two extremes lies a set of models for which normalization is **context-dependent**. Ordinary linear regression, for example, does not require scaling for estimation itself, but normalization may still be useful for numerical stability, interpretability of coefficients, or comparability across predictors. Similarly, Naive Bayes models may or may not benefit from scaling depending on the assumed feature distributions and the types of variables involved. In these cases, the decision to normalize should be guided by the modeling objective rather than by a fixed rule.</span>
<span id="cb23-90"><a href="#cb23-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-91"><a href="#cb23-91" aria-hidden="true" tabindex="-1"></a>The key takeaway is that normalization should be applied **with respect to the model’s assumptions**, not as a default preprocessing habit. To make this decision explicit, Table 1 summarizes common modeling approaches and whether normalization is typically required.</span>
<span id="cb23-92"><a href="#cb23-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-93"><a href="#cb23-93" aria-hidden="true" tabindex="-1"></a><span class="fu">### When Is Normalization Needed? A Model-Based Decision Table</span></span>
<span id="cb23-94"><a href="#cb23-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-95"><a href="#cb23-95" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Model / Method                <span class="pp">|</span> Is Normalization Recommended? <span class="pp">|</span> Rationale                                                                <span class="pp">|</span></span>
<span id="cb23-96"><a href="#cb23-96" aria-hidden="true" tabindex="-1"></a><span class="pp">|------------------|------------------|-----------------------------------|</span></span>
<span id="cb23-97"><a href="#cb23-97" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> KNN                           <span class="pp">|</span> Yes                           <span class="pp">|</span> Distance calculations are scale-sensitive                                <span class="pp">|</span></span>
<span id="cb23-98"><a href="#cb23-98" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> K-means                       <span class="pp">|</span> Yes                           <span class="pp">|</span> Cluster assignment depends on distances                                  <span class="pp">|</span></span>
<span id="cb23-99"><a href="#cb23-99" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> PCA                           <span class="pp">|</span> Yes                           <span class="pp">|</span> Variance directions dominated by scale                                   <span class="pp">|</span></span>
<span id="cb23-100"><a href="#cb23-100" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> SVM                           <span class="pp">|</span> Yes                           <span class="pp">|</span> Optimization and margins depend on feature magnitude                     <span class="pp">|</span></span>
<span id="cb23-101"><a href="#cb23-101" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Neural Networks               <span class="pp">|</span> Yes                           <span class="pp">|</span> Gradient-based optimization is scale-sensitive                           <span class="pp">|</span></span>
<span id="cb23-102"><a href="#cb23-102" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Ridge / Lasso / Elastic Net   <span class="pp">|</span> Yes                           <span class="pp">|</span> Penalty terms depend on predictor scale                                  <span class="pp">|</span></span>
<span id="cb23-103"><a href="#cb23-103" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Linear Regression (OLS)       <span class="pp">|</span> Depends                       <span class="pp">|</span> Not required for estimation, but useful for stability and interpretation <span class="pp">|</span></span>
<span id="cb23-104"><a href="#cb23-104" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Naive Bayes                   <span class="pp">|</span> Depends                       <span class="pp">|</span> Depends on feature types and distributional assumptions                  <span class="pp">|</span></span>
<span id="cb23-105"><a href="#cb23-105" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Decision Trees                <span class="pp">|</span> No                            <span class="pp">|</span> Split rules depend on ordering, not scale                                <span class="pp">|</span></span>
<span id="cb23-106"><a href="#cb23-106" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Random Forest / GBM / XGBoost <span class="pp">|</span> No                            <span class="pp">|</span> Tree-based structure is scale-invariant                                  <span class="pp">|</span></span>
<span id="cb23-107"><a href="#cb23-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-108"><a href="#cb23-108" aria-hidden="true" tabindex="-1"></a><span class="fu">## When Should Normalization Be Applied? Before or After the Train–Test Split?</span></span>
<span id="cb23-109"><a href="#cb23-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-110"><a href="#cb23-110" aria-hidden="true" tabindex="-1"></a>This is the most critical question in the entire preprocessing workflow—and the point at which many otherwise sound analyses quietly go wrong. The issue is not *whether* normalization should be applied, but **when** it should be applied. At the center of this question lies a fundamental concept in predictive modeling: **data leakage**.</span>
<span id="cb23-111"><a href="#cb23-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-112"><a href="#cb23-112" aria-hidden="true" tabindex="-1"></a>Data leakage occurs when information from outside the training set is used, directly or indirectly, during model training. In the context of normalization, leakage typically arises when scaling parameters—such as means and standard deviations (for standardization) or minimum and maximum values (for min–max scaling)—are estimated using the full dataset before splitting into training and test sets. Although this may appear harmless, it allows information from the test set to influence the preprocessing step, leading to overly optimistic performance estimates.</span>
<span id="cb23-113"><a href="#cb23-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-114"><a href="#cb23-114" aria-hidden="true" tabindex="-1"></a>The correct principle is straightforward but non-negotiable:\</span>
<span id="cb23-115"><a href="#cb23-115" aria-hidden="true" tabindex="-1"></a>**scaling parameters must be learned exclusively from the training data**.\</span>
<span id="cb23-116"><a href="#cb23-116" aria-hidden="true" tabindex="-1"></a>Once learned, the *same transformation*—with fixed parameters—must be applied to the test set and to any future, unseen data. This ensures that the test set truly represents new information and that model evaluation reflects genuine generalization rather than procedural artifacts.</span>
<span id="cb23-117"><a href="#cb23-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-118"><a href="#cb23-118" aria-hidden="true" tabindex="-1"></a>This principle is central to modern modeling frameworks. In the **tidymodels/recipes** philosophy, preprocessing steps are *trained* on the training data and then *applied* consistently to all other datasets. Similarly, in the **caret** framework, preprocessing transformations are estimated from the training set and reused when predicting on new data. In both cases, preprocessing is treated as part of the model training process—not as an independent, preliminary operation.</span>
<span id="cb23-119"><a href="#cb23-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-120"><a href="#cb23-120" aria-hidden="true" tabindex="-1"></a>To see why this distinction matters, consider the following conceptual comparison.</span>
<span id="cb23-121"><a href="#cb23-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-122"><a href="#cb23-122" aria-hidden="true" tabindex="-1"></a><span class="fu">### An Illustrative Example: Scaling Before vs. After the Split</span></span>
<span id="cb23-123"><a href="#cb23-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-124"><a href="#cb23-124" aria-hidden="true" tabindex="-1"></a>Suppose we have a dataset that we intend to split into training and test sets. We want to standardize a numeric predictor using z-score scaling.</span>
<span id="cb23-125"><a href="#cb23-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-126"><a href="#cb23-126" aria-hidden="true" tabindex="-1"></a>**Incorrect approach (scaling before the split):**</span>
<span id="cb23-127"><a href="#cb23-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-128"><a href="#cb23-128" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>Compute the mean and standard deviation using the *entire dataset*.</span>
<span id="cb23-129"><a href="#cb23-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-130"><a href="#cb23-130" aria-hidden="true" tabindex="-1"></a><span class="ss">2.  </span>Standardize all observations using these global parameters.</span>
<span id="cb23-131"><a href="#cb23-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-132"><a href="#cb23-132" aria-hidden="true" tabindex="-1"></a><span class="ss">3.  </span>Split the scaled data into training and test sets.</span>
<span id="cb23-133"><a href="#cb23-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-134"><a href="#cb23-134" aria-hidden="true" tabindex="-1"></a><span class="ss">4.  </span>Train and evaluate the model.</span>
<span id="cb23-135"><a href="#cb23-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-136"><a href="#cb23-136" aria-hidden="true" tabindex="-1"></a>At first glance, this workflow seems efficient. However, the scaling parameters already incorporate information from the test set. The test data are no longer independent of the training process, even though they were not explicitly used to fit the model.</span>
<span id="cb23-137"><a href="#cb23-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-138"><a href="#cb23-138" aria-hidden="true" tabindex="-1"></a>**Correct approach (scaling after the split):**</span>
<span id="cb23-139"><a href="#cb23-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-140"><a href="#cb23-140" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>Split the raw data into training and test sets.</span>
<span id="cb23-141"><a href="#cb23-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-142"><a href="#cb23-142" aria-hidden="true" tabindex="-1"></a><span class="ss">2.  </span>Compute scaling parameters (mean, standard deviation, etc.) *using only the training set*.</span>
<span id="cb23-143"><a href="#cb23-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-144"><a href="#cb23-144" aria-hidden="true" tabindex="-1"></a><span class="ss">3.  </span>Apply the learned transformation to the training set.</span>
<span id="cb23-145"><a href="#cb23-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-146"><a href="#cb23-146" aria-hidden="true" tabindex="-1"></a><span class="ss">4.  </span>Apply the *same* transformation to the test set.</span>
<span id="cb23-147"><a href="#cb23-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-148"><a href="#cb23-148" aria-hidden="true" tabindex="-1"></a><span class="ss">5.  </span>Train the model on the scaled training data and evaluate it on the scaled test data.</span>
<span id="cb23-149"><a href="#cb23-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-150"><a href="#cb23-150" aria-hidden="true" tabindex="-1"></a>In practice, these two approaches can lead to noticeably different evaluation results. Models trained using the incorrect workflow often appear to perform better on the test set—not because they generalize better, but because the preprocessing step has already “seen” the test data. This difference is especially pronounced in smaller datasets, in datasets with strong distributional differences between training and test splits, or when extreme values are present.</span>
<span id="cb23-151"><a href="#cb23-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-152"><a href="#cb23-152" aria-hidden="true" tabindex="-1"></a>The takeaway is unambiguous:</span>
<span id="cb23-153"><a href="#cb23-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-154"><a href="#cb23-154" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **Split the data first.\</span></span>
<span id="cb23-155"><a href="#cb23-155" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Fit preprocessing steps on the training data.\</span></span>
<span id="cb23-156"><a href="#cb23-156" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Apply the same transformations to the training and test sets.**</span></span>
<span id="cb23-157"><a href="#cb23-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-158"><a href="#cb23-158" aria-hidden="true" tabindex="-1"></a>Any deviation from this sequence undermines the validity of model evaluation, regardless of how sophisticated the modeling technique may be.</span>
<span id="cb23-159"><a href="#cb23-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-160"><a href="#cb23-160" aria-hidden="true" tabindex="-1"></a><span class="fu">## Common Normalization Methods and When to Use Them</span></span>
<span id="cb23-161"><a href="#cb23-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-162"><a href="#cb23-162" aria-hidden="true" tabindex="-1"></a>Normalization is not a single technique but a family of transformations, each designed to address a specific modeling concern. Choosing an appropriate method requires understanding **what problem the transformation is solving** and **which assumptions it implicitly makes**. In this section, we review the most commonly used scaling approaches, discuss their strengths and limitations, and clarify when each method is appropriate.</span>
<span id="cb23-163"><a href="#cb23-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-164"><a href="#cb23-164" aria-hidden="true" tabindex="-1"></a><span class="fu">### Z-score Standardization</span></span>
<span id="cb23-165"><a href="#cb23-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-166"><a href="#cb23-166" aria-hidden="true" tabindex="-1"></a>Z-score standardization rescales a variable so that it has a mean of zero and a standard deviation of one. Each observation $x_i$ is transformed as:</span>
<span id="cb23-167"><a href="#cb23-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-168"><a href="#cb23-168" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb23-169"><a href="#cb23-169" aria-hidden="true" tabindex="-1"></a>z_i = \frac{x_i - \mu}{\sigma},</span>
<span id="cb23-170"><a href="#cb23-170" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb23-171"><a href="#cb23-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-172"><a href="#cb23-172" aria-hidden="true" tabindex="-1"></a>where $\mu$ denotes the sample mean and $\sigma$ the sample standard deviation, both estimated **from the training data only**.</span>
<span id="cb23-173"><a href="#cb23-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-174"><a href="#cb23-174" aria-hidden="true" tabindex="-1"></a>**Advantages.**\</span>
<span id="cb23-175"><a href="#cb23-175" aria-hidden="true" tabindex="-1"></a>Z-score standardization places variables on a comparable scale while preserving the shape of their original distributions. It is particularly suitable for models that rely on inner products, gradient-based optimization, or regularization (e.g., penalized linear models, SVMs, neural networks).</span>
<span id="cb23-176"><a href="#cb23-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-177"><a href="#cb23-177" aria-hidden="true" tabindex="-1"></a>**Limitations.**\</span>
<span id="cb23-178"><a href="#cb23-178" aria-hidden="true" tabindex="-1"></a>A widespread misconception is that standardization assumes normally distributed data. This is incorrect. Z-score scaling does **not** require normality; it only uses the first two moments of the distribution. However, it is sensitive to extreme values: large outliers can inflate $\sigma$, thereby reducing the relative influence of most observations.</span>
<span id="cb23-179"><a href="#cb23-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-180"><a href="#cb23-180" aria-hidden="true" tabindex="-1"></a>**When to use.**\</span>
<span id="cb23-181"><a href="#cb23-181" aria-hidden="true" tabindex="-1"></a>A strong default choice when predictors differ substantially in scale and when outliers are either absent or have already been treated.</span>
<span id="cb23-182"><a href="#cb23-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-183"><a href="#cb23-183" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb23-184"><a href="#cb23-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-185"><a href="#cb23-185" aria-hidden="true" tabindex="-1"></a><span class="fu">### Min–Max (Range) Scaling</span></span>
<span id="cb23-186"><a href="#cb23-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-187"><a href="#cb23-187" aria-hidden="true" tabindex="-1"></a>Min–max scaling rescales variables to a fixed interval, most commonly $<span class="co">[</span><span class="ot">0, 1</span><span class="co">]</span>$. The transformation is:</span>
<span id="cb23-188"><a href="#cb23-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-189"><a href="#cb23-189" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb23-190"><a href="#cb23-190" aria-hidden="true" tabindex="-1"></a>x_i^{*} = \frac{x_i - \min(x)}{\max(x) - \min(x)}.</span>
<span id="cb23-191"><a href="#cb23-191" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb23-192"><a href="#cb23-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-193"><a href="#cb23-193" aria-hidden="true" tabindex="-1"></a>**Advantages.**\</span>
<span id="cb23-194"><a href="#cb23-194" aria-hidden="true" tabindex="-1"></a>Intuitive and ensures all transformed values lie within a predefined range. Often used when bounded inputs are desirable (e.g., some neural network settings).</span>
<span id="cb23-195"><a href="#cb23-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-196"><a href="#cb23-196" aria-hidden="true" tabindex="-1"></a>**Limitations.**\</span>
<span id="cb23-197"><a href="#cb23-197" aria-hidden="true" tabindex="-1"></a>Highly sensitive to extreme values: a single outlier can stretch the range and compress most observations. Also, when applied to test or future data, transformed values may fall outside $<span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>$ if they exceed the training-set min/max. This is expected and must be handled in deployment.</span>
<span id="cb23-198"><a href="#cb23-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-199"><a href="#cb23-199" aria-hidden="true" tabindex="-1"></a>**When to use.**\</span>
<span id="cb23-200"><a href="#cb23-200" aria-hidden="true" tabindex="-1"></a>When input bounds are meaningful and the training data represent the likely range of future observations.</span>
<span id="cb23-201"><a href="#cb23-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-202"><a href="#cb23-202" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb23-203"><a href="#cb23-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-204"><a href="#cb23-204" aria-hidden="true" tabindex="-1"></a><span class="fu">### Robust Scaling (Median and IQR)</span></span>
<span id="cb23-205"><a href="#cb23-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-206"><a href="#cb23-206" aria-hidden="true" tabindex="-1"></a>Robust scaling replaces mean and standard deviation with the median and the interquartile range (IQR). The transformation is:</span>
<span id="cb23-207"><a href="#cb23-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-208"><a href="#cb23-208" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb23-209"><a href="#cb23-209" aria-hidden="true" tabindex="-1"></a>x_i^{*} = \frac{x_i - \mathrm{median}(x)}{\mathrm{IQR}(x)},</span>
<span id="cb23-210"><a href="#cb23-210" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb23-211"><a href="#cb23-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-212"><a href="#cb23-212" aria-hidden="true" tabindex="-1"></a>where:</span>
<span id="cb23-213"><a href="#cb23-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-214"><a href="#cb23-214" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb23-215"><a href="#cb23-215" aria-hidden="true" tabindex="-1"></a>\mathrm{IQR}(x) = Q_{0.75} - Q_{0.25}.</span>
<span id="cb23-216"><a href="#cb23-216" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb23-217"><a href="#cb23-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-218"><a href="#cb23-218" aria-hidden="true" tabindex="-1"></a>**Advantages.**\</span>
<span id="cb23-219"><a href="#cb23-219" aria-hidden="true" tabindex="-1"></a>Less affected by extreme values and heavy-tailed distributions; useful when outliers are meaningful rather than errors.</span>
<span id="cb23-220"><a href="#cb23-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-221"><a href="#cb23-221" aria-hidden="true" tabindex="-1"></a>**Limitations.**\</span>
<span id="cb23-222"><a href="#cb23-222" aria-hidden="true" tabindex="-1"></a>Not universally stable. In highly concentrated variables, $\mathrm{IQR}(x)$ (or related robust measures such as MAD) may be zero or extremely small, making the transformation unstable or undefined. This must be checked explicitly.</span>
<span id="cb23-223"><a href="#cb23-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-224"><a href="#cb23-224" aria-hidden="true" tabindex="-1"></a>**When to use.**\</span>
<span id="cb23-225"><a href="#cb23-225" aria-hidden="true" tabindex="-1"></a>When outliers are present and structurally inherent, and you want scaling that is less sensitive to extremes.</span>
<span id="cb23-226"><a href="#cb23-226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-227"><a href="#cb23-227" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb23-228"><a href="#cb23-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-229"><a href="#cb23-229" aria-hidden="true" tabindex="-1"></a><span class="fu">### Power Transformations Combined with Scaling (Box–Cox and Yeo–Johnson)</span></span>
<span id="cb23-230"><a href="#cb23-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-231"><a href="#cb23-231" aria-hidden="true" tabindex="-1"></a>Power transformations aim to stabilize variance and reduce skewness before scaling.</span>
<span id="cb23-232"><a href="#cb23-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-233"><a href="#cb23-233" aria-hidden="true" tabindex="-1"></a>The **Box–Cox transformation** (for strictly positive data) is:</span>
<span id="cb23-234"><a href="#cb23-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-235"><a href="#cb23-235" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb23-236"><a href="#cb23-236" aria-hidden="true" tabindex="-1"></a>x_i^{(\lambda)} =</span>
<span id="cb23-237"><a href="#cb23-237" aria-hidden="true" tabindex="-1"></a>\begin{cases}</span>
<span id="cb23-238"><a href="#cb23-238" aria-hidden="true" tabindex="-1"></a>\frac{x_i^{\lambda} - 1}{\lambda}, &amp; \lambda \neq 0, <span class="sc">\\\\</span></span>
<span id="cb23-239"><a href="#cb23-239" aria-hidden="true" tabindex="-1"></a>\log(x_i), &amp; \lambda = 0.</span>
<span id="cb23-240"><a href="#cb23-240" aria-hidden="true" tabindex="-1"></a>\end{cases}</span>
<span id="cb23-241"><a href="#cb23-241" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb23-242"><a href="#cb23-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-243"><a href="#cb23-243" aria-hidden="true" tabindex="-1"></a>The **Yeo–Johnson transformation** (allows zero and negative values) is:</span>
<span id="cb23-244"><a href="#cb23-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-245"><a href="#cb23-245" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb23-246"><a href="#cb23-246" aria-hidden="true" tabindex="-1"></a>x_i^{(\lambda)} =</span>
<span id="cb23-247"><a href="#cb23-247" aria-hidden="true" tabindex="-1"></a>\begin{cases}</span>
<span id="cb23-248"><a href="#cb23-248" aria-hidden="true" tabindex="-1"></a>\frac{(x_i + 1)^{\lambda} - 1}{\lambda}, &amp; x_i \ge 0,\ \lambda \neq 0, <span class="sc">\\\\</span></span>
<span id="cb23-249"><a href="#cb23-249" aria-hidden="true" tabindex="-1"></a>\log(x_i + 1), &amp; x_i \ge 0,\ \lambda = 0, <span class="sc">\\\\</span></span>
<span id="cb23-250"><a href="#cb23-250" aria-hidden="true" tabindex="-1"></a>-\frac{(-x_i + 1)^{2 - \lambda} - 1}{2 - \lambda}, &amp; x_i &lt; 0,\ \lambda \neq 2, <span class="sc">\\\\</span></span>
<span id="cb23-251"><a href="#cb23-251" aria-hidden="true" tabindex="-1"></a>-\log(-x_i + 1), &amp; x_i &lt; 0,\ \lambda = 2.</span>
<span id="cb23-252"><a href="#cb23-252" aria-hidden="true" tabindex="-1"></a>\end{cases}</span>
<span id="cb23-253"><a href="#cb23-253" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb23-254"><a href="#cb23-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-255"><a href="#cb23-255" aria-hidden="true" tabindex="-1"></a>**Why combine with scaling?**\</span>
<span id="cb23-256"><a href="#cb23-256" aria-hidden="true" tabindex="-1"></a>Power transformations modify distributional shape but do not put variables on a common scale. After applying Box–Cox or Yeo–Johnson, variables are typically centered and scaled.</span>
<span id="cb23-257"><a href="#cb23-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-258"><a href="#cb23-258" aria-hidden="true" tabindex="-1"></a>**Order matters.**\</span>
<span id="cb23-259"><a href="#cb23-259" aria-hidden="true" tabindex="-1"></a>A practical default sequence is: **power transformation → centering → scaling**. Scaling before addressing skewness can weaken the effect of the transformation and complicate interpretation.</span>
<span id="cb23-260"><a href="#cb23-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-261"><a href="#cb23-261" aria-hidden="true" tabindex="-1"></a>**When to use.**\</span>
<span id="cb23-262"><a href="#cb23-262" aria-hidden="true" tabindex="-1"></a>When strong skewness or heteroscedasticity is present and when model assumptions or optimization benefit from more symmetric distributions.</span>
<span id="cb23-263"><a href="#cb23-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-264"><a href="#cb23-264" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb23-265"><a href="#cb23-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-266"><a href="#cb23-266" aria-hidden="true" tabindex="-1"></a><span class="fu">### Choosing a Method: No Single Best Answer</span></span>
<span id="cb23-267"><a href="#cb23-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-268"><a href="#cb23-268" aria-hidden="true" tabindex="-1"></a>There is no universally optimal normalization method. Each approach reflects a trade-off between robustness, interpretability, and sensitivity to data characteristics. The appropriate choice depends on the model, the data structure, and the modeling objective.</span>
<span id="cb23-269"><a href="#cb23-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-270"><a href="#cb23-270" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; The relevant question is not *"Which normalization method is best?"*\</span></span>
<span id="cb23-271"><a href="#cb23-271" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; but *"Which transformation aligns with my data and my model’s assumptions?"*</span></span>
<span id="cb23-272"><a href="#cb23-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-273"><a href="#cb23-273" aria-hidden="true" tabindex="-1"></a><span class="fu">## Do Different Data Types Require Different Scaling Strategies?</span></span>
<span id="cb23-274"><a href="#cb23-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-275"><a href="#cb23-275" aria-hidden="true" tabindex="-1"></a>Normalization decisions should never be made independently of data types. Different variable types carry different semantic meanings, and applying the same scaling strategy indiscriminately can lead to misleading representations or unnecessary transformations. A principled preprocessing workflow therefore begins by distinguishing between variable types and understanding how each interacts with scaling.</span>
<span id="cb23-276"><a href="#cb23-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-277"><a href="#cb23-277" aria-hidden="true" tabindex="-1"></a><span class="fu">### Continuous Numeric Variables</span></span>
<span id="cb23-278"><a href="#cb23-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-279"><a href="#cb23-279" aria-hidden="true" tabindex="-1"></a>Continuous numeric variables are the primary candidates for normalization. When such variables are measured on different scales—such as income in thousands and proportions between 0 and 1—scaling is often essential for models that rely on distances, gradients, or regularization. Z-score standardization, min–max scaling, or robust scaling are all reasonable options, depending on the presence of outliers and the modeling objective.</span>
<span id="cb23-280"><a href="#cb23-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-281"><a href="#cb23-281" aria-hidden="true" tabindex="-1"></a>In practice, most normalization methods are designed with continuous variables in mind, and applying them here rarely raises conceptual concerns. The main decision revolves around *which* scaling method is most appropriate, not *whether* scaling should be applied at all.</span>
<span id="cb23-282"><a href="#cb23-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-283"><a href="#cb23-283" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb23-284"><a href="#cb23-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-285"><a href="#cb23-285" aria-hidden="true" tabindex="-1"></a><span class="fu">### Count and Ordinal Numeric Variables</span></span>
<span id="cb23-286"><a href="#cb23-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-287"><a href="#cb23-287" aria-hidden="true" tabindex="-1"></a>Some numeric variables are technically continuous in storage but conceptually represent counts or ordered categories. Examples include the number of visits, rankings, Likert-scale responses, or discrete event counts. Treating such variables as purely continuous can be problematic, especially when their distributions are highly skewed or bounded at zero.</span>
<span id="cb23-288"><a href="#cb23-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-289"><a href="#cb23-289" aria-hidden="true" tabindex="-1"></a>In these cases, applying a logarithmic or power transformation before scaling is often more appropriate than direct normalization. Power transformations can reduce skewness and stabilize variance, after which standardization or robust scaling may be applied. The key point is that **the meaning of the variable matters**: a difference of one unit in a count variable does not necessarily carry the same interpretation across its range.</span>
<span id="cb23-290"><a href="#cb23-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-291"><a href="#cb23-291" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb23-292"><a href="#cb23-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-293"><a href="#cb23-293" aria-hidden="true" tabindex="-1"></a><span class="fu">### Categorical Variables (Factors or Characters)</span></span>
<span id="cb23-294"><a href="#cb23-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-295"><a href="#cb23-295" aria-hidden="true" tabindex="-1"></a>Categorical variables should **never** be scaled directly. Their values represent qualitative categories rather than numerical magnitudes, and applying normalization to raw category codes is meaningless.</span>
<span id="cb23-296"><a href="#cb23-296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-297"><a href="#cb23-297" aria-hidden="true" tabindex="-1"></a>When categorical variables are included in models that require numeric inputs, they must first be transformed using an encoding scheme such as one-hot (dummy) encoding. After encoding, the question of scaling arises again. In many cases, scaling encoded variables is unnecessary. However, in penalized regression models or distance-based methods, normalization of one-hot encoded variables may be beneficial to ensure that categorical and continuous predictors are treated on comparable scales.</span>
<span id="cb23-298"><a href="#cb23-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-299"><a href="#cb23-299" aria-hidden="true" tabindex="-1"></a>The important distinction is that scaling applies **after encoding**, not before, and only when the model’s assumptions justify it.</span>
<span id="cb23-300"><a href="#cb23-300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-301"><a href="#cb23-301" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb23-302"><a href="#cb23-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-303"><a href="#cb23-303" aria-hidden="true" tabindex="-1"></a><span class="fu">### Binary Variables (0/1 Indicators)</span></span>
<span id="cb23-304"><a href="#cb23-304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-305"><a href="#cb23-305" aria-hidden="true" tabindex="-1"></a>Binary variables occupy a special position. Since they already lie on a fixed and interpretable scale, normalization is usually unnecessary and may even obscure interpretation. For many models, leaving binary indicators unchanged is the most transparent choice.</span>
<span id="cb23-306"><a href="#cb23-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-307"><a href="#cb23-307" aria-hidden="true" tabindex="-1"></a>That said, binary variables often enter preprocessing pipelines automatically when a rule such as “scale all numeric predictors” is applied. In such cases, standardization will transform a 0/1 variable into values centered around zero with unit variance. While this does not usually harm model performance, it changes the interpretation of coefficients and can complicate downstream analysis.</span>
<span id="cb23-308"><a href="#cb23-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-309"><a href="#cb23-309" aria-hidden="true" tabindex="-1"></a>This highlights an important practical lesson: automated preprocessing pipelines should be used with care. Even when a transformation is mathematically valid, it may not be conceptually desirable for all variable types.</span>
<span id="cb23-310"><a href="#cb23-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-311"><a href="#cb23-311" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb23-312"><a href="#cb23-312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-313"><a href="#cb23-313" aria-hidden="true" tabindex="-1"></a><span class="fu">### Summary: Scaling Depends on Variable Meaning</span></span>
<span id="cb23-314"><a href="#cb23-314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-315"><a href="#cb23-315" aria-hidden="true" tabindex="-1"></a>The decision to normalize should always be guided by the *semantic role* of a variable, not merely by its storage type. Continuous measurements, counts, ordered responses, categorical indicators, and binary flags interact with scaling in fundamentally different ways. Effective preprocessing therefore requires more than applying a generic rule—it requires aligning transformations with the structure and meaning of the data.</span>
<span id="cb23-316"><a href="#cb23-316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-317"><a href="#cb23-317" aria-hidden="true" tabindex="-1"></a><span class="fu">## Should All Variables Be Scaled?</span></span>
<span id="cb23-318"><a href="#cb23-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-319"><a href="#cb23-319" aria-hidden="true" tabindex="-1"></a>A common mistake in preprocessing workflows is to treat normalization as a blanket operation applied to every variable in the dataset. In reality, **not all variables should be scaled**, and doing so indiscriminately can reduce interpretability or even introduce unintended distortions. Scaling decisions must therefore be made at the variable level, guided by both statistical and semantic considerations.</span>
<span id="cb23-320"><a href="#cb23-320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-321"><a href="#cb23-321" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Target Variable (y)</span></span>
<span id="cb23-322"><a href="#cb23-322" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-323"><a href="#cb23-323" aria-hidden="true" tabindex="-1"></a>In most predictive modeling tasks, the target variable should **not** be normalized. Scaling the response does not improve model estimation and often complicates interpretation, particularly in regression settings where coefficients and predictions are expected to be expressed in the original units.</span>
<span id="cb23-324"><a href="#cb23-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-325"><a href="#cb23-325" aria-hidden="true" tabindex="-1"></a>There are, however, notable exceptions. In neural network regression or other optimization-heavy models, scaling the target variable can improve numerical stability and convergence behavior. In such cases, predictions must be transformed back to the original scale before evaluation and interpretation. Outside these specific contexts, leaving the target variable unchanged remains the standard and preferred practice.</span>
<span id="cb23-326"><a href="#cb23-326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-327"><a href="#cb23-327" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb23-328"><a href="#cb23-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-329"><a href="#cb23-329" aria-hidden="true" tabindex="-1"></a><span class="fu">### Predictor Variables</span></span>
<span id="cb23-330"><a href="#cb23-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-331"><a href="#cb23-331" aria-hidden="true" tabindex="-1"></a>For predictor variables, scaling should be applied selectively rather than universally.</span>
<span id="cb23-332"><a href="#cb23-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-333"><a href="#cb23-333" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Numeric Predictors Only</span></span>
<span id="cb23-334"><a href="#cb23-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-335"><a href="#cb23-335" aria-hidden="true" tabindex="-1"></a>Normalization is meaningful only for numeric predictors. Applying scaling to non-numeric variables—either directly or implicitly through arbitrary numeric coding—has no conceptual justification. As discussed earlier, categorical variables must first be encoded, and even then, scaling is optional and model-dependent.</span>
<span id="cb23-336"><a href="#cb23-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-337"><a href="#cb23-337" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Excluding Non-informative Numeric Variables</span></span>
<span id="cb23-338"><a href="#cb23-338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-339"><a href="#cb23-339" aria-hidden="true" tabindex="-1"></a>Not all numeric variables carry meaningful quantitative information. Identifier variables such as IDs, account numbers, or arbitrary codes may be stored as numeric values but do not represent magnitudes or distances. Scaling such variables is meaningless and potentially harmful, as it introduces artificial structure where none exists. These variables should be excluded from the modeling process altogether, not merely from scaling.</span>
<span id="cb23-340"><a href="#cb23-340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-341"><a href="#cb23-341" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Handling Low-Variance Predictors</span></span>
<span id="cb23-342"><a href="#cb23-342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-343"><a href="#cb23-343" aria-hidden="true" tabindex="-1"></a>Variables with extremely low or zero variance provide little to no information for modeling. Scaling such predictors does not solve the underlying problem; it merely rescales noise. In practice, low-variance and zero-variance predictors should be identified and removed **before** normalization.</span>
<span id="cb23-344"><a href="#cb23-344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-345"><a href="#cb23-345" aria-hidden="true" tabindex="-1"></a>Many preprocessing frameworks formalize this step. For example, approaches based on the logic of zero-variance or near-zero-variance filtering (often referred to as <span class="in">`zv`</span> or <span class="in">`nzv`</span> steps) ensure that only informative predictors enter the scaling stage. This not only improves computational efficiency but also reduces the risk of numerical instability in downstream models.</span>
<span id="cb23-346"><a href="#cb23-346" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-347"><a href="#cb23-347" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb23-348"><a href="#cb23-348" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-349"><a href="#cb23-349" aria-hidden="true" tabindex="-1"></a><span class="fu">### A Practical Rule of Thumb</span></span>
<span id="cb23-350"><a href="#cb23-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-351"><a href="#cb23-351" aria-hidden="true" tabindex="-1"></a>A disciplined preprocessing workflow follows a clear sequence:</span>
<span id="cb23-352"><a href="#cb23-352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-353"><a href="#cb23-353" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>Identify and remove non-informative variables (IDs, constants, near-constants).</span>
<span id="cb23-354"><a href="#cb23-354" aria-hidden="true" tabindex="-1"></a><span class="ss">2.  </span>Select numeric predictors that represent meaningful quantities.</span>
<span id="cb23-355"><a href="#cb23-355" aria-hidden="true" tabindex="-1"></a><span class="ss">3.  </span>Apply appropriate scaling only to this subset.</span>
<span id="cb23-356"><a href="#cb23-356" aria-hidden="true" tabindex="-1"></a><span class="ss">4.  </span>Leave the target variable unscaled, unless there is a compelling model-specific reason to do otherwise.</span>
<span id="cb23-357"><a href="#cb23-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-358"><a href="#cb23-358" aria-hidden="true" tabindex="-1"></a>Scaling is most effective when it is **deliberate and selective**, not automatic. Treating normalization as a universal operation may simplify code, but it rarely leads to better models.</span>
<span id="cb23-359"><a href="#cb23-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-360"><a href="#cb23-360" aria-hidden="true" tabindex="-1"></a><span class="fu">## Application Plan in R: Data and Modeling Scenario</span></span>
<span id="cb23-361"><a href="#cb23-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-362"><a href="#cb23-362" aria-hidden="true" tabindex="-1"></a>To demonstrate the practical implications of normalization decisions, we use the **Ames Housing** dataset, a well-known benchmark dataset designed for predictive modeling. The dataset contains **2,930 observations** and a rich set of predictors describing residential properties in Ames, Iowa. These predictors span multiple data types, including continuous numeric variables, discrete counts, ordinal ratings, and categorical features. This diversity makes the dataset particularly suitable for illustrating how scaling interacts with different variable types.</span>
<span id="cb23-363"><a href="#cb23-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-364"><a href="#cb23-364" aria-hidden="true" tabindex="-1"></a>The Ames Housing dataset is distributed within the **modeldata** package in the tidymodels ecosystem. It was explicitly curated for teaching and methodological demonstrations, ensuring a realistic but well-documented structure. The presence of variables measured on vastly different scales—such as living area, lot size, and quality scores—provides a natural setting for exploring the effects of normalization.</span>
<span id="cb23-365"><a href="#cb23-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-366"><a href="#cb23-366" aria-hidden="true" tabindex="-1"></a><span class="fu">### Modeling Objective</span></span>
<span id="cb23-367"><a href="#cb23-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-368"><a href="#cb23-368" aria-hidden="true" tabindex="-1"></a>The primary goal of this application is **not** to optimize predictive performance, but to isolate and examine the impact of different normalization strategies. For this reason, the modeling task is intentionally kept simple. We focus on predicting the **sale price of a house** as a regression problem, using a fixed model specification across all experiments.</span>
<span id="cb23-369"><a href="#cb23-369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-370"><a href="#cb23-370" aria-hidden="true" tabindex="-1"></a>The model itself serves merely as a vehicle for comparison. By holding the model constant and varying only the preprocessing strategy, we can attribute differences in performance and behavior directly to scaling decisions rather than to model complexity or tuning choices.</span>
<span id="cb23-371"><a href="#cb23-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-372"><a href="#cb23-372" aria-hidden="true" tabindex="-1"></a><span class="fu">### Scope and Focus</span></span>
<span id="cb23-373"><a href="#cb23-373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-374"><a href="#cb23-374" aria-hidden="true" tabindex="-1"></a>Throughout the application section, the emphasis remains firmly on preprocessing:</span>
<span id="cb23-375"><a href="#cb23-375" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-376"><a href="#cb23-376" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>the same training–test split is used across all scenarios,</span>
<span id="cb23-377"><a href="#cb23-377" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>the same set of predictors is retained,</span>
<span id="cb23-378"><a href="#cb23-378" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>the same model structure is applied.</span>
<span id="cb23-379"><a href="#cb23-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-380"><a href="#cb23-380" aria-hidden="true" tabindex="-1"></a>Only the normalization strategy changes. This design allows us to answer a focused question:</span>
<span id="cb23-381"><a href="#cb23-381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-382"><a href="#cb23-382" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; *How much do scaling choices matter when everything else is kept equal?*</span></span>
<span id="cb23-383"><a href="#cb23-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-384"><a href="#cb23-384" aria-hidden="true" tabindex="-1"></a>By structuring the analysis in this way, the results highlight normalization as an integral component of the modeling pipeline rather than a secondary technical detail.</span>
<span id="cb23-385"><a href="#cb23-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-386"><a href="#cb23-386" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb23-387"><a href="#cb23-387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-388"><a href="#cb23-388" aria-hidden="true" tabindex="-1"></a><span class="fu">### Transition to Implementation</span></span>
<span id="cb23-389"><a href="#cb23-389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-390"><a href="#cb23-390" aria-hidden="true" tabindex="-1"></a>In the next section, we move from design to execution. We begin by defining a train–test split and establishing a baseline preprocessing workflow. From there, we introduce alternative normalization strategies and compare their effects using consistent evaluation criteria.</span>
<span id="cb23-391"><a href="#cb23-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-392"><a href="#cb23-392" aria-hidden="true" tabindex="-1"></a><span class="fu">### Data Access and Availability</span></span>
<span id="cb23-393"><a href="#cb23-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-394"><a href="#cb23-394" aria-hidden="true" tabindex="-1"></a>The Ames Housing dataset used in this application is available through the **modeldata** package, which is part of the tidymodels ecosystem. No external download is required. Once the package is installed, the dataset can be accessed directly within R.</span>
<span id="cb23-395"><a href="#cb23-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-396"><a href="#cb23-396" aria-hidden="true" tabindex="-1"></a>The dataset is provided for educational and methodological purposes and is accompanied by detailed documentation. For reference, the official description is available at:</span>
<span id="cb23-397"><a href="#cb23-397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-398"><a href="#cb23-398" aria-hidden="true" tabindex="-1"></a><span class="ot">&lt;https://modeldata.tidymodels.org/reference/ames.html&gt;</span></span>
<span id="cb23-399"><a href="#cb23-399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-400"><a href="#cb23-400" aria-hidden="true" tabindex="-1"></a>In the next section, we load the dataset directly from the package and proceed with the train–test split and preprocessing workflow.</span>
<span id="cb23-401"><a href="#cb23-401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-402"><a href="#cb23-402" aria-hidden="true" tabindex="-1"></a><span class="fu">## Implementation in R: Split, Baseline, and the Cost of Doing It Wrong</span></span>
<span id="cb23-403"><a href="#cb23-403" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-404"><a href="#cb23-404" aria-hidden="true" tabindex="-1"></a>In this section, we operationalize the key principle introduced earlier:</span>
<span id="cb23-405"><a href="#cb23-405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-406"><a href="#cb23-406" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **Split → fit preprocessing on train → apply to train/test**</span></span>
<span id="cb23-407"><a href="#cb23-407" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-408"><a href="#cb23-408" aria-hidden="true" tabindex="-1"></a>We use the Ames Housing dataset from the <span class="in">`modeldata`</span> package (no external download required) and compare three pipelines using the **same model**:</span>
<span id="cb23-409"><a href="#cb23-409" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-410"><a href="#cb23-410" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>**Baseline (no scaling)**</span>
<span id="cb23-411"><a href="#cb23-411" aria-hidden="true" tabindex="-1"></a><span class="ss">2.  </span>**Incorrect scaling (data leakage)**: scaling parameters learned from the full dataset</span>
<span id="cb23-412"><a href="#cb23-412" aria-hidden="true" tabindex="-1"></a><span class="ss">3.  </span>**Correct scaling**: scaling parameters learned from the training set only</span>
<span id="cb23-413"><a href="#cb23-413" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-414"><a href="#cb23-414" aria-hidden="true" tabindex="-1"></a>The goal is not to build the best possible model but to **isolate the effect of scaling decisions**.</span>
<span id="cb23-415"><a href="#cb23-415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-416"><a href="#cb23-416" aria-hidden="true" tabindex="-1"></a><span class="fu">### Setup and Variable Selection</span></span>
<span id="cb23-417"><a href="#cb23-417" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-418"><a href="#cb23-418" aria-hidden="true" tabindex="-1"></a>Before defining any model, we clarify what we are modeling and why these variables are used.</span>
<span id="cb23-419"><a href="#cb23-419" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-420"><a href="#cb23-420" aria-hidden="true" tabindex="-1"></a>**Modeling goal.**\</span>
<span id="cb23-421"><a href="#cb23-421" aria-hidden="true" tabindex="-1"></a>We treat <span class="in">`Sale_Price`</span> as the target variable and build a regression model that predicts house sale prices based on a small set of numeric predictors. The purpose is not to maximize predictive accuracy, but to create a controlled environment where the effect of scaling choices is easy to observe.</span>
<span id="cb23-422"><a href="#cb23-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-423"><a href="#cb23-423" aria-hidden="true" tabindex="-1"></a>**Why a small subset of predictors?**\</span>
<span id="cb23-424"><a href="#cb23-424" aria-hidden="true" tabindex="-1"></a>The Ames dataset contains many variables, including categorical and ordinal predictors. For the normalization demonstrations, we intentionally select a compact set of **numeric** features with clearly different measurement scales. This makes the consequences of scaling (and data leakage) more visible and easier to interpret.</span>
<span id="cb23-425"><a href="#cb23-425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-426"><a href="#cb23-426" aria-hidden="true" tabindex="-1"></a>**Selected variables (interpretation).**</span>
<span id="cb23-427"><a href="#cb23-427" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-428"><a href="#cb23-428" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span><span class="in">`Sale_Price`</span>: sale price of the house (response variable).</span>
<span id="cb23-429"><a href="#cb23-429" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-430"><a href="#cb23-430" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span><span class="in">`Gr_Liv_Area`</span>: above-ground living area (a size-related continuous measure).</span>
<span id="cb23-431"><a href="#cb23-431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-432"><a href="#cb23-432" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span><span class="in">`Lot_Area`</span>: lot size (typically much larger numeric range than living area).</span>
<span id="cb23-433"><a href="#cb23-433" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-434"><a href="#cb23-434" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span><span class="in">`Year_Built`</span>: construction year (a temporal numeric variable).</span>
<span id="cb23-435"><a href="#cb23-435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-436"><a href="#cb23-436" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span><span class="in">`Overall_Cond`</span>: overall condition rating (an ordinal-like numeric score).</span>
<span id="cb23-437"><a href="#cb23-437" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-438"><a href="#cb23-438" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span><span class="in">`Latitude`</span>, <span class="in">`Longitude`</span>: geographic coordinates capturing location effects.</span>
<span id="cb23-439"><a href="#cb23-439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-440"><a href="#cb23-440" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb23-441"><a href="#cb23-441" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-442"><a href="#cb23-442" aria-hidden="true" tabindex="-1"></a><span class="fu">### Load Data and Create a Working Dataset</span></span>
<span id="cb23-443"><a href="#cb23-443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-446"><a href="#cb23-446" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb23-447"><a href="#cb23-447" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidymodels)</span>
<span id="cb23-448"><a href="#cb23-448" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(modeldata)</span>
<span id="cb23-449"><a href="#cb23-449" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-450"><a href="#cb23-450" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(ames, <span class="at">package =</span> <span class="st">"modeldata"</span>)</span>
<span id="cb23-451"><a href="#cb23-451" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-452"><a href="#cb23-452" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2026</span>)</span>
<span id="cb23-453"><a href="#cb23-453" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-454"><a href="#cb23-454" aria-hidden="true" tabindex="-1"></a>ames_small <span class="ot">&lt;-</span> ames <span class="sc">%&gt;%</span></span>
<span id="cb23-455"><a href="#cb23-455" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">select</span>(</span>
<span id="cb23-456"><a href="#cb23-456" aria-hidden="true" tabindex="-1"></a>    Sale_Price,</span>
<span id="cb23-457"><a href="#cb23-457" aria-hidden="true" tabindex="-1"></a>    Gr_Liv_Area,</span>
<span id="cb23-458"><a href="#cb23-458" aria-hidden="true" tabindex="-1"></a>    Lot_Area,</span>
<span id="cb23-459"><a href="#cb23-459" aria-hidden="true" tabindex="-1"></a>    Year_Built,</span>
<span id="cb23-460"><a href="#cb23-460" aria-hidden="true" tabindex="-1"></a>    Overall_Cond,</span>
<span id="cb23-461"><a href="#cb23-461" aria-hidden="true" tabindex="-1"></a>    Latitude,</span>
<span id="cb23-462"><a href="#cb23-462" aria-hidden="true" tabindex="-1"></a>    Longitude</span>
<span id="cb23-463"><a href="#cb23-463" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb23-464"><a href="#cb23-464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-465"><a href="#cb23-465" aria-hidden="true" tabindex="-1"></a><span class="co"># Missing-value check within the selected columns</span></span>
<span id="cb23-466"><a href="#cb23-466" aria-hidden="true" tabindex="-1"></a>ames_small <span class="sc">%&gt;%</span></span>
<span id="cb23-467"><a href="#cb23-467" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(<span class="fu">across</span>(<span class="fu">everything</span>(), <span class="sc">~</span> <span class="fu">sum</span>(<span class="fu">is.na</span>(.)))) <span class="sc">%&gt;%</span></span>
<span id="cb23-468"><a href="#cb23-468" aria-hidden="true" tabindex="-1"></a>  tidyr<span class="sc">::</span><span class="fu">pivot_longer</span>(<span class="fu">everything</span>(), <span class="at">names_to =</span> <span class="st">"variable"</span>, <span class="at">values_to =</span> <span class="st">"n_missing"</span>)</span>
<span id="cb23-469"><a href="#cb23-469" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-470"><a href="#cb23-470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-471"><a href="#cb23-471" aria-hidden="true" tabindex="-1"></a>This step constructs a clean working dataset (<span class="in">`ames_small`</span>) and confirms whether missing values exist in the selected columns. For the comparisons in the next sections, it is important that the pipelines differ only by preprocessing choices (e.g., scaling), not by inconsistent handling of missing data.</span>
<span id="cb23-472"><a href="#cb23-472" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-473"><a href="#cb23-473" aria-hidden="true" tabindex="-1"></a><span class="fu">### Train–Test Split and Evaluation Setup</span></span>
<span id="cb23-474"><a href="#cb23-474" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-475"><a href="#cb23-475" aria-hidden="true" tabindex="-1"></a>Before discussing scaling, we must establish a clean evaluation setup. The key idea is simple:</span>
<span id="cb23-476"><a href="#cb23-476" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-477"><a href="#cb23-477" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **Split first. Then learn any preprocessing parameters from the training set only.**</span></span>
<span id="cb23-478"><a href="#cb23-478" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-479"><a href="#cb23-479" aria-hidden="true" tabindex="-1"></a>Without a proper train–test split, we cannot meaningfully talk about generalization, and any comparison involving normalization risks becoming misleading.</span>
<span id="cb23-480"><a href="#cb23-480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-481"><a href="#cb23-481" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb23-482"><a href="#cb23-482" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-483"><a href="#cb23-483" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Create a Stratified Train–Test Split</span></span>
<span id="cb23-484"><a href="#cb23-484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-487"><a href="#cb23-487" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb23-488"><a href="#cb23-488" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2026</span>)</span>
<span id="cb23-489"><a href="#cb23-489" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-490"><a href="#cb23-490" aria-hidden="true" tabindex="-1"></a>split_obj <span class="ot">&lt;-</span> <span class="fu">initial_split</span>(ames_small, <span class="at">prop =</span> <span class="fl">0.80</span>, <span class="at">strata =</span> Sale_Price)</span>
<span id="cb23-491"><a href="#cb23-491" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-492"><a href="#cb23-492" aria-hidden="true" tabindex="-1"></a>train_data <span class="ot">&lt;-</span> <span class="fu">training</span>(split_obj)</span>
<span id="cb23-493"><a href="#cb23-493" aria-hidden="true" tabindex="-1"></a>test_data  <span class="ot">&lt;-</span> <span class="fu">testing</span>(split_obj)</span>
<span id="cb23-494"><a href="#cb23-494" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-495"><a href="#cb23-495" aria-hidden="true" tabindex="-1"></a><span class="fu">nrow</span>(train_data)</span>
<span id="cb23-496"><a href="#cb23-496" aria-hidden="true" tabindex="-1"></a><span class="fu">nrow</span>(test_data)</span>
<span id="cb23-497"><a href="#cb23-497" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-498"><a href="#cb23-498" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-499"><a href="#cb23-499" aria-hidden="true" tabindex="-1"></a>**What this does.**</span>
<span id="cb23-500"><a href="#cb23-500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-501"><a href="#cb23-501" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span><span class="in">`prop = 0.80`</span> assigns roughly 80% of the data to training and 20% to testing.</span>
<span id="cb23-502"><a href="#cb23-502" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-503"><a href="#cb23-503" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span><span class="in">`strata = Sale_Price`</span> performs a *stratified* split based on the target variable.\</span>
<span id="cb23-504"><a href="#cb23-504" aria-hidden="true" tabindex="-1"></a>    This reduces the risk that the test set ends up with an atypical concentration of very low or very high prices—something that can easily happen with skewed targets like house prices.</span>
<span id="cb23-505"><a href="#cb23-505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-506"><a href="#cb23-506" aria-hidden="true" tabindex="-1"></a>**How to interpret the output.**</span>
<span id="cb23-507"><a href="#cb23-507" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-508"><a href="#cb23-508" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>If the full dataset contains 2,930 observations, you should see approximately:</span>
<span id="cb23-509"><a href="#cb23-509" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-510"><a href="#cb23-510" aria-hidden="true" tabindex="-1"></a><span class="in">```         </span></span>
<span id="cb23-511"><a href="#cb23-511" aria-hidden="true" tabindex="-1"></a><span class="in">-    training: 2,342 rows</span></span>
<span id="cb23-512"><a href="#cb23-512" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-513"><a href="#cb23-513" aria-hidden="true" tabindex="-1"></a><span class="in">-    test: 588 rows</span></span>
<span id="cb23-514"><a href="#cb23-514" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-515"><a href="#cb23-515" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-516"><a href="#cb23-516" aria-hidden="true" tabindex="-1"></a>This corresponds closely to the intended 80/20 split and indicates that no unintended row loss occurred during preprocessing.</span>
<span id="cb23-517"><a href="#cb23-517" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-518"><a href="#cb23-518" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Sanity Check: Is the Target Distribution Similar Across Splits?</span></span>
<span id="cb23-519"><a href="#cb23-519" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-522"><a href="#cb23-522" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb23-523"><a href="#cb23-523" aria-hidden="true" tabindex="-1"></a><span class="fu">bind_rows</span>(</span>
<span id="cb23-524"><a href="#cb23-524" aria-hidden="true" tabindex="-1"></a>  train_data <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">split =</span> <span class="st">"train"</span>),</span>
<span id="cb23-525"><a href="#cb23-525" aria-hidden="true" tabindex="-1"></a>  test_data  <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">split =</span> <span class="st">"test"</span>)</span>
<span id="cb23-526"><a href="#cb23-526" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span></span>
<span id="cb23-527"><a href="#cb23-527" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> Sale_Price, <span class="at">fill =</span> split)) <span class="sc">+</span></span>
<span id="cb23-528"><a href="#cb23-528" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="at">bins =</span> <span class="dv">40</span>, <span class="at">alpha =</span> <span class="fl">0.7</span>, <span class="at">color =</span> <span class="st">"white"</span>) <span class="sc">+</span></span>
<span id="cb23-529"><a href="#cb23-529" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span> split, <span class="at">scales =</span> <span class="st">"free_y"</span>) <span class="sc">+</span></span>
<span id="cb23-530"><a href="#cb23-530" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_fill_manual</span>(</span>
<span id="cb23-531"><a href="#cb23-531" aria-hidden="true" tabindex="-1"></a>    <span class="at">values =</span> <span class="fu">c</span>(<span class="at">train =</span> <span class="st">"#1f77b4"</span>, <span class="at">test =</span> <span class="st">"#ff7f0e"</span>)</span>
<span id="cb23-532"><a href="#cb23-532" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb23-533"><a href="#cb23-533" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb23-534"><a href="#cb23-534" aria-hidden="true" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">"Sale_Price distribution after train–test split"</span>,</span>
<span id="cb23-535"><a href="#cb23-535" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">"Sale_Price"</span>,</span>
<span id="cb23-536"><a href="#cb23-536" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">"Count"</span>,</span>
<span id="cb23-537"><a href="#cb23-537" aria-hidden="true" tabindex="-1"></a>    <span class="at">fill =</span> <span class="st">"Data split"</span></span>
<span id="cb23-538"><a href="#cb23-538" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb23-539"><a href="#cb23-539" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span>
<span id="cb23-540"><a href="#cb23-540" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-541"><a href="#cb23-541" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-542"><a href="#cb23-542" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-543"><a href="#cb23-543" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-544"><a href="#cb23-544" aria-hidden="true" tabindex="-1"></a>**What to look for.**</span>
<span id="cb23-545"><a href="#cb23-545" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-546"><a href="#cb23-546" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Both distributions should be right-skewed with a similar central mass.</span>
<span id="cb23-547"><a href="#cb23-547" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-548"><a href="#cb23-548" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>There should be no strong imbalance where most expensive (or cheapest) homes appear in only one split.</span>
<span id="cb23-549"><a href="#cb23-549" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-550"><a href="#cb23-550" aria-hidden="true" tabindex="-1"></a>In the plot, the overall shapes are highly similar and the mid-range is well represented in both sets, indicating that stratification preserved the structure of the target variable across splits.</span>
<span id="cb23-551"><a href="#cb23-551" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-552"><a href="#cb23-552" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Optional Check: Quick Summary Statistics</span></span>
<span id="cb23-553"><a href="#cb23-553" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-554"><a href="#cb23-554" aria-hidden="true" tabindex="-1"></a>This is a compact numerical confirmation of what the plot shows.</span>
<span id="cb23-555"><a href="#cb23-555" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-558"><a href="#cb23-558" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb23-559"><a href="#cb23-559" aria-hidden="true" tabindex="-1"></a>train_summary <span class="ot">&lt;-</span> train_data <span class="sc">%&gt;%</span></span>
<span id="cb23-560"><a href="#cb23-560" aria-hidden="true" tabindex="-1"></a><span class="fu">summarise</span>(</span>
<span id="cb23-561"><a href="#cb23-561" aria-hidden="true" tabindex="-1"></a><span class="at">split =</span> <span class="st">"train"</span>,</span>
<span id="cb23-562"><a href="#cb23-562" aria-hidden="true" tabindex="-1"></a><span class="at">n =</span> <span class="fu">n</span>(),</span>
<span id="cb23-563"><a href="#cb23-563" aria-hidden="true" tabindex="-1"></a><span class="at">mean =</span> <span class="fu">mean</span>(Sale_Price),</span>
<span id="cb23-564"><a href="#cb23-564" aria-hidden="true" tabindex="-1"></a><span class="at">median =</span> <span class="fu">median</span>(Sale_Price),</span>
<span id="cb23-565"><a href="#cb23-565" aria-hidden="true" tabindex="-1"></a><span class="at">sd =</span> <span class="fu">sd</span>(Sale_Price),</span>
<span id="cb23-566"><a href="#cb23-566" aria-hidden="true" tabindex="-1"></a><span class="at">min =</span> <span class="fu">min</span>(Sale_Price),</span>
<span id="cb23-567"><a href="#cb23-567" aria-hidden="true" tabindex="-1"></a><span class="at">max =</span> <span class="fu">max</span>(Sale_Price)</span>
<span id="cb23-568"><a href="#cb23-568" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb23-569"><a href="#cb23-569" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-570"><a href="#cb23-570" aria-hidden="true" tabindex="-1"></a>test_summary <span class="ot">&lt;-</span> test_data <span class="sc">%&gt;%</span></span>
<span id="cb23-571"><a href="#cb23-571" aria-hidden="true" tabindex="-1"></a><span class="fu">summarise</span>(</span>
<span id="cb23-572"><a href="#cb23-572" aria-hidden="true" tabindex="-1"></a><span class="at">split =</span> <span class="st">"test"</span>,</span>
<span id="cb23-573"><a href="#cb23-573" aria-hidden="true" tabindex="-1"></a><span class="at">n =</span> <span class="fu">n</span>(),</span>
<span id="cb23-574"><a href="#cb23-574" aria-hidden="true" tabindex="-1"></a><span class="at">mean =</span> <span class="fu">mean</span>(Sale_Price),</span>
<span id="cb23-575"><a href="#cb23-575" aria-hidden="true" tabindex="-1"></a><span class="at">median =</span> <span class="fu">median</span>(Sale_Price),</span>
<span id="cb23-576"><a href="#cb23-576" aria-hidden="true" tabindex="-1"></a><span class="at">sd =</span> <span class="fu">sd</span>(Sale_Price),</span>
<span id="cb23-577"><a href="#cb23-577" aria-hidden="true" tabindex="-1"></a><span class="at">min =</span> <span class="fu">min</span>(Sale_Price),</span>
<span id="cb23-578"><a href="#cb23-578" aria-hidden="true" tabindex="-1"></a><span class="at">max =</span> <span class="fu">max</span>(Sale_Price)</span>
<span id="cb23-579"><a href="#cb23-579" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb23-580"><a href="#cb23-580" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-581"><a href="#cb23-581" aria-hidden="true" tabindex="-1"></a><span class="fu">bind_rows</span>(train_summary, test_summary)</span>
<span id="cb23-582"><a href="#cb23-582" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-583"><a href="#cb23-583" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-584"><a href="#cb23-584" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-585"><a href="#cb23-585" aria-hidden="true" tabindex="-1"></a>**How to interpret this.**</span>
<span id="cb23-586"><a href="#cb23-586" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-587"><a href="#cb23-587" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Small differences between train and test are expected.</span>
<span id="cb23-588"><a href="#cb23-588" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-589"><a href="#cb23-589" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Large gaps—especially in the median—may indicate an unbalanced split.</span>
<span id="cb23-590"><a href="#cb23-590" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-591"><a href="#cb23-591" aria-hidden="true" tabindex="-1"></a>Your summaries show nearly identical means and medians (train: 180,447 / 160,000; test: 182,185 / 160,500) and similar standard deviations, supporting the conclusion that the split is well balanced. Differences in the maximum values are expected due to rare high-priced homes and do not indicate a problematic split.</span>
<span id="cb23-592"><a href="#cb23-592" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-593"><a href="#cb23-593" aria-hidden="true" tabindex="-1"></a>The train–test split is well balanced and suitable for downstream modeling. The test set can be treated as a genuine proxy for unseen data, allowing us to evaluate normalization strategies without confounding effects from an unbalanced split.</span>
<span id="cb23-594"><a href="#cb23-594" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-595"><a href="#cb23-595" aria-hidden="true" tabindex="-1"></a><span class="fu">### Model Specification: A Scale-Sensitive Baseline</span></span>
<span id="cb23-596"><a href="#cb23-596" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-597"><a href="#cb23-597" aria-hidden="true" tabindex="-1"></a>Before comparing different normalization strategies, we must fix the modeling component of the pipeline. This ensures that any performance differences observed later can be attributed to preprocessing choices rather than to changes in the model itself.</span>
<span id="cb23-598"><a href="#cb23-598" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-599"><a href="#cb23-599" aria-hidden="true" tabindex="-1"></a>**Why KNN Regression?**</span>
<span id="cb23-600"><a href="#cb23-600" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-601"><a href="#cb23-601" aria-hidden="true" tabindex="-1"></a>We deliberately choose **k-nearest neighbors (KNN) regression** for this demonstration. The reason is methodological, not practical.</span>
<span id="cb23-602"><a href="#cb23-602" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-603"><a href="#cb23-603" aria-hidden="true" tabindex="-1"></a>KNN is a **distance-based algorithm**: predictions are determined by the distances between observations in the feature space. As a result, KNN is highly sensitive to the scale of the predictors. Variables with larger numeric ranges can dominate distance calculations, even if they are not substantively more important.</span>
<span id="cb23-604"><a href="#cb23-604" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-605"><a href="#cb23-605" aria-hidden="true" tabindex="-1"></a>This property makes KNN an ideal diagnostic tool for studying the effects of scaling.</span>
<span id="cb23-606"><a href="#cb23-606" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-607"><a href="#cb23-607" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb23-608"><a href="#cb23-608" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-609"><a href="#cb23-609" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Model Specification</span></span>
<span id="cb23-610"><a href="#cb23-610" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-611"><a href="#cb23-611" aria-hidden="true" tabindex="-1"></a>We define a single KNN model that will be used in all subsequent scenarios.</span>
<span id="cb23-612"><a href="#cb23-612" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-615"><a href="#cb23-615" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb23-616"><a href="#cb23-616" aria-hidden="true" tabindex="-1"></a>knn_spec <span class="ot">&lt;-</span> <span class="fu">nearest_neighbor</span>(</span>
<span id="cb23-617"><a href="#cb23-617" aria-hidden="true" tabindex="-1"></a>  <span class="at">neighbors =</span> <span class="dv">15</span>,</span>
<span id="cb23-618"><a href="#cb23-618" aria-hidden="true" tabindex="-1"></a>  <span class="at">weight_func =</span> <span class="st">"rectangular"</span></span>
<span id="cb23-619"><a href="#cb23-619" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span></span>
<span id="cb23-620"><a href="#cb23-620" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">"kknn"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb23-621"><a href="#cb23-621" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_mode</span>(<span class="st">"regression"</span>)</span>
<span id="cb23-622"><a href="#cb23-622" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-623"><a href="#cb23-623" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-624"><a href="#cb23-624" aria-hidden="true" tabindex="-1"></a>**Commentary.**</span>
<span id="cb23-625"><a href="#cb23-625" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-626"><a href="#cb23-626" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>The number of neighbors is fixed at 15 to reduce variance while maintaining locality.</span>
<span id="cb23-627"><a href="#cb23-627" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-628"><a href="#cb23-628" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>No hyperparameter tuning is performed, as optimization is not the goal here.</span>
<span id="cb23-629"><a href="#cb23-629" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-630"><a href="#cb23-630" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>This model specification will remain unchanged across all preprocessing pipelines.</span>
<span id="cb23-631"><a href="#cb23-631" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-632"><a href="#cb23-632" aria-hidden="true" tabindex="-1"></a><span class="fu">### Scenario A — Baseline: No Scaling</span></span>
<span id="cb23-633"><a href="#cb23-633" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-634"><a href="#cb23-634" aria-hidden="true" tabindex="-1"></a>We begin with a baseline workflow in which **no scaling is applied**. This provides a reference point against which all normalized pipelines will be compared.</span>
<span id="cb23-635"><a href="#cb23-635" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-638"><a href="#cb23-638" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb23-639"><a href="#cb23-639" aria-hidden="true" tabindex="-1"></a>rec_none <span class="ot">&lt;-</span> <span class="fu">recipe</span>(Sale_Price <span class="sc">~</span> ., <span class="at">data =</span> train_data)</span>
<span id="cb23-640"><a href="#cb23-640" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-641"><a href="#cb23-641" aria-hidden="true" tabindex="-1"></a>wf_none <span class="ot">&lt;-</span> <span class="fu">workflow</span>() <span class="sc">%&gt;%</span></span>
<span id="cb23-642"><a href="#cb23-642" aria-hidden="true" tabindex="-1"></a><span class="fu">add_recipe</span>(rec_none) <span class="sc">%&gt;%</span></span>
<span id="cb23-643"><a href="#cb23-643" aria-hidden="true" tabindex="-1"></a><span class="fu">add_model</span>(knn_spec)</span>
<span id="cb23-644"><a href="#cb23-644" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-645"><a href="#cb23-645" aria-hidden="true" tabindex="-1"></a>fit_none <span class="ot">&lt;-</span> <span class="fu">fit</span>(wf_none, <span class="at">data =</span> train_data)</span>
<span id="cb23-646"><a href="#cb23-646" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-647"><a href="#cb23-647" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-648"><a href="#cb23-648" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-649"><a href="#cb23-649" aria-hidden="true" tabindex="-1"></a>::: callout-note</span>
<span id="cb23-650"><a href="#cb23-650" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **Note on model engines.**\</span></span>
<span id="cb23-651"><a href="#cb23-651" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; In the tidymodels ecosystem, model specifications are defined independently of the underlying computational engines. Although we specify the KNN model via </span><span class="in">`nearest_neighbor()`</span><span class="at">, the actual implementation is provided by the </span><span class="in">`kknn`</span><span class="at"> package.</span></span>
<span id="cb23-652"><a href="#cb23-652" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb23-653"><a href="#cb23-653" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; If the package is not installed, fitting the model will fail. To proceed, install and load the required engine:</span></span>
<span id="cb23-654"><a href="#cb23-654" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb23-655"><a href="#cb23-655" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; ``` r</span></span>
<span id="cb23-656"><a href="#cb23-656" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; install.packages("kknn")</span></span>
<span id="cb23-657"><a href="#cb23-657" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; library(kknn)</span></span>
<span id="cb23-658"><a href="#cb23-658" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; ```</span></span>
<span id="cb23-659"><a href="#cb23-659" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb23-660"><a href="#cb23-660" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; This separation between model specification and engine implementation is intentional and allows tidymodels to remain modular and extensible.</span></span>
<span id="cb23-661"><a href="#cb23-661" aria-hidden="true" tabindex="-1"></a><span class="at">:::</span></span>
<span id="cb23-662"><a href="#cb23-662" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-663"><a href="#cb23-663" aria-hidden="true" tabindex="-1"></a><span class="fu">#### **Evaluate on the Test Set**</span></span>
<span id="cb23-664"><a href="#cb23-664" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-667"><a href="#cb23-667" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb23-668"><a href="#cb23-668" aria-hidden="true" tabindex="-1"></a>pred_none <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit_none, test_data) <span class="sc">%&gt;%</span></span>
<span id="cb23-669"><a href="#cb23-669" aria-hidden="true" tabindex="-1"></a><span class="fu">bind_cols</span>(test_data <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">select</span>(Sale_Price))</span>
<span id="cb23-670"><a href="#cb23-670" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-671"><a href="#cb23-671" aria-hidden="true" tabindex="-1"></a>metrics_none <span class="ot">&lt;-</span> yardstick<span class="sc">::</span><span class="fu">metrics</span>(</span>
<span id="cb23-672"><a href="#cb23-672" aria-hidden="true" tabindex="-1"></a>pred_none,</span>
<span id="cb23-673"><a href="#cb23-673" aria-hidden="true" tabindex="-1"></a><span class="at">truth =</span> Sale_Price,</span>
<span id="cb23-674"><a href="#cb23-674" aria-hidden="true" tabindex="-1"></a><span class="at">estimate =</span> .pred</span>
<span id="cb23-675"><a href="#cb23-675" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb23-676"><a href="#cb23-676" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-677"><a href="#cb23-677" aria-hidden="true" tabindex="-1"></a>metrics_none</span>
<span id="cb23-678"><a href="#cb23-678" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-679"><a href="#cb23-679" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-680"><a href="#cb23-680" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-681"><a href="#cb23-681" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Interpretation</span></span>
<span id="cb23-682"><a href="#cb23-682" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-683"><a href="#cb23-683" aria-hidden="true" tabindex="-1"></a>These values are not “good” or “bad” in isolation; what matters is that they provide a **stable reference**. At this stage, the model operates on raw predictor scales. For a distance-based method like KNN, this implies:</span>
<span id="cb23-684"><a href="#cb23-684" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-685"><a href="#cb23-685" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Predictors with larger numeric ranges (e.g., <span class="in">`Lot_Area`</span>) can disproportionately influence distance calculations.</span>
<span id="cb23-686"><a href="#cb23-686" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-687"><a href="#cb23-687" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Smaller-range variables (e.g., ordinal-like <span class="in">`Overall_Cond`</span>) may contribute less than intended.</span>
<span id="cb23-688"><a href="#cb23-688" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-689"><a href="#cb23-689" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>The model’s behavior is therefore partially shaped by measurement units, not only by predictive structure.</span>
<span id="cb23-690"><a href="#cb23-690" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-691"><a href="#cb23-691" aria-hidden="true" tabindex="-1"></a>This is exactly why KNN is a useful diagnostic tool in a normalization-focused article: if scaling matters, we should see clear changes relative to this baseline once we introduce normalization.</span>
<span id="cb23-692"><a href="#cb23-692" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-693"><a href="#cb23-693" aria-hidden="true" tabindex="-1"></a>Next, we introduce scaling—but **incorrectly** on purpose. We will apply normalization *before* the train–test split (i.e., using information from the full dataset). This creates **data leakage** and can lead to deceptively improved test performance.</span>
<span id="cb23-694"><a href="#cb23-694" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-695"><a href="#cb23-695" aria-hidden="true" tabindex="-1"></a>After that, we will implement the correct workflow (fit scaling parameters on the training set only) and compare all scenarios side by side.</span>
<span id="cb23-696"><a href="#cb23-696" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-697"><a href="#cb23-697" aria-hidden="true" tabindex="-1"></a><span class="fu">### Scenario B — Incorrect Normalization (Data Leakage)</span></span>
<span id="cb23-698"><a href="#cb23-698" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-699"><a href="#cb23-699" aria-hidden="true" tabindex="-1"></a>In this scenario, we intentionally apply normalization **the wrong way**: we learn scaling parameters from the full dataset (including what will become the test set). This contaminates the evaluation because preprocessing has already “seen” information from the test distribution.</span>
<span id="cb23-700"><a href="#cb23-700" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-701"><a href="#cb23-701" aria-hidden="true" tabindex="-1"></a>The goal is not to recommend this approach, but to demonstrate how easily leakage can happen—and how it can artificially improve test metrics.</span>
<span id="cb23-702"><a href="#cb23-702" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-703"><a href="#cb23-703" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Leakage Pipeline: Normalize Using Full Data</span></span>
<span id="cb23-704"><a href="#cb23-704" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-705"><a href="#cb23-705" aria-hidden="true" tabindex="-1"></a>The <span class="in">`step_normalize()`</span> operation applies only to numeric predictors. In our dataset, <span class="in">`Overall_Cond`</span> is stored as a factor (ordinal-like category), so it must not be normalized directly.</span>
<span id="cb23-706"><a href="#cb23-706" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-709"><a href="#cb23-709" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb23-710"><a href="#cb23-710" aria-hidden="true" tabindex="-1"></a>rec_leak <span class="ot">&lt;-</span> <span class="fu">recipe</span>(Sale_Price <span class="sc">~</span> ., <span class="at">data =</span> ames_small) <span class="sc">%&gt;%</span></span>
<span id="cb23-711"><a href="#cb23-711" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_normalize</span>(<span class="fu">all_numeric_predictors</span>())</span>
<span id="cb23-712"><a href="#cb23-712" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-713"><a href="#cb23-713" aria-hidden="true" tabindex="-1"></a><span class="co"># WRONG on purpose: prepping on full data (leakage), but now type-safe</span></span>
<span id="cb23-714"><a href="#cb23-714" aria-hidden="true" tabindex="-1"></a>prep_leak <span class="ot">&lt;-</span> <span class="fu">prep</span>(rec_leak, <span class="at">training =</span> ames_small)</span>
<span id="cb23-715"><a href="#cb23-715" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-716"><a href="#cb23-716" aria-hidden="true" tabindex="-1"></a>train_leak <span class="ot">&lt;-</span> <span class="fu">bake</span>(prep_leak, <span class="at">new_data =</span> train_data)</span>
<span id="cb23-717"><a href="#cb23-717" aria-hidden="true" tabindex="-1"></a>test_leak  <span class="ot">&lt;-</span> <span class="fu">bake</span>(prep_leak, <span class="at">new_data =</span> test_data)</span>
<span id="cb23-718"><a href="#cb23-718" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-719"><a href="#cb23-719" aria-hidden="true" tabindex="-1"></a>wf_leak <span class="ot">&lt;-</span> <span class="fu">workflow</span>() <span class="sc">%&gt;%</span></span>
<span id="cb23-720"><a href="#cb23-720" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_model</span>(knn_spec) <span class="sc">%&gt;%</span></span>
<span id="cb23-721"><a href="#cb23-721" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_formula</span>(Sale_Price <span class="sc">~</span> .)</span>
<span id="cb23-722"><a href="#cb23-722" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-723"><a href="#cb23-723" aria-hidden="true" tabindex="-1"></a>fit_leak <span class="ot">&lt;-</span> <span class="fu">fit</span>(wf_leak, <span class="at">data =</span> train_leak)</span>
<span id="cb23-724"><a href="#cb23-724" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-725"><a href="#cb23-725" aria-hidden="true" tabindex="-1"></a>pred_leak <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit_leak, test_leak) <span class="sc">%&gt;%</span></span>
<span id="cb23-726"><a href="#cb23-726" aria-hidden="true" tabindex="-1"></a>  <span class="fu">bind_cols</span>(test_leak <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">select</span>(Sale_Price))</span>
<span id="cb23-727"><a href="#cb23-727" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-728"><a href="#cb23-728" aria-hidden="true" tabindex="-1"></a>metrics_leak <span class="ot">&lt;-</span> yardstick<span class="sc">::</span><span class="fu">metrics</span>(pred_leak, <span class="at">truth =</span> Sale_Price, <span class="at">estimate =</span> .pred)</span>
<span id="cb23-729"><a href="#cb23-729" aria-hidden="true" tabindex="-1"></a>metrics_leak</span>
<span id="cb23-730"><a href="#cb23-730" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-731"><a href="#cb23-731" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-732"><a href="#cb23-732" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Interpretation</span></span>
<span id="cb23-733"><a href="#cb23-733" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-734"><a href="#cb23-734" aria-hidden="true" tabindex="-1"></a>The performance obtained under this scenario reflects the consequences of **incorrect normalization with data leakage**.</span>
<span id="cb23-735"><a href="#cb23-735" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-736"><a href="#cb23-736" aria-hidden="true" tabindex="-1"></a>Compared to the baseline (no scaling), all three metrics deteriorate. This indicates that learning normalization parameters from the full dataset does **not** automatically lead to better predictive performance. In this case, the leakage-induced transformation appears to distort the distance structure in a way that is unfavorable for KNN.</span>
<span id="cb23-737"><a href="#cb23-737" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-738"><a href="#cb23-738" aria-hidden="true" tabindex="-1"></a>This result is particularly instructive because it challenges a common misconception:\</span>
<span id="cb23-739"><a href="#cb23-739" aria-hidden="true" tabindex="-1"></a>**data leakage does not necessarily inflate performance metrics**. Its effect depends on the interaction between the preprocessing step, the data distribution, and the model. What leakage *does* guarantee, however, is that the evaluation is no longer valid.</span>
<span id="cb23-740"><a href="#cb23-740" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-741"><a href="#cb23-741" aria-hidden="true" tabindex="-1"></a>Even if the metrics had improved under this scenario, they could not be trusted as estimates of out-of-sample performance. The test data would no longer represent genuinely unseen observations, since information from their distribution had already been incorporated during preprocessing.</span>
<span id="cb23-742"><a href="#cb23-742" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-743"><a href="#cb23-743" aria-hidden="true" tabindex="-1"></a>At this point, two important conclusions can be drawn:</span>
<span id="cb23-744"><a href="#cb23-744" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-745"><a href="#cb23-745" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>Scaling decisions materially affect model behavior, especially for distance-based methods.</span>
<span id="cb23-746"><a href="#cb23-746" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-747"><a href="#cb23-747" aria-hidden="true" tabindex="-1"></a><span class="ss">2.  </span>The timing of scaling—*when* parameters are learned—is as critical as *whether* scaling is applied at all.</span>
<span id="cb23-748"><a href="#cb23-748" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-749"><a href="#cb23-749" aria-hidden="true" tabindex="-1"></a>In the next scenario, we apply normalization correctly by estimating scaling parameters using the training data only and then applying them unchanged to the test set. This will provide the only defensible estimate of generalization performance among the normalization strategies considered.</span>
<span id="cb23-750"><a href="#cb23-750" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-751"><a href="#cb23-751" aria-hidden="true" tabindex="-1"></a><span class="fu">### Scenario C — Correct Normalization (Train-Only Scaling)</span></span>
<span id="cb23-752"><a href="#cb23-752" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-753"><a href="#cb23-753" aria-hidden="true" tabindex="-1"></a>In this final preprocessing scenario, normalization parameters are learned **exclusively from the training data** and then applied consistently to both the training and test sets.</span>
<span id="cb23-754"><a href="#cb23-754" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-755"><a href="#cb23-755" aria-hidden="true" tabindex="-1"></a>This workflow adheres to the core principle of leakage-free modeling.</span>
<span id="cb23-756"><a href="#cb23-756" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-757"><a href="#cb23-757" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Correct Pipeline: Normalize Using Training Data Only</span></span>
<span id="cb23-758"><a href="#cb23-758" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-761"><a href="#cb23-761" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb23-762"><a href="#cb23-762" aria-hidden="true" tabindex="-1"></a>rec_ok <span class="ot">&lt;-</span> <span class="fu">recipe</span>(Sale_Price <span class="sc">~</span> ., <span class="at">data =</span> train_data) <span class="sc">%&gt;%</span></span>
<span id="cb23-763"><a href="#cb23-763" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_normalize</span>(<span class="fu">all_numeric_predictors</span>())</span>
<span id="cb23-764"><a href="#cb23-764" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-765"><a href="#cb23-765" aria-hidden="true" tabindex="-1"></a>wf_ok <span class="ot">&lt;-</span> <span class="fu">workflow</span>() <span class="sc">%&gt;%</span></span>
<span id="cb23-766"><a href="#cb23-766" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_recipe</span>(rec_ok) <span class="sc">%&gt;%</span></span>
<span id="cb23-767"><a href="#cb23-767" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_model</span>(knn_spec)</span>
<span id="cb23-768"><a href="#cb23-768" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-769"><a href="#cb23-769" aria-hidden="true" tabindex="-1"></a>fit_ok <span class="ot">&lt;-</span> <span class="fu">fit</span>(wf_ok, <span class="at">data =</span> train_data)</span>
<span id="cb23-770"><a href="#cb23-770" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-771"><a href="#cb23-771" aria-hidden="true" tabindex="-1"></a>pred_ok <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit_ok, test_data) <span class="sc">%&gt;%</span></span>
<span id="cb23-772"><a href="#cb23-772" aria-hidden="true" tabindex="-1"></a>  <span class="fu">bind_cols</span>(test_data <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">select</span>(Sale_Price))</span>
<span id="cb23-773"><a href="#cb23-773" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-774"><a href="#cb23-774" aria-hidden="true" tabindex="-1"></a>metrics_ok <span class="ot">&lt;-</span> yardstick<span class="sc">::</span><span class="fu">metrics</span>(pred_ok, <span class="at">truth =</span> Sale_Price, <span class="at">estimate =</span> .pred)</span>
<span id="cb23-775"><a href="#cb23-775" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-776"><a href="#cb23-776" aria-hidden="true" tabindex="-1"></a>metrics_ok</span>
<span id="cb23-777"><a href="#cb23-777" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-778"><a href="#cb23-778" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-779"><a href="#cb23-779" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Interpretation</span></span>
<span id="cb23-780"><a href="#cb23-780" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-781"><a href="#cb23-781" aria-hidden="true" tabindex="-1"></a>This scenario represents the **correct normalization workflow**, where scaling parameters are learned exclusively from the training data and then applied unchanged to the test set. The results are **identical to the no-scaling baseline**. This finding is highly informative.</span>
<span id="cb23-782"><a href="#cb23-782" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-783"><a href="#cb23-783" aria-hidden="true" tabindex="-1"></a>First, it confirms that normalization itself does not automatically improve model performance. When applied correctly, scaling does not inject additional information into the modeling process; it merely changes the representation of the data. If the underlying distance structure relevant for prediction is already dominated by certain predictors, scaling may have little to no effect on performance.</span>
<span id="cb23-784"><a href="#cb23-784" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-785"><a href="#cb23-785" aria-hidden="true" tabindex="-1"></a>Second, the contrast with the leakage scenario is crucial. In Scenario B, incorrect normalization degraded performance, while in this scenario, correct normalization restores the metrics to their baseline levels. This symmetry reinforces the core message of this article:\</span>
<span id="cb23-786"><a href="#cb23-786" aria-hidden="true" tabindex="-1"></a>**the validity of preprocessing matters more than the apparent gains it may produce.**</span>
<span id="cb23-787"><a href="#cb23-787" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-788"><a href="#cb23-788" aria-hidden="true" tabindex="-1"></a>Third, these results highlight an often-overlooked point: the impact of scaling is model- and data-dependent. For this particular subset of predictors and this KNN configuration, normalization neither helps nor harms when applied correctly. In other settings—different feature sets, different distance metrics, or different models—the effect could be substantial.</span>
<span id="cb23-789"><a href="#cb23-789" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-790"><a href="#cb23-790" aria-hidden="true" tabindex="-1"></a>The key takeaway is therefore not that scaling is unnecessary, but that it must be:</span>
<span id="cb23-791"><a href="#cb23-791" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-792"><a href="#cb23-792" aria-hidden="true" tabindex="-1"></a>applied deliberately,</span>
<span id="cb23-793"><a href="#cb23-793" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-794"><a href="#cb23-794" aria-hidden="true" tabindex="-1"></a>restricted to appropriate variables,</span>
<span id="cb23-795"><a href="#cb23-795" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-796"><a href="#cb23-796" aria-hidden="true" tabindex="-1"></a>and learned at the correct stage of the modeling workflow.</span>
<span id="cb23-797"><a href="#cb23-797" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-798"><a href="#cb23-798" aria-hidden="true" tabindex="-1"></a>With all three scenarios evaluated, we can now compare them side by side and distill the practical lessons they offer.</span>
<span id="cb23-799"><a href="#cb23-799" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-800"><a href="#cb23-800" aria-hidden="true" tabindex="-1"></a><span class="fu">### Results Comparison</span></span>
<span id="cb23-801"><a href="#cb23-801" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-802"><a href="#cb23-802" aria-hidden="true" tabindex="-1"></a>With all three scenarios evaluated, we now compare them side by side. Since the model and data split were held constant, any differences observed here are entirely attributable to preprocessing choices.</span>
<span id="cb23-803"><a href="#cb23-803" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-804"><a href="#cb23-804" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Performance Summary</span></span>
<span id="cb23-805"><a href="#cb23-805" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-808"><a href="#cb23-808" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb23-809"><a href="#cb23-809" aria-hidden="true" tabindex="-1"></a>results_tbl <span class="ot">&lt;-</span> dplyr<span class="sc">::</span><span class="fu">bind_rows</span>(</span>
<span id="cb23-810"><a href="#cb23-810" aria-hidden="true" tabindex="-1"></a>  metrics_none <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">scenario =</span> <span class="st">"A — No Scaling"</span>),</span>
<span id="cb23-811"><a href="#cb23-811" aria-hidden="true" tabindex="-1"></a>  metrics_leak <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">scenario =</span> <span class="st">"B — Incorrect Scaling (Leakage)"</span>),</span>
<span id="cb23-812"><a href="#cb23-812" aria-hidden="true" tabindex="-1"></a>  metrics_ok   <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">scenario =</span> <span class="st">"C — Correct Scaling (Train-Only)"</span>)</span>
<span id="cb23-813"><a href="#cb23-813" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span></span>
<span id="cb23-814"><a href="#cb23-814" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">select</span>(scenario, .metric, .estimate) <span class="sc">%&gt;%</span></span>
<span id="cb23-815"><a href="#cb23-815" aria-hidden="true" tabindex="-1"></a>  tidyr<span class="sc">::</span><span class="fu">pivot_wider</span>(</span>
<span id="cb23-816"><a href="#cb23-816" aria-hidden="true" tabindex="-1"></a>    <span class="at">names_from =</span> .metric,</span>
<span id="cb23-817"><a href="#cb23-817" aria-hidden="true" tabindex="-1"></a>    <span class="at">values_from =</span> .estimate</span>
<span id="cb23-818"><a href="#cb23-818" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb23-819"><a href="#cb23-819" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-820"><a href="#cb23-820" aria-hidden="true" tabindex="-1"></a>results_tbl</span>
<span id="cb23-821"><a href="#cb23-821" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-822"><a href="#cb23-822" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-823"><a href="#cb23-823" aria-hidden="true" tabindex="-1"></a>This table summarizes test-set performance across all scenarios.</span>
<span id="cb23-824"><a href="#cb23-824" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-825"><a href="#cb23-825" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>**Scenario A (No Scaling)** serves as the baseline.</span>
<span id="cb23-826"><a href="#cb23-826" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-827"><a href="#cb23-827" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>**Scenario B (Incorrect Scaling with Leakage)** shows degraded performance.</span>
<span id="cb23-828"><a href="#cb23-828" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-829"><a href="#cb23-829" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>**Scenario C (Correct Scaling)** reproduces the baseline results exactly.</span>
<span id="cb23-830"><a href="#cb23-830" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-831"><a href="#cb23-831" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Visual Comparison (RMSE)</span></span>
<span id="cb23-832"><a href="#cb23-832" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-833"><a href="#cb23-833" aria-hidden="true" tabindex="-1"></a>To make the differences easier to interpret, we visualize RMSE across scenarios.</span>
<span id="cb23-834"><a href="#cb23-834" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-837"><a href="#cb23-837" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb23-838"><a href="#cb23-838" aria-hidden="true" tabindex="-1"></a>results_tbl <span class="sc">%&gt;%</span></span>
<span id="cb23-839"><a href="#cb23-839" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> scenario, <span class="at">y =</span> rmse, <span class="at">fill =</span> scenario)) <span class="sc">+</span></span>
<span id="cb23-840"><a href="#cb23-840" aria-hidden="true" tabindex="-1"></a><span class="fu">geom_col</span>(<span class="at">alpha =</span> <span class="fl">0.8</span>) <span class="sc">+</span></span>
<span id="cb23-841"><a href="#cb23-841" aria-hidden="true" tabindex="-1"></a><span class="fu">scale_fill_manual</span>(</span>
<span id="cb23-842"><a href="#cb23-842" aria-hidden="true" tabindex="-1"></a><span class="at">values =</span> <span class="fu">c</span>(</span>
<span id="cb23-843"><a href="#cb23-843" aria-hidden="true" tabindex="-1"></a><span class="st">"A — No Scaling"</span> <span class="ot">=</span> <span class="st">"#1f77b4"</span>,</span>
<span id="cb23-844"><a href="#cb23-844" aria-hidden="true" tabindex="-1"></a><span class="st">"B — Incorrect Scaling (Leakage)"</span> <span class="ot">=</span> <span class="st">"#d62728"</span>,</span>
<span id="cb23-845"><a href="#cb23-845" aria-hidden="true" tabindex="-1"></a><span class="st">"C — Correct Scaling (Train-Only)"</span> <span class="ot">=</span> <span class="st">"#2ca02c"</span></span>
<span id="cb23-846"><a href="#cb23-846" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb23-847"><a href="#cb23-847" aria-hidden="true" tabindex="-1"></a>) <span class="sc">+</span></span>
<span id="cb23-848"><a href="#cb23-848" aria-hidden="true" tabindex="-1"></a><span class="fu">labs</span>(</span>
<span id="cb23-849"><a href="#cb23-849" aria-hidden="true" tabindex="-1"></a><span class="at">title =</span> <span class="st">"RMSE comparison across preprocessing scenarios"</span>,</span>
<span id="cb23-850"><a href="#cb23-850" aria-hidden="true" tabindex="-1"></a><span class="at">x =</span> <span class="st">"Preprocessing scenario"</span>,</span>
<span id="cb23-851"><a href="#cb23-851" aria-hidden="true" tabindex="-1"></a><span class="at">y =</span> <span class="st">"RMSE"</span></span>
<span id="cb23-852"><a href="#cb23-852" aria-hidden="true" tabindex="-1"></a>) <span class="sc">+</span></span>
<span id="cb23-853"><a href="#cb23-853" aria-hidden="true" tabindex="-1"></a><span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb23-854"><a href="#cb23-854" aria-hidden="true" tabindex="-1"></a><span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"none"</span>)</span>
<span id="cb23-855"><a href="#cb23-855" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-856"><a href="#cb23-856" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-857"><a href="#cb23-857" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-858"><a href="#cb23-858" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Interpretation</span></span>
<span id="cb23-859"><a href="#cb23-859" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-860"><a href="#cb23-860" aria-hidden="true" tabindex="-1"></a>Several important conclusions emerge from this comparison.</span>
<span id="cb23-861"><a href="#cb23-861" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-862"><a href="#cb23-862" aria-hidden="true" tabindex="-1"></a>First, **normalization does not inherently improve performance**. When applied correctly (Scenario C), scaling neither improves nor degrades performance relative to the no-scaling baseline. This confirms that normalization is a representational transformation, not a source of predictive signal.</span>
<span id="cb23-863"><a href="#cb23-863" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-864"><a href="#cb23-864" aria-hidden="true" tabindex="-1"></a>Second, **incorrect normalization can be harmful**. Scenario B demonstrates that learning scaling parameters from the full dataset can distort the feature space in ways that negatively affect model behavior. Even more importantly, this scenario yields an invalid evaluation, regardless of whether the metrics appear better or worse.</span>
<span id="cb23-865"><a href="#cb23-865" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-866"><a href="#cb23-866" aria-hidden="true" tabindex="-1"></a>Third, these results reinforce a central theme of this article:\</span>
<span id="cb23-867"><a href="#cb23-867" aria-hidden="true" tabindex="-1"></a>**the correctness of the preprocessing workflow matters more than the choice of preprocessing method itself**.</span>
<span id="cb23-868"><a href="#cb23-868" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-869"><a href="#cb23-869" aria-hidden="true" tabindex="-1"></a>In practice, this means that:</span>
<span id="cb23-870"><a href="#cb23-870" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-871"><a href="#cb23-871" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>scaling should be applied only when it aligns with the model’s assumptions,</span>
<span id="cb23-872"><a href="#cb23-872" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-873"><a href="#cb23-873" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>preprocessing parameters must be learned exclusively from training data,</span>
<span id="cb23-874"><a href="#cb23-874" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-875"><a href="#cb23-875" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>and any apparent performance gains should be scrutinized for potential leakage.</span>
<span id="cb23-876"><a href="#cb23-876" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-877"><a href="#cb23-877" aria-hidden="true" tabindex="-1"></a><span class="fu">### Practical Takeaways from the Application</span></span>
<span id="cb23-878"><a href="#cb23-878" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-879"><a href="#cb23-879" aria-hidden="true" tabindex="-1"></a>From this controlled experiment, we can distill three practical lessons:</span>
<span id="cb23-880"><a href="#cb23-880" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-881"><a href="#cb23-881" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>**Do not expect normalization to be a silver bullet.** Its impact depends on the model, the data, and the feature set.</span>
<span id="cb23-882"><a href="#cb23-882" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-883"><a href="#cb23-883" aria-hidden="true" tabindex="-1"></a><span class="ss">2.  </span>**Never compromise the train–test boundary.** Leakage can invalidate results even when performance does not improve.</span>
<span id="cb23-884"><a href="#cb23-884" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-885"><a href="#cb23-885" aria-hidden="true" tabindex="-1"></a><span class="ss">3.  </span>**Treat preprocessing as part of the model.** Decisions about scaling are modeling decisions, not technical afterthoughts.</span>
<span id="cb23-886"><a href="#cb23-886" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-887"><a href="#cb23-887" aria-hidden="true" tabindex="-1"></a>These lessons generalize beyond KNN and apply to any workflow involving scale-sensitive models and data transformations.</span>
<span id="cb23-888"><a href="#cb23-888" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-889"><a href="#cb23-889" aria-hidden="true" tabindex="-1"></a><span class="fu">## Discussion and Conclusion</span></span>
<span id="cb23-890"><a href="#cb23-890" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-891"><a href="#cb23-891" aria-hidden="true" tabindex="-1"></a>Normalization is often introduced as a routine preprocessing step, applied almost reflexively before modeling. This article has argued—and demonstrated—that such a view is incomplete. Normalization is not a purely technical adjustment; it is a **modeling decision** whose consequences depend on the interaction between data, model assumptions, and evaluation design.</span>
<span id="cb23-892"><a href="#cb23-892" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-893"><a href="#cb23-893" aria-hidden="true" tabindex="-1"></a>From a theoretical perspective, scaling matters because many learning algorithms are sensitive to the relative magnitudes of predictors. Distance-based methods, regularized models, kernel methods, and optimization-driven algorithms implicitly encode assumptions about scale. Ignoring these assumptions can distort model behavior, while respecting them can improve stability and interpretability. At the same time, scaling does not create new information. It reshapes how existing information is represented.</span>
<span id="cb23-894"><a href="#cb23-894" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-895"><a href="#cb23-895" aria-hidden="true" tabindex="-1"></a>The empirical application using the Ames Housing dataset reinforced these points. By holding the model and data split constant and varying only the preprocessing strategy, we isolated the effect of normalization decisions. Three key findings emerged.</span>
<span id="cb23-896"><a href="#cb23-896" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-897"><a href="#cb23-897" aria-hidden="true" tabindex="-1"></a>First, **normalization does not guarantee performance improvements**. In the correct workflow, scaling reproduced the baseline results exactly. This confirms that normalization should not be expected to “fix” a model by itself. Its role is conditional and context-dependent.</span>
<span id="cb23-898"><a href="#cb23-898" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-899"><a href="#cb23-899" aria-hidden="true" tabindex="-1"></a>Second, **incorrect normalization compromises validity**. Learning scaling parameters from the full dataset—thereby introducing data leakage—altered model behavior and degraded performance in this example. More importantly, even if the metrics had improved, the evaluation would have been invalid. Leakage undermines the fundamental purpose of a test set: to approximate unseen data.</span>
<span id="cb23-900"><a href="#cb23-900" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-901"><a href="#cb23-901" aria-hidden="true" tabindex="-1"></a>Third, **the timing of preprocessing is as important as the method chosen**. The difference between valid and invalid evaluation hinged not on whether scaling was applied, but on *when* its parameters were learned. This distinction is often overlooked in practice, yet it is central to trustworthy modeling.</span>
<span id="cb23-902"><a href="#cb23-902" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-903"><a href="#cb23-903" aria-hidden="true" tabindex="-1"></a>Taken together, these results support a broader principle: preprocessing steps should be treated as integral components of the modeling pipeline, not as detached technical preliminaries. Decisions about normalization should be guided by model assumptions, data characteristics, and evaluation design—not by habit or generic checklists.</span>
<span id="cb23-904"><a href="#cb23-904" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-905"><a href="#cb23-905" aria-hidden="true" tabindex="-1"></a>In practical terms, this leads to a simple but robust rule:</span>
<span id="cb23-906"><a href="#cb23-906" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-907"><a href="#cb23-907" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **Split the data first. Learn preprocessing parameters from the training set only. Apply the same transformations to all future data.**</span></span>
<span id="cb23-908"><a href="#cb23-908" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-909"><a href="#cb23-909" aria-hidden="true" tabindex="-1"></a>Normalization, when used deliberately and correctly, is a powerful tool. When applied mechanically or at the wrong stage, it can mislead. Understanding this distinction is essential for building models that are not only accurate, but also scientifically defensible.</span>
<span id="cb23-910"><a href="#cb23-910" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-911"><a href="#cb23-911" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb23-912"><a href="#cb23-912" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-913"><a href="#cb23-913" aria-hidden="true" tabindex="-1"></a><span class="fu">## References</span></span>
<span id="cb23-914"><a href="#cb23-914" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-915"><a href="#cb23-915" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Hastie, T., Tibshirani, R., &amp; Friedman, J. (2009).\</span>
<span id="cb23-916"><a href="#cb23-916" aria-hidden="true" tabindex="-1"></a>    *The Elements of Statistical Learning: Data Mining, Inference, and Prediction*. Springer.</span>
<span id="cb23-917"><a href="#cb23-917" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-918"><a href="#cb23-918" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Kuhn, M., &amp; Johnson, K. (2013).\</span>
<span id="cb23-919"><a href="#cb23-919" aria-hidden="true" tabindex="-1"></a>    *Applied Predictive Modeling*. Springer.</span>
<span id="cb23-920"><a href="#cb23-920" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-921"><a href="#cb23-921" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Kuhn, M., &amp; Wickham, H. (2023).\</span>
<span id="cb23-922"><a href="#cb23-922" aria-hidden="true" tabindex="-1"></a>    *Tidymodels: A Collection of Packages for Modeling and Machine Learning Using Tidyverse Principles*.\</span>
<span id="cb23-923"><a href="#cb23-923" aria-hidden="true" tabindex="-1"></a>    <span class="ot">&lt;https://www.tidymodels.org/&gt;</span></span>
<span id="cb23-924"><a href="#cb23-924" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-925"><a href="#cb23-925" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Tidymodels Recipes Documentation.\</span>
<span id="cb23-926"><a href="#cb23-926" aria-hidden="true" tabindex="-1"></a>    <span class="ot">&lt;https://recipes.tidymodels.org/&gt;</span></span>
<span id="cb23-927"><a href="#cb23-927" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-928"><a href="#cb23-928" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Kuhn, M. (Caret package documentation).\</span>
<span id="cb23-929"><a href="#cb23-929" aria-hidden="true" tabindex="-1"></a>    <span class="ot">&lt;https://topepo.github.io/caret/&gt;</span></span>
<span id="cb23-930"><a href="#cb23-930" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-931"><a href="#cb23-931" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Modeldata package documentation (Ames Housing dataset).\</span>
<span id="cb23-932"><a href="#cb23-932" aria-hidden="true" tabindex="-1"></a>    <span class="ot">&lt;https://modeldata.tidymodels.org/reference/ames.html&gt;</span></span>
</code></pre></div><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© 2023, M. Fatih Tüzen</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p>This page is built with <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>




</body></html>