---
title: "Explained vs. Predictive Power: RÂ², Adjusted RÂ², and Beyond"
author: "M. Fatih TÃ¼zen"
date: "2025-04-30"
categories: [R, Statistics, Machine Learning, r-squared, adjusted-r-squared, predictive-modeling, tidymodels, model-evaluation]
image: quote_box.png
execute: 
  warning: false
  message: false
format:
  html:
    toc: true
    toc-depth: 3
    number-sections: true
    code-fold: false
    code-tools: true
    code-overflow: scroll
    code-block-background: true
---

![](quote_box.png){fig-align="center" width="8cm"}

# Introduction

> **You trust RÂ². Should you?**\
> You proudly present a model with RÂ² = 0.95. Everyone applauds.\
> But what if your model fails miserably on the next new data?

When building a statistical model, one of the first numbers analysts and data scientists often cite is the **RÂ²**, or coefficient of determination. It's widely reported in research, academic theses, and industry reports â€” and yet, frequently misunderstood or misused.

Does a high RÂ² mean your model is good? Is it enough to evaluate model performance? What about its adjusted or predictive counterparts?

This article will explore in depth: - What RÂ², Adjusted RÂ², and Predicted RÂ² actually mean - Why relying solely on RÂ² can mislead you - How to evaluate models using **both explanatory and predictive power** - Real-life implementation using the **{tidymodels}** framework in R

Weâ€™ll also discuss best practices and common pitfalls, and equip you with a mindset to look beyond surface-level model summaries.

# Theoretical Background

## What is RÂ²?

The **coefficient of determination**, RÂ², is defined as:

$$R^2 = 1 - \frac{\text{SS}_{\text{res}}}{\text{SS}_{\text{tot}}}$$

Where:

-   $\text{SS}_{\text{res}}$ = Sum of squares of residuals = $\sum (y_i - \hat{y}_i)^2$

-   $\text{SS}_{\text{tot}}$ = Total sum of squares = $\sum (y_i - \bar{y})^2$

It tells us the **proportion of variance explained by the model**. An RÂ² of 0.80 implies that 80% of the variability in the dependent variable is explained by the model.

But beware â€” it only measures **fit to training data**, not the model's ability to **generalize**.

## Adjusted RÂ²

When we add predictors to a regression model, RÂ² will never decrease â€” even if the added variables are irrelevant.

**Adjusted RÂ²** corrects this by penalizing the number of predictors: $$R^2_{\text{adj}} = 1 - \left(1 - R^2\right) \cdot \left(\frac{n - 1}{n - p - 1}\right)$$

Where:

-   n : number of observations

-   p : number of predictors

Thus, Adjusted RÂ² will **only increase** if the new predictor improves the model more than expected by chance.

## Predicted RÂ²

**Predicted RÂ²** (or cross-validated RÂ²) is the most honest estimate of model utility. It answers the question:

> *How well will this model predict new, unseen data?*

This is typically calculated using cross-validation, and unlike regular RÂ², it reflects **out-of-sample performance**.

You can also view it as:

$$R^2_{\text{pred}} = 1 - \frac{\text{PRESS}}{\text{SS}_{\text{tot}}}$$

Where PRESS is the **Prediction Error Sum of Squares** based on cross-validation.

# Dataset Overview

Weâ€™ll use the classic **Boston Housing Dataset** to demonstrate. It includes:

-   Socio-economic and housing variables for 506 Boston suburbs

-   Target: `medv` (median value of owner-occupied homes in \$1000s)

Below are the key variables:

-   **crim**: per capita crime rate by town
-   **zn**: proportion of residential land zoned for large lots
-   **indus**: proportion of non-retail business acres
-   **chas**: Charles River dummy variable (1 = tract bounds river; 0 = otherwise)
-   **nox**: nitric oxides concentration (parts per 10 million)
-   **rm**: average number of rooms per dwelling
-   **age**: proportion of owner-occupied units built before 1940
-   **dis**: weighted distance to employment centers
-   **rad**: index of accessibility to radial highways
-   **tax**: property-tax rate per \$10,000
-   **ptratio**: pupil-teacher ratio by town
-   **black**: 1000(Bk - 0.63)\^2 where Bk is the proportion of Black residents
-   **lstat**: percentage of lower status of the population
-   **medv**: **target** â€” median value of owner-occupied homes (in \$1000s)

This regression problem mimics common real estate or socio-economic modeling use cases. Letâ€™s first examine the datasetâ€™s summary statistics.

```{r}
library(tidymodels)
library(MASS)
library(ggplot2)
library(corrr)
library(skimr)
library(patchwork)


boston <- MASS::Boston
skim(boston)
```

**Commentary:**

-   Variables like `crim`, `tax`, and `lstat` exhibit high variability and potential skewness.

-   `chas` is binary and acts like a categorical indicator.

-   The target variable `medv` ranges from \$5,000 to \$50,000 (capped).

-   `rm` (average number of rooms) and `lstat` (lower status population) show notable spread and will likely play strong roles in the model.

Next, we examine correlations with `medv`:

```{r}
boston %>% correlate() %>% corrr::focus(medv) %>% arrange(desc(medv))
```

**Interpretation of Correlations:**

-   `rm` shows a **strong positive** correlation with `medv` â€” more rooms generally imply higher value.

-   `lstat` and `crim` have **strong negative** correlations â€” as lower status or crime increases, housing values drop.

-   `nox`, `age`, and `ptratio` also show negative correlations with price, hinting at socio-environmental effects.

These insights will guide us in building and evaluating our model.

# Exploratory Data Analysis

Letâ€™s visualize some of the most influential variables in relation to `medv`, our target variable. These exploratory graphs help reveal potential linear or nonlinear relationships, outliers, or the need for transformation.

```{r}
# Define individual plots with improved formatting for Quarto rendering
p1 <- ggplot(boston, aes(rm, medv)) +
  geom_point(alpha = 0.5, color = "#2c7fb8") +
  geom_smooth(method = "lm", se = FALSE, color = "black") +
  labs(
    title = "Rooms\nvs. Median Value",
    x = "Average Number of Rooms (rm)",
    y = "Median Value of Homes ($1000s)"
  ) +
  theme_minimal() +
  theme(plot.title = element_text(size = 11, lineheight = 1.1))

p2 <- ggplot(boston, aes(lstat, medv)) +
  geom_point(alpha = 0.5, color = "#de2d26") +
  geom_smooth(method = "loess", se = FALSE, color = "black") +
  labs(
    title = "Lower Status %\nvs. Median Value",
    x = "% Lower Status Population (lstat)",
    y = "Median Value of Homes ($1000s)"
  ) +
  theme_minimal() +
  theme(plot.title = element_text(size = 11, lineheight = 1.1))

p3 <- ggplot(boston, aes(nox, medv)) +
  geom_point(alpha = 0.5, color = "#31a354") +
  geom_smooth(method = "loess", se = FALSE, color = "black") +
  labs(
    title = "NOx Concentration\nvs. Median Value",
    x = "NOx concentration (ppm)",
    y = "Median Value of Homes ($1000s)"
  ) +
  theme_minimal() +
  theme(plot.title = element_text(size = 11, lineheight = 1.1))

p4 <- ggplot(boston, aes(age, medv)) +
  geom_point(alpha = 0.5, color = "#ff7f00") +
  geom_smooth(method = "loess", se = FALSE, color = "black") +
  labs(
    title = "Old Homes %\nvs. Median Value",
    x = "% Homes Built Before 1940 (age)",
    y = "Median Value of Homes ($1000s)"
  ) +
  theme_minimal() +
  theme(plot.title = element_text(size = 11, lineheight = 1.1))

p5 <- ggplot(boston, aes(tax, medv)) +
  geom_point(alpha = 0.5, color = "#6a3d9a") +
  geom_smooth(method = "loess", se = FALSE, color = "black") +
  labs(
    title = "Tax Rate\nvs. Median Value",
    x = "Tax Rate (per $10,000)",
    y = "Median Value of Homes ($1000s)"
  ) +
  theme_minimal() +
  theme(plot.title = element_text(size = 11, lineheight = 1.1))

p6 <- ggplot(boston, aes(dis, medv)) +
  geom_point(alpha = 0.5, color = "#1f78b4") +
  geom_smooth(method = "loess", se = FALSE, color = "black") +
  labs(
    title = "Distance to Jobs\nvs. Median Value",
    x = "Weighted Distance to Employment Centers (dis)",
    y = "Median Value of Homes ($1000s)"
  ) +
  theme_minimal() +
  theme(plot.title = element_text(size = 11, lineheight = 1.1))

```

```{r, fig.width=12, fig.height=4}
(p1 | p2) + plot_layout(guides = 'collect')
```

-   **Rooms (`rm`)**: Strong positive linear relationship with `medv`. More rooms correlate with higher home values.
-   **Lower Status Population (`lstat`)**: Strong nonlinear inverse relation. Poorer areas tend to have significantly lower housing values.

```{r, fig.width=12, fig.height=4}
(p3 | p4) + plot_layout(guides = 'collect')
```

-   **Nitric Oxide (`nox`)**: Moderate negative relationship â€” environmental factors like pollution impact price.
-   **Old Homes (`age`)**: Slight negative trend â€” older areas may have reduced appeal or value.

```{r, fig.width=12, fig.height=4}
(p5 | p6) + plot_layout(guides = 'collect')
```

-   **Tax Rate (`tax`)**: Higher taxes often relate to lower housing value, possibly due to location or socio-economic constraints.
-   **Distance to Employment Centers (`dis`)**: Weak to moderate positive correlation. Suburban or well-connected areas might command higher value.

These six plots combine both socioeconomic and environmental dimensions of housing value â€” providing both intuition and modeling direction.

# Modeling with Tidymodels

Now that weâ€™ve explored the data, it's time to fit a model using the **tidymodels** framework. We'll use a simple linear regression to predict `medv`, the median home value.

## Data Splitting and Preprocessing

We begin by splitting the dataset into training and testing sets. The training set will be used to fit the model, and the test set will evaluate its generalization performance.

```{r}
set.seed(42)
split <- initial_split(boston, prop = 0.8)
train <- training(split)
test <- testing(split)

rec <- recipe(medv ~ ., data = train)
model <- linear_reg() %>% set_engine("lm")
workflow <- workflow() %>% add_recipe(rec) %>% add_model(model)
```

## Model Fitting

We now fit the model to the training data:

```{r}
fit <- fit(workflow, data = train)
```

## Evaluating the Model on the Training Set

Letâ€™s extract the RÂ² and Adjusted RÂ² values from the fitted model:

```{r}
training_summary <- glance(extract_fit_parsnip(fit))
training_summary %>% dplyr::select(r.squared, adj.r.squared)
```

**ðŸ” Interpretation:**

-   **RÂ²** measures the proportion of variance in `medv` explained by the predictors in the training set.
-   **Adjusted RÂ²** adjusts this value by penalizing for the number of predictors, making it more reliable in multi-variable contexts.

If RÂ² and Adjusted RÂ² differ significantly, it indicates that some predictors may not be contributing meaningfully to the model.

> Example: A model with 12 predictors might show RÂ² = 0.76, but Adjusted RÂ² = 0.72 â€” suggesting some predictors are adding complexity without real explanatory power.

## Test Set Performance

Now we assess the model on the unseen test data:

```{r}
preds <- predict(fit, test) %>% bind_cols(test)
metrics(preds, truth = medv, estimate = .pred)
```

**ðŸ“‰ Interpretation:**

-   If **test RÂ²** is **much lower** than training RÂ², overfitting may be present.
-   If **test RMSE** is high, the modelâ€™s absolute prediction error is large â€” another sign of poor generalization.

## Cross-Validation for Predicted RÂ²

To get a more robust performance estimate, we use 10-fold cross-validation:

```{r}
set.seed(42)
cv <- vfold_cv(train, v = 10)
resample <- fit_resamples(
  workflow,
  resamples = cv,
  metrics = metric_set(rsq, rmse),
  control = control_resamples(save_pred = TRUE)
)
collect_metrics(resample)
```

**âœ… Interpretation:**

-   **Predicted RÂ² (via CV)** tells us how well the model would perform on unseen data across multiple resamples.
-   It typically lies between training RÂ² and test RÂ².
-   Consistency between cross-validated and test RÂ² implies a stable model.

::: callout-tip
Use cross-validation as a standard evaluation tool, especially when data is limited.
:::

**ðŸ’¬ Summary of Findings:**

-   Our linear model explains a good portion of the variance, but some predictors might be irrelevant or redundant.
-   Cross-validation confirms the model is relatively stable but leaves room for refinement â€” possibly through feature selection or nonlinear modeling.

In the next step, we can analyze residuals or explore model improvements such as polynomial terms or regularization.

## Residual Diagnostics

Letâ€™s now check if our linear model satisfies basic regression assumptions. Weâ€™ll plot residuals and assess patterns, non-linearity, and potential heteroskedasticity.

```{r, fig.width=10, fig.height=4}
library(broom)
library(ggthemes)

aug <- augment(fit$fit$fit$fit)

ggplot(aug, aes(.fitted, .resid)) +
  geom_point(alpha = 0.5, color = "#2c7fb8") +
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(
    title = "Residuals vs Fitted Values",
    x = "Fitted Values",
    y = "Residuals"
  ) +
  theme_minimal()
```

**ðŸ“Œ Interpretation:**

-   We want residuals to be randomly scattered around zero.
-   If thereâ€™s a pattern or funnel shape, that may indicate **non-linearity** or **heteroskedasticity**.

## Improving the Model: Transforming `lstat`

From our earlier EDA, we saw a strong **nonlinear relationship** between `lstat` (lower status %) and `medv`. Letâ€™s try **log-transforming** `lstat` to capture that curvature.

### Updated Recipe with Transformation

```{r}
rec_log <- recipe(medv ~ ., data = train) %>%
  step_log(lstat)

workflow_log <- workflow() %>%
  add_model(model) %>%
  add_recipe(rec_log)

fit_log <- fit(workflow_log, data = train)
```

### Evaluation of Transformed Model

```{r}
preds_log <- predict(fit_log, test) %>% bind_cols(test)
metrics(preds_log, truth = medv, estimate = .pred)
```

```{r}
glance(fit_log)
```

**ðŸ§  Interpretation:**

-   Compare RMSE and RÂ² from the transformed model to the original.
-   If we see improvement, the transformation helped capture underlying nonlinearity.
-   **Adjusted RÂ²** is especially helpful here to assess whether the transformation truly improved fit â€” not just overfit.

::: callout-tip
Transformations, polynomial terms, and splines are all valid strategies to improve linear models without abandoning interpretability.
:::

With residuals checked and a transformation tested, our next step could be to explore **regularized models** like ridge or lasso regression, or even move beyond linearity with **tree-based** models.

# Common Pitfalls and Misconceptions

Even though RÂ² is widely reported and intuitively appealing, its interpretation is often flawed â€” even by experienced analysts. Here, weâ€™ll go beyond textbook definitions and highlight real-world traps and misunderstandings related to RÂ² and its variants.

**ðŸš« Misconception 1: High RÂ² means the model is good**

-   A model with RÂ² = 0.95 may **look impressive**, but that doesnâ€™t guarantee predictive power.
-   High RÂ² can result from overfitting, especially when the model is complex or contains many predictors.
-   **Adjusted RÂ² and Predicted RÂ²** must be considered to evaluate true usefulness.

**âš ï¸ Misconception 2: Adding predictors always improves the model**

-   While RÂ² never decreases with more variables, **Adjusted RÂ² can** â€” and should â€” if the new variable doesnâ€™t add real value.
-   Including irrelevant predictors increases complexity without improving explanatory power.
-   This is a form of **dimensional overfitting**.

**âŒ Misconception 3: RÂ² indicates causality**

-   RÂ² quantifies correlation, **not causation**.
-   A high RÂ² can arise from spurious relationships or confounding variables.
-   Always supplement with **domain knowledge** and causal reasoning.

**ðŸ“‰ Misconception 4: RÂ² is a universal performance metric**

-   RÂ² only applies to **regression tasks**. Using it for classification models is inappropriate and meaningless.
-   For binary classification, use metrics like **AUC**, **accuracy**, **precision**, and **recall**.

**ðŸ” Misconception 5: Residual plots donâ€™t matter if RÂ² is high**

-   A good RÂ² doesnâ€™t guarantee that model assumptions are met.
-   Residual patterns may still reveal **non-linearity**, **heteroskedasticity**, or **influential outliers**.
-   Always inspect residual diagnostics.

**ðŸ’¡ Misconception 6: Predicted RÂ² isnâ€™t necessary**

-   Many practitioners report RÂ² and Adjusted RÂ², but **omit cross-validation entirely**.
-   **Predicted RÂ²** (e.g., via 10-fold CV) is the **most honest measure** of model generalizability.

**ðŸ”¬ Misconception 7: RÂ² has a fixed interpretation**

-   RÂ² values **depend on the context**. In social sciences, an RÂ² of 0.3 can be meaningful, while in physics we expect 0.99+.
-   A "low" RÂ² doesnâ€™t mean the model is useless â€” it may reflect inherent variability in human behavior or macroeconomic data.

------------------------------------------------------------------------

> **Insight:** Always use RÂ² in context â€” alongside other metrics, validation strategies, and graphical checks.

For a deeper dive into RÂ² misconceptions and proper regression diagnostics, see:

-   Harrell, F. (2015). *Regression Modeling Strategies*. Springer.

-   Gelman & Hill (2006). *Data Analysis Using Regression and Multilevel/Hierarchical Models*.

-   Burnham & Anderson (2002). *Model Selection and Multimodel Inference*.

-   Kutner et al. (2004). *Applied Linear Regression Models*.

Together, these references build the foundation for **responsible model interpretation**.

# Conclusion & Recommendations

## ðŸ“Œ Summary

In this post, we explored **RÂ²**, **Adjusted RÂ²**, and **Predicted RÂ²** in depth â€” not just as mathematical constructs, but as tools for critical thinking in modeling. We walked through theory, practical application in R with tidymodels, residual diagnostics, and even model improvement through transformation.

Letâ€™s recap: - **RÂ²** tells us how well our model fits the training data, but can be misleading on its own. - **Adjusted RÂ²** improves upon RÂ² by accounting for model complexity. - **Predicted RÂ²**, evaluated via cross-validation, provides the most trustworthy estimate of real-world performance.

High RÂ² values can be seductive. But as we saw, **they don't guarantee causality, generalizability, or correctness**. Only by combining RÂ² with residual diagnostics, domain knowledge, and out-of-sample validation can we judge a model responsibly.

## ðŸ’¡ Recommendations for Practitioners

1.  **Always accompany RÂ² with Adjusted and Predicted RÂ²** â€” never rely on one metric alone.
2.  **Perform residual diagnostics** to check linearity, variance assumptions, and outlier influence.
3.  **Use cross-validation (e.g., 10-fold)** as a default evaluation strategy, especially when the dataset is not large.
4.  **Transform nonlinear predictors** (as we did with `lstat`) or use flexible models (e.g., splines, GAMs) when needed.
5.  **Avoid including irrelevant predictors** â€” they inflate RÂ² without improving generalization.
6.  **Contextualize your RÂ²** â€” in some fields, a lower RÂ² is still useful; in others, it may signal inadequacy.
7.  **Complement numerical metrics with visual tools** â€” scatterplots, predicted vs. actual plots, and residuals reveal insights numbers alone may miss.

## ðŸš€ Looking Ahead

If you want to take your modeling further: - Try **ridge or lasso regression** to handle multicollinearity. - Explore **tree-based models** (e.g., random forests) when relationships are complex and nonlinear. - Use tools like **`yardstick`** and **`modeltime`** to automate robust validation and reporting.

> In the end, modeling isnâ€™t just about maximizing RÂ² â€” itâ€™s about **understanding your data, validating your decisions**, and making **informed predictions**.

Thanks for reading!

Feel free to share, fork, or reuse this analysis. Questions and comments are welcome.
